[
    {
        "doc_id": 0,
        "doc": {
            "question_id": 128509465,
            "question": "____ is typically the largest contributor to the overhead experienced by users of dynamic binary modifiers.",
            "options": [
                "The baseline system overhead",
                "The user-defined tool",
                "The static instrumentation routines",
                "The operating system"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically the largest contributor to the overhead experienced by users of dynamic binary modifiers.",
                "Expected Label": "B",
                "Predicted Label": "**B) The user-defined tool** \n\nHere's why:\n\n* **Dynamic binary modifiers (DBMs)** work by inserting code into a program while it's running.  The efficiency of this code insertion and the complexity of the analysis being done is the primary driver of overhead.\n\nLet's break down the options:\n\n* **A) The baseline system overhead:**  While all systems have overhead, it's relatively constant and less likely to be the largest contributor compared to the dynamic nature of DBMs.\n* **C) The static instrumentation routines:** Static instrumentation happens before runtime and wouldn't be a major factor in *dynamic* binary modification.\n* **D) The operating system:** The OS plays a role, but its impact is less significant than the tool itself, which is directly manipulating the running program.\n\n**In summary:** The tools used for dynamic binary modification, especially if they are complex or poorly optimized, are the most likely reason for significant overhead experienced by users. \n"
            }
        ]
    },
    {
        "doc_id": 1,
        "doc": {
            "question_id": 128509469,
            "question": "To detect data races, the Intel Parallel Inspector product uses Pin to instrument all machine instructions in the program that reference memory and records the ____.",
            "options": [
                "variable names",
                "register values",
                "machine codes",
                "effective addresses"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "To detect data races, the Intel Parallel Inspector product uses Pin to instrument all machine instructions in the program that reference memory and records the ____.",
                "Expected Label": "D",
                "Predicted Label": "D) effective addresses \n\n**Explanation:**\n\nData races occur when multiple threads access the same memory location and at least one of them is a write operation, without proper synchronization. To detect this, tools like Intel Parallel Inspector need to track which memory addresses are being accessed by different threads. \n\n* **Effective addresses** represent the final memory locations accessed by instructions.  By monitoring these, the tool can identify potential conflicts where multiple threads interact with the same memory location without proper coordination.\n\nLet's analyze why the other options are not the best fit:\n\n* **Variable names:** While useful for debugging, variable names are high-level abstractions. At the hardware level where data races occur, the processor operates with addresses, not variable names.\n* **Register values:** Register values are transient and don't necessarily reflect the memory locations being accessed. An instruction might load a value from memory into a register, modify it, and then store it back to a different memory location.\n* **Machine codes:**  Machine codes (opcodes) represent the instructions themselves. While necessary to understand the operations, they don't directly reveal the specific memory addresses being accessed. \n"
            }
        ]
    },
    {
        "doc_id": 2,
        "doc": {
            "question_id": 128509471,
            "question": "____ is a technique that allows replacing an entire routine in a guest application or shared library with a custom routine without access to the source code.",
            "options": [
                "Function replacement",
                "Instruction insertion",
                "Memory operand rewriting",
                "Control flow modification"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique that allows replacing an entire routine in a guest application or shared library with a custom routine without access to the source code.",
                "Expected Label": "A",
                "Predicted Label": "A) Function replacement \n\nLet's break down why:\n\n* **Function replacement** directly describes the action of swapping out an existing function with a custom one. This is the core concept behind techniques like DLL injection or hooking.\n\nHere's why the other options aren't as accurate:\n\n* **Instruction insertion** modifies the code by adding instructions, not replacing an entire function.\n* **Memory operand rewriting** focuses on changing the data a program uses, not the code itself.\n* **Control flow modification** alters the order in which instructions are executed, but doesn't inherently imply complete function replacement. \n"
            }
        ]
    },
    {
        "doc_id": 3,
        "doc": {
            "question_id": 128509473,
            "question": "Dynamic binary modification can be used for runtime security features, such as preventing execution of malicious code disguised as ____ .",
            "options": [
                "Shared Libraries",
                "Data",
                "System Calls",
                "Instructions"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Dynamic binary modification can be used for runtime security features, such as preventing execution of malicious code disguised as ____ .",
                "Expected Label": "B",
                "Predicted Label": "D) Instructions \n\n**Explanation:**\n\nDynamic binary modification (DBM) can alter instructions on the fly. This is often used in security contexts to prevent the execution of malicious code injected into a program, disguising itself as legitimate instructions. \n\nLet's examine why the other options are less suitable:\n\n* **A) Shared Libraries:** While DBM could theoretically modify shared libraries, it's a less common and more complex approach for preventing malicious code execution. Security measures usually focus on verifying library integrity rather than real-time modification.\n* **B) Data:** Malicious code disguised as data wouldn't be directly executable, making DBM less relevant in this scenario. \n* **C) System Calls:** DBM could be used to intercept and modify system calls, but it's not about preventing the execution of malicious code *disguised* as system calls.  \n\n**Therefore, the most direct and relevant application of DBM for runtime security is targeting malicious code masquerading as legitimate instructions.** \n"
            }
        ]
    },
    {
        "doc_id": 4,
        "doc": {
            "question_id": 128509475,
            "question": "____ is a drawback of trace-driven simulation due to the pre-determined outcome of every branch.",
            "options": [
                "Increased hardware requirements",
                "Improved accuracy",
                "Elimination of wrong-path execution",
                "Simplified implementation"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a drawback of trace-driven simulation due to the pre-determined outcome of every branch.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Elimination of wrong-path execution**.\n\nHere's why:\n\n* **Trace-driven simulation** uses pre-recorded execution traces (logs of instructions executed by a program). This means branch outcomes are already decided, limiting the simulation's ability to explore alternative execution paths that might occur with different input data.\n\nLet's look at why the other options are incorrect:\n\n* **A) Increased hardware requirements:** While trace-driven simulation can be resource-intensive, it's not primarily due to the pre-determined branch outcomes. The size of the trace file is a bigger factor.\n* **B) Improved accuracy:**  Pre-determined branches can actually *hinder* accuracy by not reflecting the dynamic behavior of real-world execution.\n* **D) Simplified implementation:**  While pre-determined branches might simplify some aspects, the overall process of collecting, storing, and using traces can be complex. \n\n**In summary, the main drawback of pre-determined branch outcomes in trace-driven simulation is the inability to explore alternative execution paths, effectively eliminating the simulation of \"wrong-path\" execution.** \n"
            }
        ]
    },
    {
        "doc_id": 5,
        "doc": {
            "question_id": 128509483,
            "question": "____ is a technique used in dynamic binary modification systems to improve the spatial locality of resulting code in memory and enable optimizations on larger code regions.",
            "options": [
                "Register allocation",
                "Loop unrolling",
                "Trace formation",
                "Instruction scheduling"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique used in dynamic binary modification systems to improve the spatial locality of resulting code in memory and enable optimizations on larger code regions.",
                "Expected Label": "C",
                "Predicted Label": "C"
            }
        ]
    },
    {
        "doc_id": 6,
        "doc": {
            "question_id": 128509489,
            "question": "The typical on-off ratio achieved in phase change memory devices is in the range of ____.",
            "options": [
                "1,000 - 10,000",
                "100 - 500",
                "10 - 50",
                "1,000,000 - 100,000,000"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The typical on-off ratio achieved in phase change memory devices is in the range of ____.",
                "Expected Label": "B",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 7,
        "doc": {
            "question_id": 128509491,
            "question": "PCM arrays with cell sizes in the range of ____ have been demonstrated using diodes and BJTs as access devices.",
            "options": [
                "1-2nm^2",
                "100-200nm^2",
                "5.5F^2 - 11.5F^2",
                "1-2mm^2"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "PCM arrays with cell sizes in the range of ____ have been demonstrated using diodes and BJTs as access devices.",
                "Expected Label": "C",
                "Predicted Label": "B) 100-200nm^2 \n\nHere's why:\n\n* **PCM Arrays:** Phase-change memory (PCM) arrays store data by altering the state of a chalcogenide glass material.\n* **Cell Size Matters:** Smaller cell sizes are crucial for achieving higher storage densities in memory technologies.\n* **Realistic Dimensions:** While options A and D represent extremely small and large sizes respectively, option B (100-200nm^2) falls within a realistic range for modern PCM technology. Option C seems to use an incorrect unit (F^2).\n\n**Therefore, B is the most plausible answer.** \n"
            }
        ]
    },
    {
        "doc_id": 8,
        "doc": {
            "question_id": 128509495,
            "question": "____ is a technique that avoids redundant writes to dirty pages in hybrid memory systems by installing all pages in DRAM and writing to PCM only on DRAM eviction.",
            "options": [
                "Lazy Write Architecture",
                "Line Level Writeback",
                "Page Level Bypass",
                "Write Queue Buffering"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique that avoids redundant writes to dirty pages in hybrid memory systems by installing all pages in DRAM and writing to PCM only on DRAM eviction.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 9,
        "doc": {
            "question_id": 128509497,
            "question": "Write Cancellation with Adaptive Threshold (WCAT) dynamically adjusts the threshold for write cancellation based on the _____.",
            "options": [
                "Number of read requests queued",
                "Number of write requests queued",
                "Average latency of recent read requests",
                "Current system temperature"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Write Cancellation with Adaptive Threshold (WCAT) dynamically adjusts the threshold for write cancellation based on the _____.",
                "Expected Label": "B",
                "Predicted Label": "C) Average latency of recent read requests \n\n**Explanation:**\n\nWCAT aims to optimize read performance in systems where writes can delay reads. It does this by canceling writes that haven't reached the memory banks when there's a high chance a subsequent read request will need that data. The average latency of recent read requests is the best indicator of this.  If read latencies are high, it suggests contention from writes, making write cancellation more beneficial. \n"
            }
        ]
    },
    {
        "doc_id": 10,
        "doc": {
            "question_id": 128509501,
            "question": "The ____ wear leveling technique can help reduce spatial correlation in the location of heavily written memory lines.",
            "options": [
                "Start-Gap",
                "Perfect",
                "Baseline",
                "Randomized Start-Gap"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ wear leveling technique can help reduce spatial correlation in the location of heavily written memory lines.",
                "Expected Label": "D",
                "Predicted Label": "D) Randomized Start-Gap \n\nHere's why:\n\n* **Randomized Start-Gap Wear Leveling** is specifically designed to combat spatial correlation in flash memory. It achieves this by randomly selecting a starting block for writing data and then sequentially filling blocks until a gap is reached. This random starting point helps distribute write/erase cycles more evenly across the memory, reducing wear in concentrated areas.\n\nLet me know if you'd like more details on wear leveling techniques! \n"
            }
        ]
    },
    {
        "doc_id": 11,
        "doc": {
            "question_id": 128509503,
            "question": "Wear leveling algorithms originally designed for ____ are often unsuitable for main memory due to their high overhead.",
            "options": [
                "Flash",
                "DRAM",
                "SRAM",
                "Magnetic storage"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Wear leveling algorithms originally designed for ____ are often unsuitable for main memory due to their high overhead.",
                "Expected Label": "A",
                "Predicted Label": "A) Flash \n\n**Explanation:**\n\nWear leveling is crucial for flash memory because it has a limited number of write cycles. Distributing writes evenly across the memory cells extends the lifespan of the device. While wear leveling techniques could theoretically be applied to other storage types, the overhead is generally unacceptable for faster technologies like DRAM or SRAM. \n"
            }
        ]
    },
    {
        "doc_id": 12,
        "doc": {
            "question_id": 128509507,
            "question": "A ____ attack repeatedly writes to a small set of memory locations while disguising the pattern with random accesses to other locations.",
            "options": [
                "Birthday Paradox",
                "Repeat Address",
                "Stealth Mode",
                "Generalized RAA"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ attack repeatedly writes to a small set of memory locations while disguising the pattern with random accesses to other locations.",
                "Expected Label": "C",
                "Predicted Label": "D \n\n**D) Generalized RAA** stands for **Generalized Rowhammer Attack**.  \n\n**Rowhammer** exploits a hardware vulnerability in DRAM. A **Generalized Rowhammer Attack** is characterized by repeatedly accessing (and writing to) a small set of memory locations, all the while masking the pattern by incorporating random accesses to other locations. This makes the attack harder to detect than simpler Rowhammer variations.\n\nLet's look at why the other options are incorrect:\n\n* **A) Birthday Paradox:**  This is a probability theory concept and not directly related to a specific memory attack pattern.\n* **B) Repeat Address:** While simple, this is a straightforward attack pattern and wouldn't involve disguising the access with randomness. \n* **C) Stealth Mode:**  This is a general term for hiding malicious activity and not a specific attack technique.\n\nLet me know if you have other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 13,
        "doc": {
            "question_id": 128509511,
            "question": "Ray tracing involves casting a ray from an observer, through a ____ output image, and through a 2-D scene.",
            "options": [
                "1-D",
                "2-D",
                "3-D",
                "4-D"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Ray tracing involves casting a ray from an observer, through a ____ output image, and through a 2-D scene.",
                "Expected Label": "A",
                "Predicted Label": "B) 2-D \n\nRay tracing casts rays through a 2D image plane (representing the screen) to determine the color of each pixel. \n"
            }
        ]
    },
    {
        "doc_id": 14,
        "doc": {
            "question_id": 128509513,
            "question": "____ designs typically have the lowest cost in terms of area and power when exploiting data parallelism.",
            "options": [
                "Superscalar",
                "SIMD",
                "Multithreaded",
                "Multi-core"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ designs typically have the lowest cost in terms of area and power when exploiting data parallelism.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **SIMD (Single Instruction, Multiple Data)** architectures are specifically designed for data parallelism. They apply the same operation to multiple data elements simultaneously, using a single instruction set and control unit. This shared control logic and the absence of complex scheduling mechanisms result in lower area and power consumption compared to other options.\n\nLet's analyze why the other options are less suitable:\n\n* **Superscalar:** Aims to exploit instruction-level parallelism, not necessarily data parallelism. They have complex logic for instruction scheduling and out-of-order execution, leading to higher area and power.\n* **Multithreaded:** Focuses on running multiple threads concurrently, which may or may not involve data parallelism. The overhead of thread management adds to the area and power.\n* **Multi-core:** Similar to multithreading, they target broader parallelism, not specifically data parallelism. Multiple cores increase the area and power consumption due to replicated components. \n\n**Therefore, SIMD is the most cost-effective design for exploiting data parallelism in terms of area and power.** \n"
            }
        ]
    },
    {
        "doc_id": 15,
        "doc": {
            "question_id": 128509515,
            "question": "As SIMD width increases, the number of ____ touched by a single SIMD memory operation also grows, potentially impacting memory system performance.",
            "options": [
                "registers",
                "cache lines",
                "instruction queues",
                "TLB entries"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "As SIMD width increases, the number of ____ touched by a single SIMD memory operation also grows, potentially impacting memory system performance.",
                "Expected Label": "B",
                "Predicted Label": "B) cache lines \n"
            }
        ]
    },
    {
        "doc_id": 16,
        "doc": {
            "question_id": 128509519,
            "question": "____ is typically used to allow SIMD functional units and registers to be separated from the scalar portion of the core.",
            "options": [
                "A dedicated SIMD register file",
                "Multiple general-purpose registers",
                "A shared register file",
                "Virtual registers"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to allow SIMD functional units and registers to be separated from the scalar portion of the core.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **A dedicated SIMD register file** is the most common and efficient way to handle SIMD operations. This allows for wider data paths and direct access to SIMD data without interfering with scalar operations.\n\nLet's analyze why the other options are not the best fit:\n\n* **Multiple general-purpose registers:** While some architectures overlap SIMD registers with general-purpose registers, it's not the ideal approach. It can lead to register pressure and complexity in managing data types. \n* **A shared register file:**  Sharing the same register file for both scalar and SIMD operations would create bottlenecks and limit the effectiveness of both.\n* **Virtual registers:** Virtual registers are an abstraction provided by the compiler and managed through register renaming. They don't directly address the architectural separation of SIMD and scalar units. \n"
            }
        ]
    },
    {
        "doc_id": 17,
        "doc": {
            "question_id": 128509521,
            "question": "If the SIMD width is ______  the cache line size, a single SIMD memory instruction may require accessing multiple cache lines.",
            "options": [
                "less than",
                "larger than",
                "equal to",
                "None of the above"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If the SIMD width is ______  the cache line size, a single SIMD memory instruction may require accessing multiple cache lines.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 18,
        "doc": {
            "question_id": 128509523,
            "question": "____ has full gather and scatter instruction support.",
            "options": [
                "AVX2",
                "Knights Corner",
                "AVX-512",
                "Cray-2"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ has full gather and scatter instruction support.",
                "Expected Label": "C",
                "Predicted Label": "C) AVX-512 \n"
            }
        ]
    },
    {
        "doc_id": 19,
        "doc": {
            "question_id": 128509525,
            "question": "____ are typically used in cores with out-of-order execution to enforce both data dependences and ordering restrictions on non-overlapping reads and writes.",
            "options": [
                "Cache coherence protocols",
                "Branch predictors",
                "Register renaming units",
                "Load and store buffers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are typically used in cores with out-of-order execution to enforce both data dependences and ordering restrictions on non-overlapping reads and writes.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 20,
        "doc": {
            "question_id": 128509531,
            "question": "The cost of hardware to rearrange data arbitrarily within a vector scales ____ with the length of the vector.",
            "options": [
                "linearly",
                "logarithmically",
                "quadratically",
                "exponentially"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The cost of hardware to rearrange data arbitrarily within a vector scales ____ with the length of the vector.",
                "Expected Label": "C",
                "Predicted Label": "A) linearly \n"
            }
        ]
    },
    {
        "doc_id": 21,
        "doc": {
            "question_id": 128509535,
            "question": "_____ accounts for the number of jobs completed per unit of time in a multiprogram workload.",
            "options": [
                "System throughput",
                "Normalized progress",
                "Idle loop",
                "Spin-lock loop"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "_____ accounts for the number of jobs completed per unit of time in a multiprogram workload.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) System throughput**. Here's why:\n\n* **System throughput** is a measure of the amount of work done by a system over time. In a multiprogram environment, it directly reflects how effectively the system manages and completes multiple jobs.\n\nLet's look at why the other options are incorrect:\n\n* **Normalized progress** isn't a standard term in computer architecture or performance measurement.\n* **Idle loop** and **Spin-lock loop** are mechanisms that can negatively impact throughput. They represent situations where a processor is actively waiting (and not doing useful work), thus reducing the overall number of jobs completed over time. \n"
            }
        ]
    },
    {
        "doc_id": 22,
        "doc": {
            "question_id": 128509541,
            "question": "Principal Component 2 (PC2) in PCA-based workload analysis captures ____.",
            "options": [
                "low I-cache miss rates",
                "more shift ops and less ld/st ops",
                "few dynamically executed branch instructions",
                "high levels of ILP and poor branch predictability"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Principal Component 2 (PC2) in PCA-based workload analysis captures ____.",
                "Expected Label": "D",
                "Predicted Label": "D) high levels of ILP and poor branch predictability \n"
            }
        ]
    },
    {
        "doc_id": 23,
        "doc": {
            "question_id": 128509543,
            "question": "____ modeling builds a performance model based on first principles, starting from a basic understanding of the mechanics of the underlying system.",
            "options": [
                "Mechanistic",
                "Empirical",
                "Hybrid",
                "Statistical"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ modeling builds a performance model based on first principles, starting from a basic understanding of the mechanics of the underlying system.",
                "Expected Label": "A",
                "Predicted Label": "A) Mechanistic \n"
            }
        ]
    },
    {
        "doc_id": 24,
        "doc": {
            "question_id": 128509545,
            "question": "In the context of interval modeling for superscalar out-of-order processors, the dispatch width (D) is roughly equivalent to the ____ under optimal conditions.",
            "options": [
                "cache line size",
                "instructions per cycle (IPC)",
                "branch prediction accuracy",
                "pipeline depth"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the context of interval modeling for superscalar out-of-order processors, the dispatch width (D) is roughly equivalent to the ____ under optimal conditions.",
                "Expected Label": "B",
                "Predicted Label": "B) instructions per cycle (IPC) \n\nLet's break down why:\n\n* **Dispatch Width (D):** This refers to the maximum number of instructions an out-of-order processor can send to its execution units per clock cycle. \n* **Instructions Per Cycle (IPC):**  A key performance metric measuring how many instructions, on average, are completed by the processor each clock cycle.\n\n**Why the Connection?**\n\nUnder ideal circumstances (perfect branch prediction, no data dependencies stalling execution, etc.), a superscalar processor aims to dispatch and execute instructions at its maximum capacity. This means the dispatch width becomes the limiting factor on how many instructions can be completed per cycle. \n\n**Let's eliminate the other options:**\n\n* **Cache Line Size:** While important for memory performance, it doesn't directly dictate the dispatch width.\n* **Branch Prediction Accuracy:**  Crucial for out-of-order performance, but doesn't directly translate to dispatch width.  Poor prediction hurts IPC, but not necessarily the dispatch width itself.\n* **Pipeline Depth:**  A deeper pipeline can improve clock speed but doesn't directly determine how many instructions can be dispatched per cycle.\n\n**In summary:** The dispatch width represents the processor's potential to execute instructions concurrently. Under optimal conditions, this potential translates directly to the instructions per cycle (IPC). \n"
            }
        ]
    },
    {
        "doc_id": 25,
        "doc": {
            "question_id": 128509555,
            "question": "In SimPoint, a ____ is used to represent the code executed in an interval of a program's execution.",
            "options": [
                "Basic Block Vector",
                "Random Projection",
                "Cluster Analysis",
                "Simulation Point"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In SimPoint, a ____ is used to represent the code executed in an interval of a program's execution.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nSimPoint uses **Basic Block Vectors (BBVs)** to represent the code executed within a specific interval. Here's why:\n\n* **Basic Blocks:**  A basic block is a straight sequence of code with one entry point and one exit point, making it a fundamental unit of code execution.\n* **Vectors:** BBVs use a vector format where each element corresponds to a specific basic block. The value in each element represents how many times that basic block was executed within the interval.\n\n**Why other options are incorrect:**\n\n* **Random Projection:** Used for dimensionality reduction, not directly for code representation in SimPoint.\n* **Cluster Analysis:**  A technique to group similar data points, not a primary representation method in SimPoint.\n* **Simulation Point:** While SimPoint uses the concept of simulation points to mark intervals, it's not the method used to represent the code executed within those intervals. \n"
            }
        ]
    },
    {
        "doc_id": 26,
        "doc": {
            "question_id": 128509557,
            "question": "For simulating large programs, full architecture state initialization checkpoints can reach sizes up to ____ .",
            "options": [
                "A few kilobytes",
                "Multiple gigabytes",
                "Hundreds of megabytes",
                "A few megabytes"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "For simulating large programs, full architecture state initialization checkpoints can reach sizes up to ____ .",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) Hundreds of megabytes**. \n\nHere's why:\n\n* **Full architecture state** includes everything needed to perfectly recreate the state of a processor or system. This means registers, cache contents, memory, and potentially even disk states for complex simulations.\n* **Large programs** imply substantial memory footprints and complex execution states.\n\nConsidering these factors, even a few gigabytes (option B) could be possible in extreme scenarios. However, hundreds of megabytes represents a more common upper bound for most large program simulations. \n"
            }
        ]
    },
    {
        "doc_id": 27,
        "doc": {
            "question_id": 128515316,
            "question": "_____ processors used array syntax and were especially efficient for applications like image processing due to their ability to handle an array of pixels.",
            "options": [
                "Early SIMD",
                "Cray vector processors",
                "VLIW",
                "DMM"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "_____ processors used array syntax and were especially efficient for applications like image processing due to their ability to handle an array of pixels.",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 28,
        "doc": {
            "question_id": 128515320,
            "question": "The Alliant FX/8, released in 1985, boasted a performance of 11.8 ____.",
            "options": [
                "MIPS",
                "GHz",
                "Megaflops",
                "MHz"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Alliant FX/8, released in 1985, boasted a performance of 11.8 ____.",
                "Expected Label": "C",
                "Predicted Label": "C) Megaflops \n\n**Explanation:**\n\nThe term \"flops\" (floating-point operations per second) is the standard measure of processing speed for supercomputers and high-performance computing systems.  The Alliant FX/8 was known for its floating-point performance, making Megaflops the most logical unit of measurement. \n"
            }
        ]
    },
    {
        "doc_id": 29,
        "doc": {
            "question_id": 128515322,
            "question": "____ introduced the concept of Cache Only Memory Access (COMA).",
            "options": [
                "Intel QuickPath Interconnect",
                "Nvidia SIMT",
                "Convex Exemplar",
                "KSR"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ introduced the concept of Cache Only Memory Access (COMA).",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 30,
        "doc": {
            "question_id": 128515324,
            "question": "____ clusters are DMM machines made with Commercial Off The Shelf (COTS) microprocessor boards and switches.",
            "options": [
                "Ordinary",
                "Integrated",
                "Hybrid",
                "Specialized"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ clusters are DMM machines made with Commercial Off The Shelf (COTS) microprocessor boards and switches.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 31,
        "doc": {
            "question_id": 128515326,
            "question": "With a single core running, ____ allows the core to operate at a higher frequency than with multiple cores active.",
            "options": [
                "Enhanced Speedstep",
                "Turbo Boost",
                "Speed Select",
                "FMA"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "With a single core running, ____ allows the core to operate at a higher frequency than with multiple cores active.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 32,
        "doc": {
            "question_id": 128515330,
            "question": "____ is a programming approach that combines aspects of shared and distributed memory models.",
            "options": [
                "SPMD",
                "MPI",
                "OpenMP",
                "PGAS"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a programming approach that combines aspects of shared and distributed memory models.",
                "Expected Label": "D",
                "Predicted Label": "D) PGAS \n\n**Explanation:**\n\n* **PGAS (Partitioned Global Address Space)** allows programmers to treat distributed memory as a single, large address space. This combines aspects of shared memory (easy data access) and distributed memory (scalability). \n\nLet's look at why the other options are incorrect:\n\n* **SPMD (Single Program Multiple Data):**  While common in distributed computing, SPMD is a programming model, not an approach that specifically blends shared and distributed memory concepts. \n* **MPI (Message Passing Interface):**  MPI is a standard for message passing, primarily used in distributed memory systems. It doesn't inherently combine shared memory aspects.\n* **OpenMP (Open Multi-Processing):** OpenMP is primarily designed for shared memory multiprocessing and doesn't inherently address distributed memory models. \n"
            }
        ]
    },
    {
        "doc_id": 33,
        "doc": {
            "question_id": 128515334,
            "question": "____ is an example of an FEA application that was vectorized on the Cray-1 in 1976.",
            "options": [
                "ANSYS",
                "DYNA3D",
                "NASTRAN",
                "LS-DYNA"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is an example of an FEA application that was vectorized on the Cray-1 in 1976.",
                "Expected Label": "B",
                "Predicted Label": "C) NASTRAN \n\n**Explanation:**\n\nWhile other options are FEA applications, NASTRAN (**N**ASA **St**ructural **An**alysis) was specifically vectorized for the Cray-1 in 1976, marking a significant leap in computational power for structural analysis. \n"
            }
        ]
    },
    {
        "doc_id": 34,
        "doc": {
            "question_id": 128515336,
            "question": "Computational chemistry applications typically require ____ for high performance.",
            "options": [
                "large amounts of memory",
                "extensive I/O bandwidth",
                "many processors",
                "high clock speeds"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Computational chemistry applications typically require ____ for high performance.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 35,
        "doc": {
            "question_id": 128515340,
            "question": "The machines we build in the NISQ era will be equipped with ____ qubits and are capable of performing operations with error rate around 10\u22123 or 10\u22124.",
            "options": [
                "50\u20131000",
                "1000\u20135000",
                "5000\u201310000",
                "10\u201350"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The machines we build in the NISQ era will be equipped with ____ qubits and are capable of performing operations with error rate around 10\u22123 or 10\u22124.",
                "Expected Label": "A",
                "Predicted Label": "A) 50\u20131000 \n"
            }
        ]
    },
    {
        "doc_id": 36,
        "doc": {
            "question_id": 128515342,
            "question": "The ____ is used to visualize a qubit in three dimensions.",
            "options": [
                "Quantum Sphere",
                "Bloch Sphere",
                "Qubit Globe",
                "State Orb"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ is used to visualize a qubit in three dimensions.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 37,
        "doc": {
            "question_id": 128515344,
            "question": "The ______ of a superconducting qubit is typically determined using dispersive readout, which analyzes state-dependent frequency shifts in a coupled resonator.",
            "options": [
                "coherence time",
                "energy level",
                "state",
                "anharmonicity"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ______ of a superconducting qubit is typically determined using dispersive readout, which analyzes state-dependent frequency shifts in a coupled resonator.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) state**. Here's why:\n\n* **Dispersive readout** is a technique used to determine the state of a qubit without directly measuring its energy. \n* It works by coupling the qubit to a resonator (like an LC circuit). The qubit's state (ground or excited) influences the resonator's resonant frequency.\n* By measuring the resonator's frequency shift, we can infer the **state** of the qubit.\n\nLet's look at the other options:\n\n* **A) Coherence time:** This refers to how long a qubit can maintain its superposition state. It's not directly measured by dispersive readout.\n* **B) Energy level:** While the qubit's state is related to its energy level, dispersive readout doesn't directly measure the energy level itself. It infers the state based on the frequency shift.\n* **D) Anharmonicity:** This property is important for distinguishing different energy levels in the qubit, but it's not the primary quantity determined by dispersive readout.\n\n**Therefore, the most accurate answer is (C).** \n"
            }
        ]
    },
    {
        "doc_id": 38,
        "doc": {
            "question_id": 128515346,
            "question": "In quantum computing, a black-box function is often referred to as a(n) ____.",
            "options": [
                "Gate",
                "Node",
                "Transistor",
                "Oracle"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In quantum computing, a black-box function is often referred to as a(n) ____.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 39,
        "doc": {
            "question_id": 128515348,
            "question": "____ is a proposed approach to demonstrate quantum advantage by using photons.",
            "options": [
                "Boson Sampling",
                "Random Circuit Sampling",
                "HHL Algorithm",
                "Shor's Algorithm"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a proposed approach to demonstrate quantum advantage by using photons.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 40,
        "doc": {
            "question_id": 128515350,
            "question": "In a NISQ machine, a common method to execute a long-distance two-qubit gate is to move one qubit closer to the other using a chain of ______ gates.",
            "options": [
                "CNOT",
                "SWAP",
                "Hadamard",
                "Pauli-X"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a NISQ machine, a common method to execute a long-distance two-qubit gate is to move one qubit closer to the other using a chain of ______ gates.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 41,
        "doc": {
            "question_id": 128515352,
            "question": "Relational quantum Hoare logic is developed for ____.",
            "options": [
                "circuit optimization",
                "theorem proving",
                "security protocols",
                "reversible circuits"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Relational quantum Hoare logic is developed for ____.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(D) reversible circuits**. \n\nHere's why:\n\n* **Relational quantum Hoare logic (RQHL)** is a formal verification tool used to reason about the correctness of quantum programs. \n* **Reversible circuits** are a fundamental concept in quantum computing.  Unlike classical circuits, every gate in a reversible circuit can be undone, ensuring information isn't lost (a key principle in quantum mechanics).\n\nRQHL is particularly well-suited for analyzing reversible circuits because:\n\n* **Reversibility:** RQHL can express properties related to the backward execution of quantum programs, which is essential for reversible computations.\n* **Relational Reasoning:** RQHL can reason about the relationships between different quantum states, making it possible to verify that a reversible circuit correctly transforms one state into another.\n\n**The other options are less relevant:**\n\n* **Circuit optimization:** While optimization is important, RQHL is primarily a verification tool, not an optimization technique.\n* **Theorem proving:** RQHL is a specific type of logic, not a general-purpose theorem prover.\n* **Security protocols:** While RQHL could be used in the context of security, its primary application is in verifying the correctness of quantum computations, particularly those involving reversible circuits. \n"
            }
        ]
    },
    {
        "doc_id": 42,
        "doc": {
            "question_id": 128515354,
            "question": "In quantum algorithms, ____ are typically the most resource-demanding computational components of the entire quantum circuit.",
            "options": [
                "Quantum error correction codes",
                "Qubit initialization procedures",
                "Measurement operations",
                "Reversible arithmetic subroutines"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In quantum algorithms, ____ are typically the most resource-demanding computational components of the entire quantum circuit.",
                "Expected Label": "D",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 43,
        "doc": {
            "question_id": 128515358,
            "question": "In a detected community of qubits, the KMeans clustering algorithm is used to pinpoint ____.",
            "options": [
                "interaction graph peaks",
                "the centroid of each cluster",
                "the edge betweenness score",
                "qubit decoherence times"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a detected community of qubits, the KMeans clustering algorithm is used to pinpoint ____.",
                "Expected Label": "B",
                "Predicted Label": "B) the centroid of each cluster \n\nKMeans clustering is designed to find cluster centers (centroids). \n"
            }
        ]
    },
    {
        "doc_id": 44,
        "doc": {
            "question_id": 128515360,
            "question": "____ is a technique used in some quantum architectures to reduce crosstalk by limiting the number of connections between qubits.",
            "options": [
                "Qubit frequency tuning",
                "Coupler tuning",
                "Connectivity reduction",
                "Sudoku-style patterning"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique used in some quantum architectures to reduce crosstalk by limiting the number of connections between qubits.",
                "Expected Label": "C",
                "Predicted Label": "C.\n"
            }
        ]
    },
    {
        "doc_id": 45,
        "doc": {
            "question_id": 128515364,
            "question": "Functions in C work on the property of ____.",
            "options": [
                "call-by-value",
                "call-by-reference",
                "call-by-pointer",
                "call-by-name"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Functions in C work on the property of ____.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 46,
        "doc": {
            "question_id": 128515366,
            "question": "The ____ transformation involves reorganizing data structures to improve memory access patterns.",
            "options": [
                "loop unrolling",
                "array of structs to struct of arrays",
                "function inlining",
                "branch prediction"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ transformation involves reorganizing data structures to improve memory access patterns.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 47,
        "doc": {
            "question_id": 128515368,
            "question": "A typical DRAM access takes on the order of approximately ____ of processor cycles.",
            "options": [
                "10s",
                "100s",
                "1000s",
                "10000s"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A typical DRAM access takes on the order of approximately ____ of processor cycles.",
                "Expected Label": "B",
                "Predicted Label": "B) 100s \n"
            }
        ]
    },
    {
        "doc_id": 48,
        "doc": {
            "question_id": 128515370,
            "question": "Each operation (precharge, activate, column access) takes anywhere from a few ____ to several tens of nanoseconds.",
            "options": [
                "microseconds",
                "milliseconds",
                "picoseconds",
                "nanoseconds"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Each operation (precharge, activate, column access) takes anywhere from a few ____ to several tens of nanoseconds.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 49,
        "doc": {
            "question_id": 128515372,
            "question": "____ is a common organization for DRAM command queues in memory controllers.",
            "options": [
                "Per-rank queues",
                "Per-core queues",
                "Per-thread queues",
                "Per-cache queues"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a common organization for DRAM command queues in memory controllers.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 50,
        "doc": {
            "question_id": 128515374,
            "question": "One approach to mitigating the challenges associated with high power consumption in DRAM systems involves utilizing _____ memory devices capable of self-managing their power states.",
            "options": [
                "volatile",
                "non-slave",
                "synchronous",
                "read-only"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "One approach to mitigating the challenges associated with high power consumption in DRAM systems involves utilizing _____ memory devices capable of self-managing their power states.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **A) volatile**. Here's why:\n\n* **Volatile memory** like DRAM requires a constant power supply to retain data.  Modern DRAM systems use various techniques to reduce power consumption, and allowing memory devices to self-manage their power states (going into low-power modes when idle) is a key strategy.\n\nLet's look at why the other options are incorrect:\n\n* **Non-slave** isn't a standard memory characteristic and doesn't relate to power management.\n* **Synchronous** memory operates using a clock signal, which is important for timing but not directly related to self-managed power states.\n* **Read-only** memory (ROM) inherently consumes less power as it doesn't need to constantly refresh data, making self-managed power states less relevant. \n\n**Therefore, the most accurate answer is A.** \n"
            }
        ]
    },
    {
        "doc_id": 51,
        "doc": {
            "question_id": 128515376,
            "question": "The memory latency in a system becomes many times the idle latency in the ____ region of the bandwidth-latency curve.",
            "options": [
                "constant",
                "linear",
                "exponential",
                "initial"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The memory latency in a system becomes many times the idle latency in the ____ region of the bandwidth-latency curve.",
                "Expected Label": "C",
                "Predicted Label": "C) exponential \n"
            }
        ]
    },
    {
        "doc_id": 52,
        "doc": {
            "question_id": 128515378,
            "question": "In memory system modeling, a ____ keeps track of bus utilization to ensure a set bandwidth threshold is not exceeded.",
            "options": [
                "fixed-latency model",
                "Poisson distribution",
                "prefetch buffer",
                "queueing model"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In memory system modeling, a ____ keeps track of bus utilization to ensure a set bandwidth threshold is not exceeded.",
                "Expected Label": "D",
                "Predicted Label": "D) queueing model \n\n**Explanation:**\n\nQueueing models are specifically designed to analyze and simulate systems involving queues, which perfectly describes how bus utilization is managed. By representing memory requests as arriving entities and the bus as a service facility, queueing models can estimate waiting times, queue lengths, and most importantly, **bus utilization**. \n \nLet's look at why the other options are incorrect:\n\n* **A) Fixed-latency model:** This model simplifies memory access by assuming a constant access time, neglecting the complexities of bus contention and varying request patterns.\n* **B) Poisson distribution:** This is a probability distribution used to model the arrival rate of random events, often applied to memory requests. While relevant, it doesn't directly track or control bus utilization.\n* **C) Prefetch buffer:**  This hardware mechanism aims to reduce memory latency by predicting and loading data before it's needed. It doesn't directly manage bus bandwidth thresholds. \n\nLet me know if you'd like more detail on queueing models or any other aspect of computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 53,
        "doc": {
            "question_id": 128515384,
            "question": "As system complexity increases, ____ models are required to evaluate performance accurately.",
            "options": [
                "simpler",
                "faster",
                "more detailed",
                "less precise"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "As system complexity increases, ____ models are required to evaluate performance accurately.",
                "Expected Label": "C",
                "Predicted Label": "C) more detailed \n"
            }
        ]
    },
    {
        "doc_id": 54,
        "doc": {
            "question_id": 128515392,
            "question": "Core fusion allows a system to scale by splitting a core into a narrow-issue core and an additional component that acts as a modular ____ component.",
            "options": [
                "Execute",
                "Issue",
                "Fetch/decode/commit",
                "Retire"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Core fusion allows a system to scale by splitting a core into a narrow-issue core and an additional component that acts as a modular ____ component.",
                "Expected Label": "C",
                "Predicted Label": "B) Issue \n\n**Explanation:**\n\nCore fusion enhances performance by dividing a traditional core into:\n\n* **Narrow-issue core:** Handles simpler instructions efficiently.\n* **Modular issue component:** Takes on the complex task of issuing instructions, especially those out-of-order or with dependencies. This offloads the main core, allowing it to focus on execution.\n\nLet me know if you'd like more details on core fusion! \n"
            }
        ]
    },
    {
        "doc_id": 55,
        "doc": {
            "question_id": 128515396,
            "question": "____ is a primary advantage of using software-based arbitration for managing access to Loosely Coupled Accelerators (LCAs) shared among multiple cores.",
            "options": [
                "Flexibility in implementing complex scheduling policies",
                "Lower latency for accelerator invocation",
                "Reduced system complexity",
                "Improved energy efficiency"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a primary advantage of using software-based arbitration for managing access to Loosely Coupled Accelerators (LCAs) shared among multiple cores.",
                "Expected Label": "A",
                "Predicted Label": "A) Flexibility in implementing complex scheduling policies \n\nLet's break down why:\n\n* **Flexibility in implementing complex scheduling policies (A):**  Software-based arbitration allows for dynamic and intricate scheduling algorithms.  This is crucial for LCAs where different cores might have varying priorities or require specific access patterns to the accelerator.\n\n* **Lower latency for accelerator invocation (B):** Software arbitration generally introduces *higher* latency compared to hardware solutions due to the overhead of software layers and potential context switching.\n\n* **Reduced system complexity (C):** While software arbitration might simplify the accelerator's hardware design, it often increases overall system complexity due to the need for software management and potential synchronization issues.\n\n* **Improved energy efficiency (D):**  Software arbitration doesn't inherently guarantee energy efficiency.  It can even be less efficient than hardware solutions if not carefully optimized.\n\n**In summary:** The primary advantage of software-based arbitration for LCAs is the ability to implement flexible and complex scheduling policies to optimize resource utilization among cores. \n"
            }
        ]
    },
    {
        "doc_id": 56,
        "doc": {
            "question_id": 128515402,
            "question": "____ is a technique that can be used to disable a subset of ways in a set-associative cache to reduce leakage energy.",
            "options": [
                "Cache decay",
                "Drowsy cache",
                "Resizable i-cache",
                "Selective cache ways"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a technique that can be used to disable a subset of ways in a set-associative cache to reduce leakage energy.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 57,
        "doc": {
            "question_id": 128515404,
            "question": "The maximum size of a configurable scratchpad memory within an M-way N-set set-associative cache, ensuring at least one cache block per set for cache functionality, is ____ blocks.",
            "options": [
                "(M-1)*N",
                "M*N",
                "(M-1)/(N-1)",
                "M/(N-1)"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The maximum size of a configurable scratchpad memory within an M-way N-set set-associative cache, ensuring at least one cache block per set for cache functionality, is ____ blocks.",
                "Expected Label": "A",
                "Predicted Label": "A) (M-1)*N \n\n**Explanation:**\n\n* **M-way set associative:** Each set in the cache can hold M cache blocks.\n* **N sets:** The cache is divided into N sets.\n\nTo have at least one cache block per set dedicated to caching, we can only use (M-1) blocks per set for the scratchpad.  Therefore, the maximum size of the scratchpad is:\n\n(M-1) blocks/set * N sets = **(M-1)*N blocks** \n"
            }
        ]
    },
    {
        "doc_id": 58,
        "doc": {
            "question_id": 128515408,
            "question": "In hybrid cache architectures, ____ is often used to monitor whether a block in the fast region was recently reused.",
            "options": [
                "Saturation counters",
                "Swap buffers",
                "Sticky bits",
                "Tag arrays"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In hybrid cache architectures, ____ is often used to monitor whether a block in the fast region was recently reused.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Sticky bits** are a simple and effective mechanism used in hybrid cache architectures (like combining SRAM and DRAM caches). When a block in the fast cache (e.g., SRAM) is accessed, its sticky bit is set. If the block remains unused for a period, the sticky bit is cleared.  This helps determine which blocks are frequently used and should remain in the faster cache.\n\nLet's look at why the other options aren't the best fit:\n\n* **Saturation counters:** These are more commonly used in replacement policies (like LRU approximations) to track how frequently blocks are used, not specifically for monitoring recent reuse in hybrid caches.\n* **Swap buffers:** These are temporary storage areas used during data swapping processes and are not directly related to cache monitoring.\n* **Tag arrays:** While tag arrays are a fundamental part of cache organization for identifying blocks, they don't inherently track recent reuse patterns. \n"
            }
        ]
    },
    {
        "doc_id": 59,
        "doc": {
            "question_id": 128515412,
            "question": "In dark silicon scenarios, what type of architectural approach is increasingly considered due to the excess of transistors?",
            "options": [
                "Specialized accelerators",
                "Increased core frequency",
                "More general-purpose cores",
                "Enhanced cooling systems"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In dark silicon scenarios, what type of architectural approach is increasingly considered due to the excess of transistors?",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Specialized accelerators**.\n\nHere's why:\n\n* **Dark silicon** refers to the trend where we have more transistors available than we can power and use efficiently. Simply adding more general-purpose cores or increasing frequency becomes impractical due to power constraints.\n\n* **Specialized accelerators** are designed for specific tasks (like AI processing, graphics rendering, or encryption). This specialization allows for much greater efficiency than general-purpose cores for those tasks, making better use of the available transistors within the power budget.\n\n**Let's look at the other options:**\n\n* **B) Increased core frequency:**  Increasing frequency directly increases power consumption, exacerbating the dark silicon problem.\n* **C) More general-purpose cores:** While this seems like a natural progression, it becomes inefficient under dark silicon conditions as all cores might not be utilized effectively, wasting power. \n* **D) Enhanced cooling systems:** While better cooling helps manage heat, it doesn't address the fundamental issue of power efficiency and making the most of available transistors. \n\n**Therefore, specialized accelerators are the most suitable architectural approach to address the challenges posed by dark silicon.** \n"
            }
        ]
    },
    {
        "doc_id": 60,
        "doc": {
            "question_id": 128515414,
            "question": "Dynamic voltage and frequency scaling (DVFS) techniques primarily focus on reducing ____ power consumption in processors.",
            "options": [
                "static",
                "dynamic",
                "leakage",
                "standby"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Dynamic voltage and frequency scaling (DVFS) techniques primarily focus on reducing ____ power consumption in processors.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 61,
        "doc": {
            "question_id": 128515418,
            "question": "The ____ model assumes that the number of stall cycles is approximately equivalent to the memory latency for both isolated and overlapping misses.",
            "options": [
                "Leading Loads",
                "IQ Drain",
                "ROB-fill",
                "Stall-based"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ model assumes that the number of stall cycles is approximately equivalent to the memory latency for both isolated and overlapping misses.",
                "Expected Label": "D",
                "Predicted Label": "D) Stall-based \n"
            }
        ]
    },
    {
        "doc_id": 62,
        "doc": {
            "question_id": 128515420,
            "question": "____ is a component in the Linux kernel that handles Dynamic Voltage and Frequency Scaling.",
            "options": [
                "cpufreq",
                "PowerSleuth",
                "ScarPhase",
                "SpeedStep"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a component in the Linux kernel that handles Dynamic Voltage and Frequency Scaling.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 63,
        "doc": {
            "question_id": 128515426,
            "question": "____ is a technique that can be used to reduce leakage energy at smaller feature sizes in response to Dark Silicon trends.",
            "options": [
                "3D integrated circuits",
                "Homogeneous on-chip parallelism",
                "Dynamic voltage and frequency scaling",
                "Silicon on insulator"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a technique that can be used to reduce leakage energy at smaller feature sizes in response to Dark Silicon trends.",
                "Expected Label": "D",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 64,
        "doc": {
            "question_id": 128515436,
            "question": "Transactional memory implementations often use one of the following forms of versioning: eager versioning or ______.",
            "options": [
                "lazy versioning",
                "delayed versioning",
                "immediate versioning",
                "planned versioning"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Transactional memory implementations often use one of the following forms of versioning: eager versioning or ______.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 65,
        "doc": {
            "question_id": 128515438,
            "question": "____ version management in transactional memory systems involves directly modifying data in memory while maintaining an undo-log for potential aborts.",
            "options": [
                "Lazy",
                "Eager",
                "Deferred",
                "Indirect"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ version management in transactional memory systems involves directly modifying data in memory while maintaining an undo-log for potential aborts.",
                "Expected Label": "B",
                "Predicted Label": "**A) Lazy** \n\n**Explanation:**\n\nLazy version management (also known as optimistic version management) allows transactions to directly modify data in memory. It maintains an undo-log to revert changes if the transaction aborts. This approach optimizes for successful transactions as it avoids unnecessary copying overhead. \n\nLet me know if you'd like more details on transactional memory! \n"
            }
        ]
    },
    {
        "doc_id": 66,
        "doc": {
            "question_id": 128515440,
            "question": "The problem where transactional writes interfere with non-transactional accesses, even after a transaction commits or aborts, is known as the ____ problem.",
            "options": [
                "Granular inconsistent read",
                "Racy publication",
                "Delayed cleanup",
                "Memory inconsistency"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The problem where transactional writes interfere with non-transactional accesses, even after a transaction commits or aborts, is known as the ____ problem.",
                "Expected Label": "C",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Racy publication** occurs when a transaction commits its writes, but another thread reads a mix of old and new values because it accessed data structures being modified by the transaction before the transaction had fully published all its changes.\n\nLet's break down why the other options are incorrect:\n\n* **Granular inconsistent read (A):**  This refers to reading inconsistent data within the *same* structure due to partial updates, not necessarily due to transactional interference.\n* **Delayed cleanup (C):** While related to transactions, this refers to resources held by a transaction not being released promptly after its completion, potentially causing performance issues, but not directly the described inconsistency.\n* **Memory inconsistency (D):** This is a broader term encompassing various consistency issues, not specifically the interference between transactional and non-transactional accesses after a transaction completes. \n\n**Therefore, 'Racy Publication (B)' is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 67,
        "doc": {
            "question_id": 128515442,
            "question": "____ is responsible for causing threads to back off temporarily to ensure liveness in obstruction-free algorithms.",
            "options": [
                "Lock manager",
                "Progress monitor",
                "Thread scheduler",
                "Contention manager"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is responsible for causing threads to back off temporarily to ensure liveness in obstruction-free algorithms.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 68,
        "doc": {
            "question_id": 128515444,
            "question": "____ is a mechanism used to delimit code that should execute atomically and in isolation from other threads in transactional memory systems.",
            "options": [
                "Atomic blocks",
                "Critical sections",
                "Mutex locks",
                "Semaphores"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a mechanism used to delimit code that should execute atomically and in isolation from other threads in transactional memory systems.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile other options deal with concurrency, **Atomic blocks (A)** are specifically designed for transactional memory systems. Here's why:\n\n* **Atomic blocks** ensure that all operations within the block complete successfully or none of them do. This all-or-nothing execution is fundamental to transactional memory's approach.\n* **Critical sections (B), Mutex locks (C), and Semaphores (D)** are traditional synchronization primitives used in shared-memory concurrency but don't inherently provide the atomicity and isolation guarantees of transactional memory. \n"
            }
        ]
    },
    {
        "doc_id": 69,
        "doc": {
            "question_id": 128515446,
            "question": "A key advantage of the 'orElse' operation in transactional programming is its ability to adapt ____ operations into those that return a failure code.",
            "options": [
                "Atomic",
                "Blocking",
                "Concurrent",
                "Non-blocking"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A key advantage of the 'orElse' operation in transactional programming is its ability to adapt ____ operations into those that return a failure code.",
                "Expected Label": "B",
                "Predicted Label": "D) Non-blocking \n\n**Explanation:**\n\nThe `orElse` operation is commonly used in scenarios where you want to provide an alternative path of execution if a particular operation fails in a non-blocking manner.  Here's why:\n\n* **Non-blocking operations** are designed to return immediately, even if the requested operation cannot be completed right away. They typically return a future or promise, allowing the program to continue with other tasks. \n* **`orElse`** allows you to specify an alternative action to be taken if the initial operation represented by the future or promise fails. This failure handling happens gracefully without blocking the main thread of execution.\n\nLet me know if you'd like more details on transactional programming or non-blocking operations! \n"
            }
        ]
    },
    {
        "doc_id": 70,
        "doc": {
            "question_id": 128515448,
            "question": "Transaction processing monitors generally use a ____ commit protocol.",
            "options": [
                "single-phase",
                "three-phase",
                "two-phase",
                "multi-phase"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Storage Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Transaction processing monitors generally use a ____ commit protocol.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 71,
        "doc": {
            "question_id": 128515450,
            "question": "____ Transactional Memory systems typically have fewer intrinsic limitations imposed by fixed-size hardware structures compared to their hardware counterparts.",
            "options": [
                "Hybrid",
                "Distributed",
                "Hardware",
                "Software"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ Transactional Memory systems typically have fewer intrinsic limitations imposed by fixed-size hardware structures compared to their hardware counterparts.",
                "Expected Label": "D",
                "Predicted Label": "**D) Software**\n\n**Explanation:**\n\nSoftware Transactional Memory (STM) systems are generally more flexible than their Hardware Transactional Memory (HTM) counterparts because they are implemented in software and are not bound by the physical constraints of hardware structures. This allows STM to handle larger data structures and more complex transactions, which might be limited by the fixed-size buffers and cache lines often used in HTM. \n"
            }
        ]
    },
    {
        "doc_id": 72,
        "doc": {
            "question_id": 128515452,
            "question": "The ______ operation in a Software Transactional Memory (STM) system is responsible for obtaining a descriptor representing the active transaction for a thread.",
            "options": [
                "GetTxDescriptor",
                "OpenForWriteTx",
                "LogForUndoIntTx",
                "IsZero"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ______ operation in a Software Transactional Memory (STM) system is responsible for obtaining a descriptor representing the active transaction for a thread.",
                "Expected Label": "A",
                "Predicted Label": "**A) GetTxDescriptor** \n\n**Explanation:**\n\nThe other options are not standard operations within the context of STM:\n\n* **B) OpenForWriteTx:**  While STM systems manage write operations, \"OpenForWriteTx\" isn't a typical operation name.  Operations usually focus on beginning, validating, or committing transactions.\n* **C) LogForUndoIntTx:** This suggests an internal logging mechanism for potential rollbacks, which is implementation-specific and not a fundamental STM operation.\n* **D) IsZero:**  This is a generic comparison operation and doesn't relate to the core functionality of managing transactional descriptors in STM. \n\nLet me know if you'd like a more in-depth explanation of Software Transactional Memory! \n"
            }
        ]
    },
    {
        "doc_id": 73,
        "doc": {
            "question_id": 128515454,
            "question": "The ______ phase in NOrec's commit sequence occurs without needing to acquire the lock.",
            "options": [
                "write-back",
                "validation",
                "conflict-checking",
                "initialization"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ______ phase in NOrec's commit sequence occurs without needing to acquire the lock.",
                "Expected Label": "B",
                "Predicted Label": "**B) validation**\n\n**Explanation:**\n\nIn the NOrec (Non-blocking Ordered Record) protocol, the validation phase checks if a transaction's read set is still valid. This check can be performed without acquiring the lock, as it only involves reading the timestamps of the accessed data items. \n\nHere's why the other options are incorrect:\n\n* **A) write-back:**  Write-back involves actually writing changes to memory, which would require acquiring the lock.\n* **C) conflict-checking:** Conflict checking determines if two transactions have accessed the same data, which would likely require some form of locking or synchronization.\n* **D) initialization:**  Initialization sets up the transaction, and while it may not require the lock during setup, it's not the phase that specifically avoids it by design like validation. \n"
            }
        ]
    },
    {
        "doc_id": 74,
        "doc": {
            "question_id": 128515456,
            "question": "____ is a technique used in software transactional memory to integrate non-transactional features by allowing a designated transaction to run without the possibility of being rolled back.",
            "options": [
                "Versioning",
                "Condition synchronization",
                "Irrevocability",
                "Validation"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique used in software transactional memory to integrate non-transactional features by allowing a designated transaction to run without the possibility of being rolled back.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 75,
        "doc": {
            "question_id": 128515458,
            "question": "Jensen et al.'s optimistic synchronization proposal, a precursor to modern HTM systems, allowed programmers to monitor one memory location for conflicts and ____ another location only if no conflict occurred.",
            "options": [
                "read from",
                "write to",
                "allocate",
                "deallocate"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Jensen et al.'s optimistic synchronization proposal, a precursor to modern HTM systems, allowed programmers to monitor one memory location for conflicts and ____ another location only if no conflict occurred.",
                "Expected Label": "B",
                "Predicted Label": "B) write to \n"
            }
        ]
    },
    {
        "doc_id": 76,
        "doc": {
            "question_id": 128884066,
            "question": "Deep learning methods fundamentally trace their origins to _____ neural networks.",
            "options": [
                "computational analogs of biological",
                "abstract mathematical models",
                "high-level programming languages",
                "quantum computing-based"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Deep learning methods fundamentally trace their origins to _____ neural networks.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 77,
        "doc": {
            "question_id": 128884070,
            "question": "The values for ____ in a deep neural network change based on the data provided during training.",
            "options": [
                "activation functions",
                "neuron weights",
                "number of layers",
                "loss functions"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The values for ____ in a deep neural network change based on the data provided during training.",
                "Expected Label": "B",
                "Predicted Label": "B) neuron weights \n\n**Explanation:**\n\n* **Neuron weights** are the parameters that a neural network adjusts during training to minimize the difference between its predictions and the actual target values.  The training process iteratively modifies these weights to learn the optimal representation from the data.\n\nLet's look at the other options:\n\n* **Activation functions** introduce non-linearity into the network, but their form (e.g., ReLU, sigmoid) is usually predefined and doesn't change during training.\n* **Number of layers** is a hyperparameter, meaning it's set before training and typically doesn't change during the training process.\n* **Loss functions** are used to measure the error between predictions and actual values. While the value of the loss function changes during training, the function itself (e.g., mean squared error, cross-entropy) is also typically predefined. \n \nLet me know if you have any other questions! \n"
            }
        ]
    },
    {
        "doc_id": 78,
        "doc": {
            "question_id": 128884072,
            "question": "____ can mitigate the vanishing gradient problem by allowing gradients to grow above 1.",
            "options": [
                "Sigmoid",
                "tanh",
                "Residual Networks",
                "ReLU"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ can mitigate the vanishing gradient problem by allowing gradients to grow above 1.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 79,
        "doc": {
            "question_id": 128884076,
            "question": "AlexNet introduced ____ as a regularization mechanism.",
            "options": [
                "experience replay",
                "dropout",
                "convolutional layers",
                "minibatches"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "AlexNet introduced ____ as a regularization mechanism.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 80,
        "doc": {
            "question_id": 128884086,
            "question": "____ is often considered one of the most significant challenges in neural network hardware design due to its high cost.",
            "options": [
                "Computation",
                "Model support",
                "Data movement",
                "Fault tolerance"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is often considered one of the most significant challenges in neural network hardware design due to its high cost.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 81,
        "doc": {
            "question_id": 128884088,
            "question": "The _____  effort by Microsoft explores the use of FPGAs for data center applications, including neural networks.",
            "options": [
                "Tensor Processing Unit (TPU)",
                "Random Edge Removal (RER)",
                "Dark Knowledge",
                "Catapult"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The _____  effort by Microsoft explores the use of FPGAs for data center applications, including neural networks.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 82,
        "doc": {
            "question_id": 128884090,
            "question": "The software responsible for choosing which thread to execute when is known as a __.",
            "options": [
                "scheduler",
                "dispatcher",
                "executor",
                "manager"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The software responsible for choosing which thread to execute when is known as a __.",
                "Expected Label": "A",
                "Predicted Label": "A) scheduler \n"
            }
        ]
    },
    {
        "doc_id": 83,
        "doc": {
            "question_id": 128884092,
            "question": "In computer architecture, ____ refers to the property that all accesses to a given memory location appear in some global total order.",
            "options": [
                "Global ordering",
                "Coherence",
                "Program order",
                "Local order"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ refers to the property that all accesses to a given memory location appear in some global total order.",
                "Expected Label": "A",
                "Predicted Label": "B\n \n\n**Coherence** ensures that all accesses to a given memory location, regardless of the processor or cache making the access, appear in a consistent order.  \n\nLet's break down why the other options are incorrect:\n\n* **Global ordering (A):** While related to ordering, it's a broader term. Coherence is a specific type of ordering related to shared memory locations.\n* **Program order (C):** Refers to the order in which instructions appear within a single program's execution flow. \n* **Local order (D):**  Refers to the order of operations within a specific processor or cache, not across a shared memory system. \n"
            }
        ]
    },
    {
        "doc_id": 84,
        "doc": {
            "question_id": 128884098,
            "question": "____ is considered the simplest known 2-thread spin lock algorithm.",
            "options": [
                "Peterson's algorithm",
                "Dekker's algorithm",
                "Lamport's bakery algorithm",
                "Dijkstra's algorithm"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered the simplest known 2-thread spin lock algorithm.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 85,
        "doc": {
            "question_id": 128884100,
            "question": "In the CLH lock, each thread spins on a field located in ____.",
            "options": [
                "its own queue node",
                "its predecessor's queue node",
                "the shared lock variable",
                "a separate spinlock structure"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the CLH lock, each thread spins on a field located in ____.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 86,
        "doc": {
            "question_id": 128884102,
            "question": "An elimination tree can be used to build a stack that is ____ and quiescently consistent.",
            "options": [
                "linearizable",
                "block",
                "nonblocking",
                "contention-free"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "An elimination tree can be used to build a stack that is ____ and quiescently consistent.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) nonblocking**. Here's why:\n\n* **Elimination Trees** are designed to offer high concurrency. They achieve this by allowing concurrent operations to \"meet\" and potentially satisfy each other without always reaching a single, centralized point of contention.\n\nLet's break down the options:\n\n* **A) linearizable:** Linearizability guarantees a strong order of operations, making it appear as if each operation happened instantaneously. While desirable, elimination trees prioritize high concurrency over strict linearizability.\n* **B) block:**  \"Block\" usually refers to a contiguous chunk of memory. This isn't directly relevant to the properties of a stack built with an elimination tree.\n* **D) contention-free:** Contention-free means operations can always proceed without waiting.  Elimination trees significantly reduce contention but might not eliminate it entirely in all scenarios.\n\n**Quiescently Consistent:**  This means the data structure eventually reaches a consistent state when operations cease. Elimination trees can ensure this because, after a period of no operations, all pending eliminations will have resolved. \n\n**In summary:** Elimination trees are known for creating **nonblocking**, quiescently consistent stacks that offer excellent concurrency by reducing contention. \n"
            }
        ]
    },
    {
        "doc_id": 87,
        "doc": {
            "question_id": 128884104,
            "question": "In Read-Copy Update (RCU), ____ is used to determine when all old readers have completed their operations.",
            "options": [
                "A global lock",
                "A read-write semaphore",
                "A thread-local storage mechanism",
                "A grace period mechanism"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In Read-Copy Update (RCU), ____ is used to determine when all old readers have completed their operations.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 88,
        "doc": {
            "question_id": 128884106,
            "question": "A ____ signal idiom can be employed in the absence of broadcast operations where only one thread is initially awoken and if unable to proceed, it explicitly re-signals the condition variable before waiting on it again.",
            "options": [
                "cascading",
                "recursive",
                "nested",
                "parallel"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ signal idiom can be employed in the absence of broadcast operations where only one thread is initially awoken and if unable to proceed, it explicitly re-signals the condition variable before waiting on it again.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 89,
        "doc": {
            "question_id": 128884108,
            "question": "A type-preserving allocator ensures that a block of memory is reused only for an object of the same type and ____.",
            "options": [
                "allocation pattern",
                "alignment",
                "size",
                "usage"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A type-preserving allocator ensures that a block of memory is reused only for an object of the same type and ____.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) alignment**. Here's why:\n\n* **Type-preserving allocators** are used to optimize memory management in situations where you frequently allocate and deallocate objects of the same type. \n\n* **Alignment** is crucial in these scenarios because:\n    * Different data types often have different alignment requirements (e.g., a `double` might need to be aligned on an 8-byte boundary).\n    * Reusing a block with the correct alignment avoids costly realignment operations, improving performance.\n\n**Let's look at why the other options are less suitable:**\n\n* **A) allocation pattern:** While allocation patterns can be optimized, a type-preserving allocator's primary focus is on type and the implications for alignment.\n* **C) size:**  While size is important, a type-preserving allocator might handle objects of the same type but slightly different sizes (e.g., a string with varying lengths). Alignment remains critical even with size variations.\n* **D) usage:**  Usage patterns are broader than what a type-preserving allocator directly manages. It's concerned with type and the resulting alignment requirements. \n\n**In summary, alignment is the most critical factor that a type-preserving allocator ensures alongside reusing memory for the same type.** \n"
            }
        ]
    },
    {
        "doc_id": 90,
        "doc": {
            "question_id": 128884110,
            "question": "____ is a common data structure used in parallel programming to manage tasks for worker threads.",
            "options": [
                "Stack",
                "Tree",
                "Deque",
                "Graph"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a common data structure used in parallel programming to manage tasks for worker threads.",
                "Expected Label": "C",
                "Predicted Label": "C) Deque \n\n**Explanation:**\n\n* **Deque (Double-ended queue)** is well-suited for managing tasks in parallel programming because threads can efficiently add or remove tasks from both ends. This flexibility is crucial for load balancing and task scheduling in a multi-threaded environment.\n\nLet's consider why the other options are less suitable:\n\n* **Stack (A):** Stacks operate on a Last-In-First-Out (LIFO) principle, which might not be ideal for distributing tasks among threads effectively.\n* **Tree (B) and Graph (D):**  While useful for representing hierarchical relationships and connections, trees and graphs are generally more complex than necessary for basic task management in parallel programming. \n\n**In summary:** Deques provide a simple and efficient way to hold and distribute tasks among worker threads, making them a common choice in parallel programming scenarios. \n"
            }
        ]
    },
    {
        "doc_id": 91,
        "doc": {
            "question_id": 128884114,
            "question": "____ simulation predicts the performance and timing behaviors of the target.",
            "options": [
                "Performance",
                "Functional",
                "Cycle",
                "Structural"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ simulation predicts the performance and timing behaviors of the target.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Performance**. Here's why:\n\n* **Performance Simulation** focuses on analyzing and predicting the speed, throughput, latency, and other performance-related aspects of a computer system design. It goes beyond just verifying if the design works and delves into how well it performs.\n\nLet's look at why the other options are not the best fit:\n\n* **Functional Simulation** verifies the logical correctness of a design. It ensures that the system performs the intended operations but doesn't focus on timing or performance aspects.\n* **Cycle Simulation (or Cycle-Accurate Simulation)** is a type of performance simulation but is more specific. It tracks the behavior of the system at each clock cycle, making it highly accurate but also computationally expensive. \n* **Structural Simulation** isn't a standard term in computer architecture.\n\n**Therefore, \"Performance Simulation\" is the most accurate and comprehensive answer to the question.** \n"
            }
        ]
    },
    {
        "doc_id": 92,
        "doc": {
            "question_id": 128884116,
            "question": "____ partitioning divides a simulator based on the structural components of the target system.",
            "options": [
                "Temporal",
                "Spatial",
                "Functional",
                "Logical"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ partitioning divides a simulator based on the structural components of the target system.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) Functional**. Here's why:\n\n* **Functional partitioning** in simulation means dividing the simulation workload based on the *functions* of the system being modeled.  Each \"partition\" simulates a specific part of the system's behavior.  \n\nLet's look at why the other options are incorrect:\n\n* **Temporal partitioning** divides the simulation based on *time* steps.\n* **Spatial partitioning** divides the simulation based on *physical* areas or regions.\n* **Logical partitioning** is a more general term and could refer to various ways of dividing a system, making it less specific to simulation and not necessarily tied to structural components. \n\n**In summary:** Functional partitioning aligns directly with simulating different parts of a system based on their specific tasks or operations, making it the most suitable answer. \n"
            }
        ]
    },
    {
        "doc_id": 93,
        "doc": {
            "question_id": 128884120,
            "question": "A monolithic simulator typically requires every register in the design to be instantiated and correctly written on each ____.",
            "options": [
                "microsecond",
                "nanosecond",
                "instruction cycle",
                "target cycle"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A monolithic simulator typically requires every register in the design to be instantiated and correctly written on each ____.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **(D) target cycle**. \n\nHere's why:\n\n* **Monolithic simulators** simulate the entire system at a very low level. \n* **Target cycle** refers to the smallest unit of time simulated by the simulator. \n\nFor every target cycle, a monolithic simulator needs to evaluate the logic of the entire design, including updating the state of every register.\n\nLet's look at the other options:\n\n* **Microsecond & Nanosecond:** These are units of time and are not directly tied to the simulator's operation granularity.\n* **Instruction cycle:** While important in computer architecture,  a monolithic simulator might simulate many smaller events within a single instruction cycle. \n\n**Therefore, 'target cycle' is the most accurate term in this context.** \n"
            }
        ]
    },
    {
        "doc_id": 94,
        "doc": {
            "question_id": 128884124,
            "question": "A-Ports, used in some computer architecture simulators, require activity every ____ cycle to maintain synchronization.",
            "options": [
                "clock",
                "target",
                "instruction",
                "pipeline"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A-Ports, used in some computer architecture simulators, require activity every ____ cycle to maintain synchronization.",
                "Expected Label": "B",
                "Predicted Label": "A) clock \n\n**Explanation:**\n\nPorts in simulators often rely on a synchronized clock signal to ensure data consistency. Each clock cycle represents a discrete time step where data transfer or communication through ports can occur. \n"
            }
        ]
    },
    {
        "doc_id": 95,
        "doc": {
            "question_id": 128884126,
            "question": "In timing-directed simulators, implementing the _____ partition on an FPGA consumes resources proportional to the complexity of the instruction set architecture.",
            "options": [
                "timing",
                "memory",
                "functional",
                "synchronization"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In timing-directed simulators, implementing the _____ partition on an FPGA consumes resources proportional to the complexity of the instruction set architecture.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) functional**. Here's why:\n\n* **Timing-directed simulators** aim to model the timing behavior of a system. They are concerned with when events happen but don't necessarily execute the full functionality of every instruction.\n\n* **Functional Partition:** This part of the simulator is responsible for interpreting the instructions and determining their effect on the system's state. The complexity of this partition directly correlates to the complexity of the instruction set architecture (ISA). A more complex ISA means more instructions, addressing modes, and potential operations to simulate.\n\nLet's look at why the other options are less suitable:\n\n* **Timing Partition:** Focuses on modeling delays, clock cycles, and signal propagation. While important for timing-directed simulation, its resource usage is less directly tied to ISA complexity.\n* **Memory Partition:** Simulates memory systems (cache, RAM). Resource usage here depends more on memory size and organization than ISA complexity.\n* **Synchronization Partition:**  Handles communication and synchronization between different parts of the simulated system. Its complexity is more related to the system's overall architecture and communication protocols than the ISA itself. \n\n**In summary:** The functional partition is where the simulator decodes and processes instructions. Its resource requirements are most directly impacted by the complexity of the instruction set being simulated. \n"
            }
        ]
    },
    {
        "doc_id": 96,
        "doc": {
            "question_id": 128884128,
            "question": "____ synchronization schemes are typically more scalable for large numbers of events in FPGA-accelerated simulators.",
            "options": [
                "Centralized",
                "Hierarchical",
                "Round-robin",
                "Decentralized"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ synchronization schemes are typically more scalable for large numbers of events in FPGA-accelerated simulators.",
                "Expected Label": "D",
                "Predicted Label": "D) Decentralized \n\nHere's why:\n\n* **Decentralized synchronization** is crucial for scalability in FPGA-accelerated simulators.  These systems often involve many parallel processing elements. Decentralized schemes allow events to be synchronized locally within smaller groups, reducing the communication overhead and potential bottlenecks that arise from a central authority managing everything.\n\nLet's look at why the other options aren't as suitable:\n\n* **Centralized Synchronization (A):**  A single point of control becomes a bottleneck as the number of events grows, hindering scalability.\n* **Hierarchical Synchronization (B):** While better than purely centralized, it can still suffer from bottlenecks at higher levels of the hierarchy.\n* **Round-robin Synchronization (C):** Not generally suitable for event synchronization in complex simulations. Round-robin is more about fair resource allocation. \n"
            }
        ]
    },
    {
        "doc_id": 97,
        "doc": {
            "question_id": 128884130,
            "question": "Software-based full-system simulations, when instrumented, can experience slowdowns of ____ or more.",
            "options": [
                "10X",
                "2X",
                "50X",
                "100X"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Software-based full-system simulations, when instrumented, can experience slowdowns of ____ or more.",
                "Expected Label": "A",
                "Predicted Label": "D) 100X \n\nSoftware-based full-system simulations can experience extremely high slowdowns when instrumented, often reaching **100X** or even more. This is because they need to simulate every aspect of the hardware, and instrumentation adds further overhead. \n"
            }
        ]
    },
    {
        "doc_id": 98,
        "doc": {
            "question_id": 128884132,
            "question": "Hierarchical simulation profiles behaviors to determine which components to accelerate, starting from _________.",
            "options": [
                "FPGA simulations",
                "a software-based full-system simulator",
                "a hardware-based model",
                "a hybrid accelerator"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Hierarchical simulation profiles behaviors to determine which components to accelerate, starting from _________.",
                "Expected Label": "B",
                "Predicted Label": "B) a software-based full-system simulator \n\n**Explanation:**\n\nHierarchical simulation starts with a high-level, software-based simulator that models the entire system. This allows for early exploration of different architectural choices and their impact on performance. As the design progresses, more detailed simulations are used for specific components, potentially using FPGAs or hardware-based models. \n"
            }
        ]
    },
    {
        "doc_id": 99,
        "doc": {
            "question_id": 128884134,
            "question": "In computer architecture simulation, ____ allows designers to omit rare behaviors from FPGA implementations, simplifying the design process.",
            "options": [
                "Pipelining",
                "Multithreading",
                "Hierarchical simulation",
                "Virtualization"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In computer architecture simulation, ____ allows designers to omit rare behaviors from FPGA implementations, simplifying the design process.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Hierarchical simulation** allows for different levels of abstraction within a simulated system. Designers can choose to simulate certain components at a very detailed level (e.g., cycle-accurate) while abstracting away others. This allows them to focus on specific areas of the design and omit simulating rare behaviors that might unnecessarily complicate the FPGA implementation. \n\nHere's why the other options are not the best fit:\n\n* **A) Pipelining:** A technique to improve instruction throughput, not related to simplifying simulation by omitting rare behaviors.\n* **B) Multithreading:** Allows multiple instruction streams to execute concurrently, not directly related to simulation simplification.\n* **D) Virtualization:**  Enables running multiple operating systems on a single physical machine, not focused on hardware design simulation. \n"
            }
        ]
    },
    {
        "doc_id": 100,
        "doc": {
            "question_id": 128884138,
            "question": "In the context of interconnection networks, collective operations like global sum are commonplace in ______ applications.",
            "options": [
                "supercomputing",
                "cloud computing",
                "database",
                "multimedia"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In the context of interconnection networks, collective operations like global sum are commonplace in ______ applications.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) supercomputing**. Here's why:\n\n* **Collective operations** involve coordinated communication and computation across multiple processing units.  Global sum, where all nodes contribute to calculating a single sum, is a prime example.\n\n* **Supercomputing** applications frequently rely on massive parallelism to solve complex problems (think climate modeling, drug discovery).  Collective operations are essential for these tasks to aggregate results efficiently across thousands of processors.\n\nLet's examine why the other options are less suitable:\n\n* **Cloud computing (B):** While cloud systems can benefit from collective operations, their focus is broader than supercomputing, often emphasizing data storage and web services.\n* **Database (C):** Databases prioritize efficient data storage and retrieval. While some operations might resemble collective operations, they are tailored to database-specific tasks.\n* **Multimedia (D):** Multimedia applications often focus on real-time processing of audio and video, where the need for large-scale collective operations is less common. \n\n**Therefore, the most fitting context for widespread use of collective operations like global sum is supercomputing (A).** \n"
            }
        ]
    },
    {
        "doc_id": 101,
        "doc": {
            "question_id": 128884144,
            "question": "If a network has more injection bandwidth than uplink bandwidth, it is considered _______.",
            "options": [
                "balancing",
                "underutilized",
                "negligible",
                "oversubscribed"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "If a network has more injection bandwidth than uplink bandwidth, it is considered _______.",
                "Expected Label": "D",
                "Predicted Label": "D) oversubscribed \n"
            }
        ]
    },
    {
        "doc_id": 102,
        "doc": {
            "question_id": 128884146,
            "question": "____ topology extends the flattened butterfly topology to create a more general class of topology, allowing different number of switches in each dimension.",
            "options": [
                "HyperX",
                "Dragonfly",
                "Folded-Clos",
                "Hypercube"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ topology extends the flattened butterfly topology to create a more general class of topology, allowing different number of switches in each dimension.",
                "Expected Label": "A",
                "Predicted Label": "B) Dragonfly \n"
            }
        ]
    },
    {
        "doc_id": 103,
        "doc": {
            "question_id": 128884148,
            "question": "UGAL routing makes its routing decision at the _______.",
            "options": [
                "destination router",
                "source router",
                "intermediate router",
                "receiver"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "UGAL routing makes its routing decision at the _______.",
                "Expected Label": "B",
                "Predicted Label": "**A) destination router** \n\nUGAL (Unicast Greedy Adaptive) routing makes its forwarding decisions at the **destination router**.  Here's why:\n\n* **Destination-Based Routing:** UGAL, like many routing protocols, is destination-based. This means the routing path is determined by the destination address of the data packet.\n* **Reverse Path Forwarding:** UGAL typically employs a technique called reverse path forwarding (RPF). In RPF, the destination router uses the path it would take to send data back to the source to make forwarding decisions.\n\nLet me know if you'd like more details on UGAL routing! \n"
            }
        ]
    },
    {
        "doc_id": 104,
        "doc": {
            "question_id": 128884150,
            "question": "The minimal deterministic algorithm does not require additional ____ for routing deadlock.",
            "options": [
                "paths",
                "routers",
                "virtual channels",
                "dimensions"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The minimal deterministic algorithm does not require additional ____ for routing deadlock.",
                "Expected Label": "C",
                "Predicted Label": "C) virtual channels \n\n**Explanation:**\n\nDeterministic routing algorithms can lead to deadlock situations where packets are stuck in a cycle, unable to proceed. Virtual channels are a common technique to prevent this type of deadlock in deterministic routing. They provide multiple \"lanes\" within a physical channel, allowing packets blocked by one message to be overtaken by others in different virtual channels. \n"
            }
        ]
    },
    {
        "doc_id": 105,
        "doc": {
            "question_id": 128884152,
            "question": "In a distributed separable allocator design for high-radix routers, the ____ typically takes place over multiple stages to improve scalability.",
            "options": [
                "routing computation",
                "virtual channel allocation",
                "crossbar traversal",
                "switch allocation"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a distributed separable allocator design for high-radix routers, the ____ typically takes place over multiple stages to improve scalability.",
                "Expected Label": "D",
                "Predicted Label": "**D) switch allocation**\n\n**Explanation:**\n\nIn high-radix routers, distributed separable allocators are often used for switch allocation to improve scalability. The switch allocation process is divided into multiple stages, each handling a subset of the input/output ports. This allows the allocation to be performed in parallel, reducing the overall allocation time and improving scalability.\n\nThe other options are not typically implemented in multiple stages in a distributed separable allocator:\n\n- **A) routing computation:** Routing computation is typically performed once at the ingress port.\n- **B) virtual channel allocation:** Virtual channel allocation is often performed in a single stage.\n- **C) crossbar traversal:** Crossbar traversal is a hardware function and not part of the allocation process. \n"
            }
        ]
    },
    {
        "doc_id": 106,
        "doc": {
            "question_id": 128884156,
            "question": "The ________ is a measure that describes the ratio of a datacenter\u2019s total power to the power actually used by computing equipment.",
            "options": [
                "cooling efficiency",
                "power usage effectiveness",
                "power density",
                "energy consumption ratio"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ________ is a measure that describes the ratio of a datacenter\u2019s total power to the power actually used by computing equipment.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 107,
        "doc": {
            "question_id": 128884158,
            "question": "The ____ is typically about 95% efficient, and is a small contributor to the power usage effectiveness (PUE) of a datacenter.",
            "options": [
                "DRAM",
                "CPU",
                "UPS",
                "GPU"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is typically about 95% efficient, and is a small contributor to the power usage effectiveness (PUE) of a datacenter.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **UPS (Uninterruptible Power Supply)** is designed for high efficiency to minimize energy loss during power conversion. A 95% efficiency rating is common for modern UPS systems. \n\nLet's analyze why the other options are incorrect:\n\n* **DRAM (Dynamic Random Access Memory)** and **CPU (Central Processing Unit)** are significant power consumers in a data center and don't operate at such high efficiency levels.\n* **GPU (Graphics Processing Unit)**, while highly efficient for specific workloads,  can also be a major power consumer, especially in high-performance computing scenarios.\n\n**In summary:** The UPS plays a crucial role in maintaining data center uptime, and its high efficiency minimizes its contribution to the overall PUE. \n"
            }
        ]
    },
    {
        "doc_id": 108,
        "doc": {
            "question_id": 128895042,
            "question": "In conjoined core architectures, the resources with the highest potential for sharing are usually those along the ____.",
            "options": [
                "periphery of the core",
                "central execution path",
                "primary memory",
                "secondary memory"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In conjoined core architectures, the resources with the highest potential for sharing are usually those along the ____.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nConjoined cores are placed next to each other, making it easier to share resources located on the periphery. These resources might include:\n\n* **L2/L3 caches:** Sharing larger cache levels can improve performance for both cores.\n* **Interconnection networks:** Efficient communication pathways are crucial for conjoined cores.\n* **Peripheral controllers:**  Sharing controllers for I/O devices can reduce complexity and cost.\n\nThe other options are less likely:\n\n* **Central execution path:**  Each core usually needs its own dedicated execution units for optimal performance.\n* **Primary memory (RAM):** While some sharing mechanisms exist, primary memory is typically accessed independently by each core.\n* **Secondary memory (disk):**  This is too far removed from the cores for efficient sharing in a conjoined architecture. \n"
            }
        ]
    },
    {
        "doc_id": 109,
        "doc": {
            "question_id": 128895044,
            "question": "____ multithreading allows instructions from multiple threads to be issued in the same cycle.",
            "options": [
                "Coarse-grain",
                "Simultaneous",
                "Fine-grain",
                "Chip"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ multithreading allows instructions from multiple threads to be issued in the same cycle.",
                "Expected Label": "B",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 110,
        "doc": {
            "question_id": 128895048,
            "question": "In a multithreaded architecture, the physically accessed cache does not need modifications to support ____.",
            "options": [
                "Higher clock speeds",
                "Larger cache sizes",
                "Branch prediction logic",
                "Multiple threads"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a multithreaded architecture, the physically accessed cache does not need modifications to support ____.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 111,
        "doc": {
            "question_id": 128895050,
            "question": "____ enables multiple threads to utilize execution bandwidth when a single thread lacks sufficient instruction-level parallelism.",
            "options": [
                "Simultaneous multithreading",
                "Out-of-order execution",
                "Superscalar processing",
                "Branch prediction"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ enables multiple threads to utilize execution bandwidth when a single thread lacks sufficient instruction-level parallelism.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 112,
        "doc": {
            "question_id": 128895052,
            "question": "To minimize resource contention in a multithreaded machine, ____ partitioning of the active list is preferred.",
            "options": [
                "dynamic",
                "static",
                "randomized",
                "cache-aware"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To minimize resource contention in a multithreaded machine, ____ partitioning of the active list is preferred.",
                "Expected Label": "B",
                "Predicted Label": "The answer is: **A) dynamic**\n\nHere's why:\n\n* **Dynamic Partitioning:** This method adjusts the allocation of threads to processors on-the-fly, based on factors like current load and resource demands.  This adaptability is key to minimizing resource contention in a multithreaded environment. \n\nLet's break down why the other options aren't as suitable:\n\n* **Static Partitioning:** Assigns a fixed number of threads to processors regardless of their workload. This can lead to resource contention if some processors become overloaded while others remain idle.\n* **Randomized Partitioning:** While it might seem like a good way to distribute load, it lacks the intelligence to adapt to changing resource demands, potentially leading to contention.\n* **Cache-Aware Partitioning:**  This method focuses on keeping data close to the processors that need it, primarily optimizing for memory access. While important, it doesn't directly address the broader issue of resource contention.\n\n**In summary, dynamic partitioning offers the flexibility needed to monitor and adjust thread allocation in real-time, making it the most effective way to minimize resource contention in a multithreaded machine.** \n"
            }
        ]
    },
    {
        "doc_id": 113,
        "doc": {
            "question_id": 128895054,
            "question": "Multithreaded caches tend to prefer ____ linesizes than single-threaded cores.",
            "options": [
                "larger",
                "equal",
                "smaller",
                "variable"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Multithreaded caches tend to prefer ____ linesizes than single-threaded cores.",
                "Expected Label": "C",
                "Predicted Label": "A) larger \n\n**Explanation:**\n\nMultithreaded caches often benefit from larger linesizes because they can exploit spatial locality across multiple threads running concurrently. When one thread fetches data, a larger line size increases the chance that data needed by other threads will also be present in the cache, reducing the need for additional memory accesses. \n"
            }
        ]
    },
    {
        "doc_id": 114,
        "doc": {
            "question_id": 128895056,
            "question": "____ is a technique that allows multiple narrow width values to be stored in a single register to improve register file utilization.",
            "options": [
                "Register inlining",
                "Register hierarchy",
                "Register renaming",
                "Register packing"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a technique that allows multiple narrow width values to be stored in a single register to improve register file utilization.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 115,
        "doc": {
            "question_id": 128895062,
            "question": "Fine-grain multithreading allows ____ execution of instructions from different threads.",
            "options": [
                "sequential",
                "out-of-order",
                "interleaved",
                "speculative"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Fine-grain multithreading allows ____ execution of instructions from different threads.",
                "Expected Label": "C",
                "Predicted Label": "C) interleaved \n"
            }
        ]
    },
    {
        "doc_id": 116,
        "doc": {
            "question_id": 128895066,
            "question": "___ are attacks that have never been seen before.",
            "options": [
                "Zero-day attacks",
                "Signature attacks",
                "Operational attacks",
                "Insider attacks"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "___ are attacks that have never been seen before.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Zero-day attacks** exploit vulnerabilities that are unknown to the software developers and security vendors, hence the term \"zero-day\" - meaning there's been zero days to patch it. \n"
            }
        ]
    },
    {
        "doc_id": 117,
        "doc": {
            "question_id": 128895068,
            "question": "In the ____ security policy model, subjects are prevented from reading objects with a lower integrity classification than their own integrity clearance.",
            "options": [
                "Bell-LaPadula",
                "Biba",
                "Confidentiality",
                "Star Property"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the ____ security policy model, subjects are prevented from reading objects with a lower integrity classification than their own integrity clearance.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 118,
        "doc": {
            "question_id": 128895070,
            "question": "In the Clark-Wilson security policy, a ______ is a series of operations that transitions the system from one consistent state to another, ensuring data integrity.",
            "options": [
                "Certified Data Item (CDI)",
                "Integrity Verification Procedure (IVP)",
                "Well-formed transaction",
                "Transformation Procedure (TP)"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the Clark-Wilson security policy, a ______ is a series of operations that transitions the system from one consistent state to another, ensuring data integrity.",
                "Expected Label": "C",
                "Predicted Label": "D\n \n**Explanation:**\n\n* **Transformation Procedures (TPs)** in the Clark-Wilson model are controlled programs that are designed to ensure data integrity by only allowing changes to data in authorized ways.  They enforce the idea of moving the system from one valid state to another.\n\nLet's look at why the other options are incorrect:\n\n* **Certified Data Items (CDIs)** represent data in a consistent state. TPs act upon CDIs.\n* **Integrity Verification Procedures (IVPs)** audit the CDIs to confirm they are in a valid state. They don't perform the state transitions themselves.\n* **Well-formed transaction** is a more general term and while related, it doesn't specifically represent the controlled operations within the Clark-Wilson model like TPs do. \n"
            }
        ]
    },
    {
        "doc_id": 119,
        "doc": {
            "question_id": 128895074,
            "question": "____ is an access control method where access is given to roles rather than individual users.",
            "options": [
                "Role-Based Access Control (RBAC)",
                "Mandatory Access Control (MAC)",
                "Discretionary Access Control (DAC)",
                "Originator-Controlled Access Control (ORCON)"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is an access control method where access is given to roles rather than individual users.",
                "Expected Label": "A",
                "Predicted Label": "Answer: A \n\n**Explanation:**\n\n* **Role-Based Access Control (RBAC)** is specifically designed around assigning permissions to roles (e.g., administrator, manager, user) and then assigning users to those roles. This simplifies management and improves security.\n\nLet's look at why the other options are incorrect:\n\n* **Mandatory Access Control (MAC)** uses security labels (like classified, secret) and enforces access based on strict rules set by a central authority, not user roles.\n* **Discretionary Access Control (DAC)** allows owners of resources to decide who else can access them, leading to less centralized control.\n* **Originator-Controlled Access Control (ORCON)** focuses on controlling access to information based on the creator's intentions, often used in document management. \n"
            }
        ]
    },
    {
        "doc_id": 120,
        "doc": {
            "question_id": 128895076,
            "question": "The A5/1 stream cipher, utilizes ______ to generate a pseudo-random sequence of bits for encryption.",
            "options": [
                "Arithmetic Logic Units (ALUs)",
                "Linear Feedback Shift Registers (LFSRs)",
                "Cache Memory",
                "Instruction Pipelines"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The A5/1 stream cipher, utilizes ______ to generate a pseudo-random sequence of bits for encryption.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 121,
        "doc": {
            "question_id": 128895078,
            "question": "The instruction that enables reordering of bytes in a register is called ____.",
            "options": [
                "Pread",
                "AES-NI",
                "byte_permute",
                "MixColumn"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instruction that enables reordering of bytes in a register is called ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile none of the options are standard, universally recognized instruction names for byte reordering, **\"byte_permute\"** comes closest to describing the operation. Here's why:\n\n* **Pread:**  Deals with reading data from file descriptors into buffers, not byte manipulation within a register.\n* **AES-NI:**  Refers to Advanced Encryption Standard New Instructions, a set of instructions designed to accelerate cryptographic operations, not byte reordering specifically.\n* **MixColumn:** Is a specific operation *within* the AES encryption standard, focused on diffusion within a matrix, not general-purpose byte reordering.\n\nLet's clarify:  There isn't a single, universally named instruction for arbitrary byte reordering across all computer architectures.  Specific CPUs often have instructions tailored for this:\n\n* **x86 (Intel/AMD):**  Use combinations of bitwise operations (AND, OR, SHIFT) or specialized instructions like `PSHUFB` (Packed Shuffle Bytes) for efficient byte reordering.\n* **ARM:** Offers instructions like `REV` (Byte-Reverse Word) for specific reversal patterns and SIMD (Single Instruction Multiple Data) instructions for more complex rearrangements. \n\n**In essence, while \"byte_permute\" isn't a real instruction, it best conveys the concept among the choices given.** \n"
            }
        ]
    },
    {
        "doc_id": 122,
        "doc": {
            "question_id": 128895082,
            "question": "A ____ attack is when the value in a memory location is illegitimately changed to another value.",
            "options": [
                "spoofing",
                "splicing",
                "replay",
                "root"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ attack is when the value in a memory location is illegitimately changed to another value.",
                "Expected Label": "A",
                "Predicted Label": "None of the provided options accurately describe the attack mentioned. Here's why:\n\n* **Spoofing:** Impersonating a legitimate user or device.\n* **Splicing:**  Typically refers to modifying network traffic or audio/video data.\n* **Replay:**  Capturing and retransmitting valid data to gain unauthorized access. \n* **Root:** Refers to the highest level of administrative access in a system.\n\nThe attack described in the question, where a memory location's value is illegitimately changed, is most likely a **Data Corruption** or **Memory Corruption** attack.  \n\n**Since none of the provided options are correct, there is no single correct answer (A, B, C, or D).** \n"
            }
        ]
    },
    {
        "doc_id": 123,
        "doc": {
            "question_id": 128895086,
            "question": "Elliptic Curve Cryptography (ECC) can use ____ public and private keys than RSA for an equivalent level of security.",
            "options": [
                "longer",
                "equivalent",
                "much shorter",
                "more complex"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Elliptic Curve Cryptography (ECC) can use ____ public and private keys than RSA for an equivalent level of security.",
                "Expected Label": "C",
                "Predicted Label": "C) much shorter \n"
            }
        ]
    },
    {
        "doc_id": 124,
        "doc": {
            "question_id": 129000840,
            "question": "In OLAP, a ____ operation modifies the perspective of a data cube to visualize data based on different relationships.",
            "options": [
                "slice_and_dice",
                "pivoting",
                "Group-by",
                "drill-down"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In OLAP, a ____ operation modifies the perspective of a data cube to visualize data based on different relationships.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 125,
        "doc": {
            "question_id": 129000846,
            "question": "_____ memory accesses pose a significant challenge in accelerating analytics workloads, especially for computations involving sparse matrices and graphs.",
            "options": [
                "Non-contiguous",
                "Sequential",
                "Cache-coherent",
                "Virtual"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "_____ memory accesses pose a significant challenge in accelerating analytics workloads, especially for computations involving sparse matrices and graphs.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 126,
        "doc": {
            "question_id": 129000848,
            "question": "A traditional scalable HPC system may not be a good ____ system.",
            "options": [
                "processing",
                "analytics",
                "computation",
                "scheduling"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A traditional scalable HPC system may not be a good ____ system.",
                "Expected Label": "B",
                "Predicted Label": "D) scheduling \n\nHere's why:\n\n* **Scheduling is crucial in HPC systems:** High-performance computing often involves complex workloads with varying resource requirements. Efficient scheduling algorithms are vital to maximize resource utilization and minimize execution time.\n\n* **Traditional HPC systems often lack dynamic scheduling:** They are typically designed for large, tightly coupled jobs. Dynamically adapting to changing workloads or efficiently handling diverse tasks is not their strength.\n\nLet me know if you'd like more explanation on any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 127,
        "doc": {
            "question_id": 129000850,
            "question": "A simple energy efficiency metric is the ratio of information sent (in bits) to the number of ____ that transmit the information.",
            "options": [
                "spikes",
                "channels",
                "neurons",
                "axon"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A simple energy efficiency metric is the ratio of information sent (in bits) to the number of ____ that transmit the information.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 128,
        "doc": {
            "question_id": 129000854,
            "question": "In the context of Time Neural Networks (TNNs), the concept of \"temporal invariance\" is analogous to the concept of ______ in general space-time computing paradigms.",
            "options": [
                "spatial locality",
                "data parallelism",
                "invariance",
                "instruction pipelining"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the context of Time Neural Networks (TNNs), the concept of \"temporal invariance\" is analogous to the concept of ______ in general space-time computing paradigms.",
                "Expected Label": "C",
                "Predicted Label": "C) invariance \n\n**Explanation:**\n\nTemporal invariance in TNNs means the network's ability to recognize patterns regardless of their position in time (similar to how Convolutional Neural Networks are spatially invariant). This directly corresponds to the broader concept of **invariance** in space-time computing, where computations remain consistent regardless of their specific space or time coordinates.\n\nLet's look at why the other options are not the best fit:\n\n* **A) Spatial Locality:** This refers to the tendency of data accesses to cluster in nearby memory locations. While important for optimization, it doesn't directly relate to the core concept of invariance.\n* **B) Data Parallelism:** This involves performing the same operation on multiple data elements simultaneously.  It's a processing technique, not directly analogous to invariance.\n* **D) Instruction Pipelining:** This is a technique to improve CPU efficiency by overlapping instruction execution.  Again, it's a performance optimization and not directly comparable to invariance. \n"
            }
        ]
    },
    {
        "doc_id": 129,
        "doc": {
            "question_id": 129000858,
            "question": "____ is considered the 'Rosetta Stone of neuroscience' according to Hawkins.",
            "options": [
                "The columnar hypothesis",
                "The uniformity assumption",
                "The neocortical hierarchy",
                "The processing function"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered the 'Rosetta Stone of neuroscience' according to Hawkins.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nJeff Hawkins, the founder of Numenta and author of \"On Intelligence,\" refers to **the columnar hypothesis** as the \"Rosetta Stone of neuroscience.\"  This hypothesis posits that the neocortex's fundamental computational unit is a cortical column, a vertical structure containing thousands of neurons. Understanding how these columns function could unlock the secrets of the brain's remarkable abilities, much like the Rosetta Stone unlocked the mysteries of Egyptian hieroglyphs. \n"
            }
        ]
    },
    {
        "doc_id": 130,
        "doc": {
            "question_id": 129000864,
            "question": "____ neurons are the primary computational elements in the prototype TNN.",
            "options": [
                "Inhibitory",
                "Spiking",
                "Artificial",
                "Excitatory"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ neurons are the primary computational elements in the prototype TNN.",
                "Expected Label": "D",
                "Predicted Label": "**B) Spiking** \n\nSpiking neurons are the primary computational elements used in TrueNorth, a type of  \"neuromorphic\" hardware designed by IBM that aims to mimic the brain's structure and function. \n\nLet me know if you'd like to know more about spiking neural networks or TrueNorth! \n"
            }
        ]
    },
    {
        "doc_id": 131,
        "doc": {
            "question_id": 129000870,
            "question": "In neural network models, the ____ is typically used to calculate weights during training, rather than iterative weight updates.",
            "options": [
                "median spike time",
                "mode spike time",
                "arithmetic mean spike time",
                "geometric mean spike time"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In neural network models, the ____ is typically used to calculate weights during training, rather than iterative weight updates.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **not listed**. None of the provided options are typically used to calculate weights during training in neural networks. \n\nHere's why:\n\n* **Weight Calculation in Neural Networks:**  Weights in neural networks are adjusted using algorithms like **backpropagation**, which relies on concepts like:\n    * **Gradient Descent:**  This method iteratively updates weights based on the error between the network's output and the desired output.\n    * **Learning Rate:** A parameter that determines the size of weight adjustments during each iteration.\n\n* **Spike Times:** Spike times are relevant to **Spiking Neural Networks (SNNs)**, a specialized type of neural network. While SNNs use spike times for information processing, weight updates are not typically calculated directly from simple averages of these times.  SNNs often employ learning rules inspired by biological mechanisms like Spike-Timing-Dependent Plasticity (STDP).\n\n**In summary:** The provided options are not standard methods for calculating weights in neural networks. \n"
            }
        ]
    },
    {
        "doc_id": 132,
        "doc": {
            "question_id": 129000876,
            "question": "____ are responsible for handling outstanding memory requests in a memory-to-network interface.",
            "options": [
                "Message format and send blocks",
                "Transaction status handling registers",
                "Protocol finite state machines",
                "Message receive blocks"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ are responsible for handling outstanding memory requests in a memory-to-network interface.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **(B) Transaction status handling registers**. Here's why:\n\n* **Transaction status handling registers** store the status of memory requests (pending, complete, error, etc.). This information is crucial for the memory-to-network interface to manage outstanding requests, ensuring data is delivered reliably.\n\nLet's break down why the other options aren't the primary answer:\n\n* **A) Message format and send blocks:** These define data structure and buffering for sending messages but don't directly manage outstanding requests.\n* **C) Protocol finite state machines:** These govern the overall communication flow but rely on status information from registers to handle individual requests.\n* **D) Message receive blocks:**  Similar to send blocks, these handle received data but don't track the status of outgoing memory requests. \n\nLet me know if you'd like a deeper dive into any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 133,
        "doc": {
            "question_id": 129000880,
            "question": "A 4 \u00d7 4 \u00d7 4 mesh or torus is a ____-ary 3-cube.",
            "options": [
                "3",
                "2",
                "8",
                "4"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A 4 \u00d7 4 \u00d7 4 mesh or torus is a ____-ary 3-cube.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 134,
        "doc": {
            "question_id": 129000882,
            "question": "____ is a technique for customizing network topology that begins with a large fully-connected switch and iteratively divides it into smaller switches.",
            "options": [
                "Splitting",
                "Merging",
                "Floorplanning",
                "Min-cut partitioning"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique for customizing network topology that begins with a large fully-connected switch and iteratively divides it into smaller switches.",
                "Expected Label": "A",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 135,
        "doc": {
            "question_id": 129000884,
            "question": "The ____ network topology has been employed in the MIT Raw chip and its commercial successor, the Tilera TILE64 chip.",
            "options": [
                "Concentrated Mesh",
                "Mesh",
                "Torus",
                "Fat Tree"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ network topology has been employed in the MIT Raw chip and its commercial successor, the Tilera TILE64 chip.",
                "Expected Label": "B",
                "Predicted Label": "B) Mesh \n"
            }
        ]
    },
    {
        "doc_id": 136,
        "doc": {
            "question_id": 129000886,
            "question": "Adaptive routing algorithms often rely on _____ information such as queue occupancy and queuing delay to gauge congestion and select links.",
            "options": [
                "user",
                "predictive",
                "local router",
                "topological"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Adaptive routing algorithms often rely on _____ information such as queue occupancy and queuing delay to gauge congestion and select links.",
                "Expected Label": "C",
                "Predicted Label": "C) local router \n\n**Explanation:**\n\nAdaptive routing algorithms make decisions based on the current network state.  \"Local router information\" like queue occupancy and queuing delay directly indicate congestion levels on the router and its connected links. This real-time data is essential for adaptive algorithms to dynamically choose the least congested paths. \n\nLet's look at why the other options aren't the best fit:\n\n* **A) user:** User information (like application type) might influence routing indirectly at higher levels, but isn't the primary factor for adaptive routing *algorithms* themselves.\n* **B) predictive:** While some algorithms might use predictive models, the core of adaptive routing relies on *current* network conditions, not just predictions.\n* **D) topological:**  Topology information (the network's layout) is important for routing in general, but adaptive algorithms need more dynamic data than just the static topology. \n"
            }
        ]
    },
    {
        "doc_id": 137,
        "doc": {
            "question_id": 129000888,
            "question": "____ routing requires the packet to carry the entire path specification.",
            "options": [
                "Adaptive",
                "Dimension-order",
                "Table-based",
                "Source"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ routing requires the packet to carry the entire path specification.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) Source**.\n\nHere's why:\n\n* **Source routing** embeds the complete path from source to destination within the packet header.  Routers along the way simply follow the predetermined path. \n\nLet's look at why the other options are incorrect:\n\n* **Adaptive routing** allows routers to dynamically choose the best path based on network conditions. It doesn't require a fixed path in the packet.\n* **Dimension-order routing** is used in multi-dimensional networks (like meshes and tori) and doesn't involve a full path specification in the packet.\n* **Table-based routing** (the most common type) uses routing tables at each router to determine the next hop. The packet doesn't contain the entire path. \n"
            }
        ]
    },
    {
        "doc_id": 138,
        "doc": {
            "question_id": 129000890,
            "question": "In _______ flow control, the entire packet must be received at a node before it can be forwarded.",
            "options": [
                "store-and-forward",
                "circuit",
                "packet-based",
                "interleaved"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In _______ flow control, the entire packet must be received at a node before it can be forwarded.",
                "Expected Label": "A",
                "Predicted Label": "A) store-and-forward \n"
            }
        ]
    },
    {
        "doc_id": 139,
        "doc": {
            "question_id": 129000892,
            "question": "The IBM Cell uses _____ to guard resource allocation access to rings.",
            "options": [
                "DMA transfers",
                "the ring arbiter",
                "wormhole flow control",
                "virtual channels"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The IBM Cell uses _____ to guard resource allocation access to rings.",
                "Expected Label": "B",
                "Predicted Label": "B) the ring arbiter \n"
            }
        ]
    },
    {
        "doc_id": 140,
        "doc": {
            "question_id": 129000898,
            "question": "When instructions are completed, they are removed from the pipeline, ____ resources and doing some bookkeeping.",
            "options": [
                "releasing",
                "allocating",
                "retaining",
                "restricting"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When instructions are completed, they are removed from the pipeline, ____ resources and doing some bookkeeping.",
                "Expected Label": "A",
                "Predicted Label": "A) releasing \n"
            }
        ]
    },
    {
        "doc_id": 141,
        "doc": {
            "question_id": 129000900,
            "question": "A serial tag and data array access design typically requires ____ to access the cache compared to a parallel access design.",
            "options": [
                "the same number of cycles",
                "one more cycle",
                "two more cycles",
                "one less cycle"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A serial tag and data array access design typically requires ____ to access the cache compared to a parallel access design.",
                "Expected Label": "B",
                "Predicted Label": "B) one more cycle \n\n**Explanation:**\n\nSerial access means fetching tag and data one after the other, while parallel access fetches them simultaneously. This means a serial design will always need an extra cycle to first fetch the tag and then, based on the tag comparison, fetch the data in the subsequent cycle. \n"
            }
        ]
    },
    {
        "doc_id": 142,
        "doc": {
            "question_id": 129000902,
            "question": "Multibanking is a technique used to emulate ____ in high-performance microprocessors.",
            "options": [
                "virtual memory",
                "multiple cores",
                "multiple ports",
                "out-of-order execution"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Multibanking is a technique used to emulate ____ in high-performance microprocessors.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 143,
        "doc": {
            "question_id": 129000908,
            "question": "Complex x86 instructions that require more than four \u03bcops are handled by a ____.",
            "options": [
                "simple decoder",
                "microsequencer (MSROM)",
                "\u03bcop queue",
                "register renaming unit"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Complex x86 instructions that require more than four \u03bcops are handled by a ____.",
                "Expected Label": "B",
                "Predicted Label": "B) microsequencer (MSROM) \n"
            }
        ]
    },
    {
        "doc_id": 144,
        "doc": {
            "question_id": 129000912,
            "question": "____ are typically used to store the source operand identifiers in an issue queue.",
            "options": [
                "Content-addressable memories",
                "Shift registers",
                "Flip-flops",
                "Symmetric arrays"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are typically used to store the source operand identifiers in an issue queue.",
                "Expected Label": "D",
                "Predicted Label": "A.\n"
            }
        ]
    },
    {
        "doc_id": 145,
        "doc": {
            "question_id": 129000914,
            "question": "The number of read ports required in the register file when reading before issue is determined by the _____. ",
            "options": [
                "machine width",
                "size of the reorder buffer",
                "number of functional units",
                "cache line size"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The number of read ports required in the register file when reading before issue is determined by the _____. ",
                "Expected Label": "A",
                "Predicted Label": "C) number of functional units \n\n**Explanation:**\n\nThe number of read ports in a register file is directly related to how many operands can be fetched simultaneously to feed the functional units. Here's why:\n\n* **Functional Units:** These units perform the actual operations (addition, multiplication, etc.).  If you have multiple functional units that can work in parallel, you need to be able to supply them with operands simultaneously.\n* **Read Ports:** Each read port allows the register file to output the value stored in a specific register.  More read ports mean more simultaneous operand fetches.\n\n**Why the other options are incorrect:**\n\n* **Machine width (A):**  While related to data path size, it doesn't directly dictate the number of read ports.\n* **Size of the reorder buffer (B):** The reorder buffer manages instruction order for out-of-order execution but doesn't directly impact the need for read ports in the register file.\n* **Cache line size (D):** Cache line size affects memory access, not register file access. \n"
            }
        ]
    },
    {
        "doc_id": 146,
        "doc": {
            "question_id": 129000918,
            "question": "The ____ is a structure used in some architectures to queue instructions that may need to be reissued, allowing the main issue queue to be freed up more quickly.",
            "options": [
                "wait table",
                "store queue",
                "replay queue",
                "load queue"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is a structure used in some architectures to queue instructions that may need to be reissued, allowing the main issue queue to be freed up more quickly.",
                "Expected Label": "C",
                "Predicted Label": "C) replay queue \n"
            }
        ]
    },
    {
        "doc_id": 147,
        "doc": {
            "question_id": 129000920,
            "question": "One method of simplifying the Address Generation Unit (AGU) in an x86 architecture involves splitting the address calculation into multiple ____.",
            "options": [
                "clock cycles",
                "stages",
                "decoders",
                "\u03bcops"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "One method of simplifying the Address Generation Unit (AGU) in an x86 architecture involves splitting the address calculation into multiple ____.",
                "Expected Label": "D",
                "Predicted Label": "B) stages \n"
            }
        ]
    },
    {
        "doc_id": 148,
        "doc": {
            "question_id": 129000926,
            "question": "The AMBA AXI protocol, a standardized protocol for on-chip communications, defines the sizes of the address and data channels to range from ____.",
            "options": [
                "1 to 64 bits",
                "2 to 128 bits",
                "8 to 1024 bits",
                "16 to 2048 bits"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The AMBA AXI protocol, a standardized protocol for on-chip communications, defines the sizes of the address and data channels to range from ____.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 149,
        "doc": {
            "question_id": 129000928,
            "question": "A folded version of the butterfly network is known as a ____.",
            "options": [
                "2D Torus",
                "Crossbar",
                "Non-blocking network",
                "Flattened Butterfly"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A folded version of the butterfly network is known as a ____.",
                "Expected Label": "D",
                "Predicted Label": "D) Flattened Butterfly \n"
            }
        ]
    },
    {
        "doc_id": 150,
        "doc": {
            "question_id": 129000930,
            "question": "____ is a key factor that can dominate the energy consumption of an on-chip network if not optimized well.",
            "options": [
                "Wire capacitance",
                "Router node degree",
                "Clock frequency",
                "Crossbar size"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor that can dominate the energy consumption of an on-chip network if not optimized well.",
                "Expected Label": "A",
                "Predicted Label": "A) Wire capacitance \n\n**Explanation:**\n\nWhile all the options impact energy consumption, wire capacitance is the most dominant factor in on-chip networks. Here's why:\n\n* **Wire Capacitance:**  As signals travel across wires, they charge and discharge the inherent capacitance of the wire itself. This charging and discharging consume a significant amount of energy, especially over long distances and at high frequencies.\n* **Router Node Degree:**  A higher node degree means more connections, potentially leading to more active wires and higher energy consumption. However, its impact is less significant than wire capacitance.\n* **Clock Frequency:** Higher clock frequencies lead to faster signal transitions, which increases power consumption. However, this is a general factor affecting all components, not just the network.\n* **Crossbar Size:** Larger crossbars might have longer wires, indirectly increasing capacitance. However, the impact of wire capacitance itself remains the primary concern. \n\n**In summary:** Wire capacitance directly dictates the energy needed to transmit signals on the on-chip network, making it the most critical factor to optimize for energy efficiency. \n"
            }
        ]
    },
    {
        "doc_id": 151,
        "doc": {
            "question_id": 129000932,
            "question": "The primary benefit of increasing the information available to routing circuitry is ____.",
            "options": [
                "reduced energy consumption",
                "adaptivity",
                "lower bandwidth utilization",
                "simplicity"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The primary benefit of increasing the information available to routing circuitry is ____.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) adaptivity**. Here's why:\n\n* **Adaptivity:** More information about the system's state (like congestion, link failures, etc.) allows the routing circuitry to make smarter, more dynamic decisions. This leads to better performance and resilience in changing network conditions. \n\nLet's look at why the other options are incorrect:\n\n* **A) reduced energy consumption:** While optimized routing can contribute to lower energy use, it's not the primary benefit of increased information. \n* **C) lower bandwidth utilization:**  More information often leads to *better* bandwidth utilization, as the routing can be optimized to avoid congested areas.\n* **D) simplicity:** Increased information generally leads to *more* complex routing algorithms, not simpler ones.\n\n**In summary, the primary benefit of providing more information to routing circuitry is to enable it to adapt and make better decisions, improving overall network performance.** \n"
            }
        ]
    },
    {
        "doc_id": 152,
        "doc": {
            "question_id": 129000938,
            "question": "A key challenge with ____ allocators lies in the potential inefficiency of matching requests to resources due to the first stage's lack of awareness regarding the second stage's outcome.",
            "options": [
                "separable",
                "wavefront",
                "parallel",
                "distributed"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A key challenge with ____ allocators lies in the potential inefficiency of matching requests to resources due to the first stage's lack of awareness regarding the second stage's outcome.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 153,
        "doc": {
            "question_id": 129000940,
            "question": "____ is primarily due to the clocked latches at low-loads in a state-of-the-art mesh router with four VCs.",
            "options": [
                "Static power",
                "Dynamic power",
                "Power saturation",
                "Switch allocation power"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is primarily due to the clocked latches at low-loads in a state-of-the-art mesh router with four VCs.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **A) Static power**. Here's why:\n\n* **Static Power:** This refers to the power consumed even when a circuit is not actively switching. Clocked latches, even when idle, leak a small amount of current, contributing to static power. \n* **Dynamic Power:** This is the power consumed due to the switching activity of transistors. While dynamic power is a significant factor in routers, the question specifically highlights \"low-loads,\" where switching activity would be minimal.\n* **Power Saturation:** This phenomenon occurs when a transistor's performance plateaus despite increasing voltage, leading to wasted power. It's not directly related to latches at low loads.\n* **Switch Allocation Power:** This is not a standard term in computer architecture and doesn't directly relate to the scenario described.\n\n**Therefore, the primary power consumption factor in a state-of-the-art mesh router with four VCs at low loads, due to clocked latches, is static power.** \n"
            }
        ]
    },
    {
        "doc_id": 154,
        "doc": {
            "question_id": 129000942,
            "question": "The ____ of an on-chip network is defined as the inverse of the maximum load across the bisection channels of any topology.",
            "options": [
                "latency",
                "energy consumption",
                "bisection bandwidth",
                "area footprint"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ of an on-chip network is defined as the inverse of the maximum load across the bisection channels of any topology.",
                "Expected Label": "C",
                "Predicted Label": "C) bisection bandwidth \n"
            }
        ]
    },
    {
        "doc_id": 155,
        "doc": {
            "question_id": 129000944,
            "question": "____ simulations provide the most accurate traffic movement within a system, but they require substantial simulation time.",
            "options": [
                "Trace-driven",
                "Synthetic traffic",
                "Emulation-based",
                "Full-system"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ simulations provide the most accurate traffic movement within a system, but they require substantial simulation time.",
                "Expected Label": "D",
                "Predicted Label": "D) Full-system \n"
            }
        ]
    },
    {
        "doc_id": 156,
        "doc": {
            "question_id": 129000946,
            "question": "______ is the least common source of propagation loss in on-chip waveguides.",
            "options": [
                "Two-Photon Absorption (TPA)",
                "Intrinsic absorption",
                "Radiative coupling",
                "Scattering"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is the least common source of propagation loss in on-chip waveguides.",
                "Expected Label": "A",
                "Predicted Label": "D) Scattering \n\n**Explanation:**\n\nWhile all the options can contribute to propagation loss in on-chip waveguides, scattering is generally the least significant among them. Here's why:\n\n* **Scattering:** This occurs due to imperfections and roughness on the waveguide's surface or material inhomogeneities. Modern fabrication techniques have significantly minimized these imperfections, making scattering a less dominant loss factor. \n* **Intrinsic absorption:**  This is inherent to the waveguide material itself and is usually more pronounced at specific wavelengths. It's a fundamental material property and can be a significant source of loss.\n* **Two-Photon Absorption (TPA):**  This nonlinear optical effect becomes prominent at high optical intensities and involves the simultaneous absorption of two photons. It's less common at typical operating powers in on-chip waveguides.\n* **Radiative coupling:** This happens when light leaks from the waveguide into the surrounding substrate or cladding. Careful design and fabrication can mitigate this loss mechanism. \n\n**In summary:** Scattering is typically minimized in well-fabricated waveguides, making it the least common source of propagation loss compared to the other options. \n"
            }
        ]
    },
    {
        "doc_id": 157,
        "doc": {
            "question_id": 129000948,
            "question": "In on-chip photonic interconnects, ____ are used to remove individual wavelengths from the incoming waveguide and route them to photodetectors at the receiver.",
            "options": [
                "Photodiodes",
                "Microrings",
                "Waveguides",
                "Lasers"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In on-chip photonic interconnects, ____ are used to remove individual wavelengths from the incoming waveguide and route them to photodetectors at the receiver.",
                "Expected Label": "B",
                "Predicted Label": "B) Microrings \n\n**Explanation:**\n\nMicrorings are wavelength-selective devices. They can be designed to resonate at specific wavelengths, dropping that particular wavelength from the input waveguide and coupling it into a separate waveguide connected to a photodetector. \n\nLet's look at why the other options are incorrect:\n\n* **Photodiodes:** These are the actual light detectors that convert optical signals to electrical signals. They don't selectively route specific wavelengths.\n* **Waveguides:**  These are the channels that guide light, similar to wires for electricity. They don't inherently separate wavelengths.\n* **Lasers:** These generate light, they don't separate or route existing light signals. \n"
            }
        ]
    },
    {
        "doc_id": 158,
        "doc": {
            "question_id": 129000950,
            "question": "The dominant factor influencing the power required for microring modulation is the ____.",
            "options": [
                "Data width",
                "Microring resistance",
                "Microring capacitance",
                "Signal propagation delay"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The dominant factor influencing the power required for microring modulation is the ____.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) Microring capacitance**. Here's why:\n\n* **Microring modulation** works by changing the refractive index of the microring material. This alters the optical path length and thus the resonance frequency of the ring. \n* **Capacitance** directly influences the amount of charge required to achieve the desired change in refractive index. Higher capacitance means more charge is needed to achieve the same modulation effect.\n* **Power consumption** in electronic circuits is directly related to the amount of charge being moved and the speed at which it's moved.  \n\n**Let's look at the other options:**\n\n* **A) Data width:** Data width affects overall system power but isn't the dominant factor *specifically* for microring modulation.\n* **B) Microring resistance:** While resistance plays a role in power dissipation, it's not the primary factor determining the power needed for modulation itself.\n* **D) Signal propagation delay:** Propagation delay is important for performance but doesn't directly dictate the power required for modulation. \n\n**In summary:**  The energy needed to change the refractive index within the microring, primarily determined by its capacitance, is the dominant factor influencing the power consumption of microring modulation. \n"
            }
        ]
    },
    {
        "doc_id": 159,
        "doc": {
            "question_id": 129000952,
            "question": "A ____ switch is necessary to create a fully flexible 2D mesh topology using a photonic switch.",
            "options": [
                "2x2",
                "4x4",
                "3x3",
                "5x5"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A ____ switch is necessary to create a fully flexible 2D mesh topology using a photonic switch.",
                "Expected Label": "D",
                "Predicted Label": "B) 4x4 \n\nHere's why:\n\n* **2D Mesh Topology:**  A 2D mesh network connects nodes horizontally and vertically. To route data through this network, you need switches capable of directing traffic in four directions (North, South, East, West).\n\n* **4x4 Switch:** A 4x4 switch has four input ports and four output ports. This configuration perfectly allows a single switch to handle the four directions necessary within a 2D mesh.\n\n**Why other options are incorrect:**\n\n* **2x2 Switch:**  Offers only two directions, limiting the flexibility of a 2D mesh.\n* **3x3 and 5x5 Switches:** While these could technically be used, they introduce unnecessary complexity for a basic 2D mesh. A 4x4 switch provides the most efficient and direct routing. \n"
            }
        ]
    },
    {
        "doc_id": 160,
        "doc": {
            "question_id": 129000954,
            "question": "____ is a technique used to reduce the thermal sensitivity of microring resonators in photonic interconnects.",
            "options": [
                "Using polymers as cladding",
                "Increasing microring radius",
                "Applying UV light",
                "Cooling with liquid nitrogen"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "IP Design and Manufacturing"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used to reduce the thermal sensitivity of microring resonators in photonic interconnects.",
                "Expected Label": "A",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 161,
        "doc": {
            "question_id": 129000956,
            "question": "HARQ protocols can ensure reliable on-chip network communication even with some _____ faults, provided they are correctable by FEC.",
            "options": [
                "transient",
                "permanent",
                "intermittent",
                "software-induced"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "HARQ protocols can ensure reliable on-chip network communication even with some _____ faults, provided they are correctable by FEC.",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 162,
        "doc": {
            "question_id": 129000958,
            "question": "One technique for creating silicon lasers involves stimulated ______ scattering.",
            "options": [
                "Raman",
                "Brillouin",
                "Rayleigh",
                "Compton"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "One technique for creating silicon lasers involves stimulated ______ scattering.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 163,
        "doc": {
            "question_id": 129000960,
            "question": "In optical fiber communication systems, the dominant source of power inefficiency is often the ____ .",
            "options": [
                "photodetector",
                "waveguide",
                "modulator",
                "laser"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In optical fiber communication systems, the dominant source of power inefficiency is often the ____ .",
                "Expected Label": "D",
                "Predicted Label": "D) laser \n\n**Explanation:**\n\nLasers used in optical fiber communication, while highly efficient compared to other light sources, are still a major source of power inefficiency. This is primarily due to:\n\n* **Conversion Loss:**  Converting electrical energy into light energy within the laser diode isn't perfectly efficient.\n* **Heat Dissipation:** Lasers generate heat, which represents energy loss.\n* **Threshold Current:** Lasers require a minimum current (threshold current) to operate, even when no data is being transmitted.\n\nLet me know if you'd like more detail on any of these points! \n"
            }
        ]
    },
    {
        "doc_id": 164,
        "doc": {
            "question_id": 129012540,
            "question": "Performance loss on average can be recaptured up to ____ by avoiding running with timing margins based on worst-case conditions.",
            "options": [
                "20%",
                "10%",
                "5%",
                "33%"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Performance loss on average can be recaptured up to ____ by avoiding running with timing margins based on worst-case conditions.",
                "Expected Label": "A",
                "Predicted Label": "D) 33% \n"
            }
        ]
    },
    {
        "doc_id": 165,
        "doc": {
            "question_id": 129012542,
            "question": "In a power-delivery network model, ____ represents the decoupling capacitance placed in the free space between functional units.",
            "options": [
                "Cblk",
                "Cspc",
                "Cpcb",
                "Cchp"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a power-delivery network model, ____ represents the decoupling capacitance placed in the free space between functional units.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Cspc**. \n\nHere's why:\n\n* **Cspc** typically represents the capacitance of the space between components (like functional units). This space is often filled with a dielectric material, and the capacitance it provides is crucial for decoupling and reducing noise in power delivery.\n\nLet's break down the other options:\n\n* **Cblk:**  Usually refers to the capacitance of a larger power distribution block or plane.\n* **Cpcb:**  Represents the capacitance of the printed circuit board itself.\n* **Cchp:** This is not a standard abbreviation in the context of power-delivery networks. \n\n**In summary:** When modeling power delivery, Cspc is the most accurate representation of decoupling capacitance in the free space between functional units. \n"
            }
        ]
    },
    {
        "doc_id": 166,
        "doc": {
            "question_id": 129012544,
            "question": "____ current profiles, often seen in scenarios like firmware-controlled sleep/active state transitions, involve a rapid and significant change in current consumption.",
            "options": [
                "Pulse Current",
                "Step Current",
                "Resonating Current",
                "Steady-State Current"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ current profiles, often seen in scenarios like firmware-controlled sleep/active state transitions, involve a rapid and significant change in current consumption.",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 167,
        "doc": {
            "question_id": 129012552,
            "question": "Today's production processors use operating voltage margins that are nearly ____ of nominal supply voltage.",
            "options": [
                "10%",
                "15%",
                "20%",
                "25%"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Today's production processors use operating voltage margins that are nearly ____ of nominal supply voltage.",
                "Expected Label": "C",
                "Predicted Label": "C) 20% \n\n**Explanation:** Modern processors are designed with tight operating voltage margins, typically around 20% of the nominal supply voltage. This is crucial for managing power consumption and heat dissipation.  \n"
            }
        ]
    },
    {
        "doc_id": 168,
        "doc": {
            "question_id": 129012554,
            "question": "____ can perform fine-grained instruction-level code transformations to eliminate voltage emergencies in frequently executing loops.",
            "options": [
                "Hardware throttling",
                "Checkpoint recovery",
                "Emergency predictors",
                "Intelligent compilers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ can perform fine-grained instruction-level code transformations to eliminate voltage emergencies in frequently executing loops.",
                "Expected Label": "D",
                "Predicted Label": "D) Intelligent compilers \n\n**Explanation:**\n\n* **Intelligent compilers** can analyze code and perform optimizations, including loop transformations. Some advanced compilers can identify potential voltage emergencies within frequently executed loops and rearrange instructions to mitigate these issues.\n\nLet's look at why the other options are less suitable:\n\n* **Hardware throttling** reacts to voltage emergencies *after* they occur by slowing down the processor, not by preventing them through code modification.\n* **Checkpoint recovery** is a technique for recovering from errors, not for preventing voltage emergencies.\n* **Emergency predictors** aren't a standard architectural component specifically designed to handle voltage fluctuations in loops. \n\n**Therefore, D is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 169,
        "doc": {
            "question_id": 129012556,
            "question": "A key challenge in mitigating soft errors lies in establishing a _______ mechanism.",
            "options": [
                "robust error-detection",
                "low-latency data recovery",
                "frequency scaling",
                "voltage over-provisioning"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A key challenge in mitigating soft errors lies in establishing a _______ mechanism.",
                "Expected Label": "A",
                "Predicted Label": "A) \n\n**Explanation:**\n\n* **Soft errors** are transient errors caused by external factors like radiation. The primary challenge is to **detect** these errors reliably. \n* While the other options play a role in system reliability, they aren't the **key challenge** in mitigating soft errors specifically. \n\nTherefore, **robust error-detection (A)** is the most critical mechanism. \n"
            }
        ]
    },
    {
        "doc_id": 170,
        "doc": {
            "question_id": 129012558,
            "question": "During rollback due to a voltage emergency, the processor starts at a reduced frequency for some number of cycles called the ___.",
            "options": [
                "restore period",
                "throttling period",
                "delay period",
                "recovery period"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "During rollback due to a voltage emergency, the processor starts at a reduced frequency for some number of cycles called the ___.",
                "Expected Label": "B",
                "Predicted Label": "D) recovery period \n\n**Explanation:**\n\nThe recovery period is the specific term used in computer architecture to describe the initial phase after a voltage emergency when the processor operates at a reduced frequency. This gradual return to full operational speed helps ensure system stability. \n"
            }
        ]
    },
    {
        "doc_id": 171,
        "doc": {
            "question_id": 129012562,
            "question": "Setting the soft threshold for voltage throttling aggressively requires a _____ reaction time by the sensor and actuation system to avoid correctness violations.",
            "options": [
                "slow",
                "moderate",
                "negligible",
                "fast"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Setting the soft threshold for voltage throttling aggressively requires a _____ reaction time by the sensor and actuation system to avoid correctness violations.",
                "Expected Label": "D",
                "Predicted Label": "D) fast \n\n**Explanation:**\n\nAggressive voltage throttling means pushing the voltage lower, closer to the point where errors might occur.  To prevent actual errors (correctness violations), the system needs to detect voltage drops quickly and react immediately.  A fast reaction time is crucial in this scenario. \n"
            }
        ]
    },
    {
        "doc_id": 172,
        "doc": {
            "question_id": 129012564,
            "question": "An example of an error model is the well-known ____ model, which models the impact of faults that cause a circuit value to be stuck at either 0 or 1.",
            "options": [
                "stuck-at",
                "bridging",
                "fail-stop",
                "delay"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "An example of an error model is the well-known ____ model, which models the impact of faults that cause a circuit value to be stuck at either 0 or 1.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 173,
        "doc": {
            "question_id": 129012566,
            "question": "____ is a significant concern in modern processor design that can cause unexpected behaviors like performance degradation.",
            "options": [
                "Cache coherence",
                "Process variability",
                "Branch prediction",
                "Pipelining"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a significant concern in modern processor design that can cause unexpected behaviors like performance degradation.",
                "Expected Label": "B",
                "Predicted Label": "B) Process variability \n"
            }
        ]
    },
    {
        "doc_id": 174,
        "doc": {
            "question_id": 129012568,
            "question": "The use of different but functionally equivalent hardware for redundancy is referred to as ____.",
            "options": [
                "Spatial Redundancy",
                "Temporal Redundancy",
                "Design Diversity",
                "Homogeneous Redundancy"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The use of different but functionally equivalent hardware for redundancy is referred to as ____.",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Spatial Redundancy (A)** uses duplicated hardware components (e.g., multiple processors, memory modules) to allow a system to continue operating correctly even if one of the components fails. \n* **Temporal Redundancy (B)** involves repeating an operation in time (like retransmitting data) to overcome transient errors.\n* **Design Diversity (C)** uses different implementations of the same functionality to avoid common points of failure.\n* **Homogeneous Redundancy (D)** isn't a commonly used term in computer architecture.\n\nTherefore, **Spatial Redundancy (A)** is the most appropriate answer as it directly relates to using different but functionally equivalent hardware for redundancy. \n"
            }
        ]
    },
    {
        "doc_id": 175,
        "doc": {
            "question_id": 129012570,
            "question": "Correcting an erasure is easier than correcting an error because we know ____.",
            "options": [
                "the type of error",
                "the cause of the error",
                "the number of errors",
                "the location of the erased bit"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Correcting an erasure is easier than correcting an error because we know ____.",
                "Expected Label": "D",
                "Predicted Label": "D) the location of the erased bit \n"
            }
        ]
    },
    {
        "doc_id": 176,
        "doc": {
            "question_id": 129012574,
            "question": "In the Argus framework, ______ of the Data Flow Graph (DFG) is used to manage its potentially unbounded size.",
            "options": [
                "replication",
                "hashing",
                "compression",
                "caching"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the Argus framework, ______ of the Data Flow Graph (DFG) is used to manage its potentially unbounded size.",
                "Expected Label": "B",
                "Predicted Label": "D) caching \n"
            }
        ]
    },
    {
        "doc_id": 177,
        "doc": {
            "question_id": 129012576,
            "question": "Error detecting (and correcting) codes are primarily implemented in _____ to protect against errors.",
            "options": [
                "CPU registers",
                "I/O devices",
                "memory hierarchy levels below the L1 cache",
                "instruction pipeline"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Error detecting (and correcting) codes are primarily implemented in _____ to protect against errors.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nError detecting and correcting codes are most crucial in **memory hierarchy levels below the L1 cache** (like main memory, hard drives, etc.). Here's why:\n\n* **CPU registers:** Registers are temporary storage within the CPU itself and have very robust error detection mechanisms built directly into the CPU's design. ECC is less critical here.\n* **I/O devices:** While error detection is important for I/O, it's often handled through protocols specific to the device and communication channel. \n* **Instruction pipeline:**  Pipeline errors are usually handled by detecting inconsistencies in instruction flow and data hazards, leading to pipeline flushes or stalls rather than relying on error correcting codes.\n\n**Memory hierarchy levels below the L1 cache** are more susceptible to errors due to factors like:\n\n* **Larger scale:**  Main memory is significantly larger than cache, increasing the statistical likelihood of errors.\n* **External factors:** Environmental factors like radiation can impact data integrity in RAM more readily than within the protected CPU environment.\n\nTherefore, ECC is vital in these memory levels to ensure data reliability. \n"
            }
        ]
    },
    {
        "doc_id": 178,
        "doc": {
            "question_id": 129012578,
            "question": "____ is a global invariant that lends itself to dynamic verification in computer memory systems.",
            "options": [
                "Branch prediction",
                "Instruction pipelining",
                "Virtual memory",
                "Cache coherence"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a global invariant that lends itself to dynamic verification in computer memory systems.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Cache coherence** is a global invariant in computer memory systems. This means that all processors in a multi-core system must have a consistent view of the data stored in shared memory, regardless of which cache holds the data. This property can be dynamically verified to ensure the system's correctness. \n\nLet's look at why the other options are not the best fit:\n\n* **Branch prediction** and **Instruction pipelining** are performance optimization techniques and do not represent global invariants. They don't directly relate to the consistency of data views across multiple processors.\n* **Virtual memory** provides an abstraction of physical memory and is important for memory management, but it's not a global invariant that ensures data consistency across caches. \n\nTherefore, **Cache coherence** is the most suitable answer as it represents a critical global invariant directly related to data consistency in multi-core systems, and its correctness can be verified dynamically. \n"
            }
        ]
    },
    {
        "doc_id": 179,
        "doc": {
            "question_id": 129012580,
            "question": "____ is a technique for detecting errors in caches by periodically reading and verifying the data.",
            "options": [
                "Cache scrubbing",
                "Token coherence",
                "Dynamic dataflow verification",
                "Register lifetime prediction"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique for detecting errors in caches by periodically reading and verifying the data.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 180,
        "doc": {
            "question_id": 129012582,
            "question": "A common form of BER is to periodically ____ the state of the system and ____ the system state to a pre-error checkpoint if an error is detected.",
            "options": [
                "verify / revert",
                "checkpoint / restore",
                "replicate / revert",
                "checkpoint / replicate"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A common form of BER is to periodically ____ the state of the system and ____ the system state to a pre-error checkpoint if an error is detected.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 181,
        "doc": {
            "question_id": 129012584,
            "question": "____ is typically used to protect recovery point state in software-implemented backward error recovery schemes.",
            "options": [
                "Main memory",
                "Cache",
                "Disk",
                "Register file"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is typically used to protect recovery point state in software-implemented backward error recovery schemes.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Disk (C)** is the most suitable option for storing recovery point state in software-implemented backward error recovery. This is because:\n    * **Persistence:** Disk storage is non-volatile, meaning data remains intact even when the system powers down. This is crucial for recovery points, as they need to survive crashes.\n    * **Capacity:** Disks offer significantly larger storage capacity compared to main memory, cache, or registers, allowing for storage of multiple recovery points or large state information.\n\n* **Other options are less suitable:**\n    * **Main memory (A)** and **Cache (B)** are volatile, losing data upon power loss. This makes them unsuitable for storing recovery information.\n    * **Register file (D)** is extremely fast but too small for storing meaningful recovery point data. \n\nLet me know if you'd like to explore other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 182,
        "doc": {
            "question_id": 129012588,
            "question": "In a type-1 environment, the VMM must perform ____.",
            "options": [
                "the system's scheduling and (real) resource allocation",
                "just the user-level task management",
                "only memory management",
                "only I/O device management"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a type-1 environment, the VMM must perform ____.",
                "Expected Label": "A",
                "Predicted Label": "**A) the system's scheduling and (real) resource allocation** \n\n**Explanation:**\n\nIn a type-1 hypervisor (or bare-metal hypervisor), the VMM (Virtual Machine Monitor) runs directly on the host's hardware. This gives it complete control over the system's resources.  Therefore, the VMM is responsible for critical tasks like:\n\n* **Scheduling:** Deciding which VM gets access to the CPU and for how long.\n* **Resource Allocation:** Managing and allocating physical resources (CPU, memory, I/O devices) to the different VMs running on the host. \n\nLet me know if you'd like more details on hypervisor types! \n"
            }
        ]
    },
    {
        "doc_id": 183,
        "doc": {
            "question_id": 129012592,
            "question": "Access permissions in ARM processors are used by the virtual memory system to restrict memory access based on the _____.",
            "options": [
                "Number of processes running",
                "Cache coherence protocol",
                "CPU mode",
                "Instruction type"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Access permissions in ARM processors are used by the virtual memory system to restrict memory access based on the _____.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nARM processors utilize CPU modes (also known as processor modes) to define different privilege levels.  Access permissions are tied to these modes. For example, user mode has restricted access compared to system or kernel modes. \n"
            }
        ]
    },
    {
        "doc_id": 184,
        "doc": {
            "question_id": 129012594,
            "question": "____ are the only non-privileged, user-sensitive instructions in the x86-32 architecture.",
            "options": [
                "imul, idiv, iret, leave",
                "jo, jno, jmp, call",
                "aas, aam, aad, daa",
                "sgdt, sidt, sldt, smsw"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are the only non-privileged, user-sensitive instructions in the x86-32 architecture.",
                "Expected Label": "D",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Option A:** These instructions are privileged (except for `leave`) and deal with interrupts and function return, not specifically user-sensitive operations.\n* **Option B:** These are jump and call instructions, fundamental to program flow but not inherently user-sensitive.\n* **Option C:**  These are the ASCII Adjust instructions. They are specifically designed to operate on ASCII representations of decimal numbers, which are often used in user input and output. These are non-privileged instructions.\n* **Option D:** These instructions are highly privileged, dealing with segment registers and system-level operations.\n\n**Therefore, option C is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 185,
        "doc": {
            "question_id": 129012596,
            "question": "____ is a virtualization approach that automatically modifies guest OS kernel code to issue hypervisor calls instead of sensitive instructions.",
            "options": [
                "Lightweight paravirtualization",
                "Full virtualization",
                "Binary translation",
                "Hardware-assisted virtualization"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a virtualization approach that automatically modifies guest OS kernel code to issue hypervisor calls instead of sensitive instructions.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Lightweight paravirtualization** modifies the guest OS kernel to replace sensitive instructions with hypercalls directly. This is in contrast to:\n\n* **Full virtualization:**  Relies on trapping and emulating sensitive instructions, leading to higher overhead.\n* **Binary translation:** Translates guest instructions to host instructions on the fly, also with higher overhead. \n* **Hardware-assisted virtualization:** Uses hardware extensions (like Intel VT-x or AMD-V) to improve virtualization performance, but doesn't inherently modify the guest OS kernel in the way described. \n"
            }
        ]
    },
    {
        "doc_id": 186,
        "doc": {
            "question_id": 129012598,
            "question": "The cost of a single hardware round-trip in early VT-x generations, like Prescott, was measured in ____.",
            "options": [
                "nanoseconds",
                "microseconds",
                "milliseconds",
                "picoseconds"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The cost of a single hardware round-trip in early VT-x generations, like Prescott, was measured in ____.",
                "Expected Label": "B",
                "Predicted Label": "C) milliseconds \n"
            }
        ]
    },
    {
        "doc_id": 187,
        "doc": {
            "question_id": 129012600,
            "question": "Today, ____ super pages available are a standard feature of commodity operating systems such as Linux.",
            "options": [
                "1 KB",
                "256 KB",
                "2 MB",
                "1 GB"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Today, ____ super pages available are a standard feature of commodity operating systems such as Linux.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 188,
        "doc": {
            "question_id": 129012602,
            "question": "The ____ space of PCIe devices allows the operating system to identify and configure devices.",
            "options": [
                "I/O",
                "Memory",
                "Register",
                "Configuration"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ space of PCIe devices allows the operating system to identify and configure devices.",
                "Expected Label": "D",
                "Predicted Label": "D) Configuration \n"
            }
        ]
    },
    {
        "doc_id": 189,
        "doc": {
            "question_id": 129012604,
            "question": "Virtio-net's superior performance compared to e1000 is attributed to a reduction in _____.",
            "options": [
                "exits and interrupts",
                "TCP segment size",
                "Ethernet frame size",
                "average processing time per segment"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Virtio-net's superior performance compared to e1000 is attributed to a reduction in _____.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) exits and interrupts**. Here's why:\n\n* **Virtio-net** is designed for virtualization. It minimizes the overhead associated with communication between guest operating systems and the hypervisor.  Exits and interrupts are a primary source of this overhead. \n* **e1000** is an emulated hardware network card. It requires more interaction with the hypervisor, leading to more exits and interrupts.\n\nLet me know if you'd like a deeper dive into virtio! \n"
            }
        ]
    },
    {
        "doc_id": 190,
        "doc": {
            "question_id": 129012608,
            "question": "____ is a key requirement for constructing a virtual machine monitor according to virtualization theory.",
            "options": [
                "Interrupt handling",
                "Memory segmentation",
                "Sensitive instruction trapping",
                "Register renaming"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a key requirement for constructing a virtual machine monitor according to virtualization theory.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Sensitive instruction trapping** is crucial for virtualization. These are instructions that, when executed by a guest OS, would impact the underlying system but shouldn't be allowed to do so directly. The VMM must trap these instructions to maintain isolation and control.\n\nLet's look at why the other options are less accurate:\n\n* **Interrupt handling** is important for any OS, including those running in VMs, but it's not a defining requirement for a VMM itself.\n* **Memory segmentation** is one approach to memory management but not essential for all forms of virtualization.\n* **Register renaming** is a performance optimization technique within a processor and not directly related to the core requirements of virtualization. \n"
            }
        ]
    },
    {
        "doc_id": 191,
        "doc": {
            "question_id": 129012616,
            "question": "Cluster-level infrastructure software, similar to an operating system, provides functionalities like ____ at a larger scale.",
            "options": [
                "game development libraries",
                "web browser rendering engines",
                "resource management and task scheduling",
                "audio and video processing codecs"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Cluster-level infrastructure software, similar to an operating system, provides functionalities like ____ at a larger scale.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 192,
        "doc": {
            "question_id": 129012618,
            "question": "System operators must keep track of Internet service target levels within ____ to avoid significant disruption.",
            "options": [
                "days",
                "hours",
                "minutes",
                "seconds"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "System operators must keep track of Internet service target levels within ____ to avoid significant disruption.",
                "Expected Label": "D",
                "Predicted Label": "D) seconds \n\n**Explanation:** Internet services are highly time-sensitive. Delays noticeable beyond a few seconds can disrupt user experience, trigger timeouts, and cause cascading failures in interconnected systems.  Therefore, system operators need to monitor and react to performance fluctuations within seconds to maintain service levels. \n"
            }
        ]
    },
    {
        "doc_id": 193,
        "doc": {
            "question_id": 129012620,
            "question": "____ is a key factor that contributes to the cost-efficiency of low-end servers used in warehouse-scale computers.",
            "options": [
                "Economies of scale",
                "Custom hardware design",
                "Proprietary technologies",
                "Vertical integration"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor that contributes to the cost-efficiency of low-end servers used in warehouse-scale computers.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 194,
        "doc": {
            "question_id": 129012622,
            "question": "The typical lifespan of a server in a datacenter is ____ years.",
            "options": [
                "1-2",
                "3-4",
                "5-6",
                "7-8"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The typical lifespan of a server in a datacenter is ____ years.",
                "Expected Label": "B",
                "Predicted Label": "B) 3-4 \n"
            }
        ]
    },
    {
        "doc_id": 195,
        "doc": {
            "question_id": 129012626,
            "question": "The ____ is typically the source of most power conversion losses in a datacenter.",
            "options": [
                "transformer",
                "chiller",
                "power distribution unit",
                "uninterruptible power supply"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is typically the source of most power conversion losses in a datacenter.",
                "Expected Label": "D",
                "Predicted Label": "A \n\n**Explanation:**\n\nTransformers, while generally efficient, handle the largest power conversions from high voltage grid power down to usable levels. Even small percentage losses at this stage represent significant energy waste. \n\nLet's look at why the other options are less likely:\n\n* **Chillers:**  While important for cooling, they are a separate system from power *conversion*. Their inefficiency is a factor in overall energy use, but not directly in power conversion losses.\n* **Power Distribution Units (PDUs):** These handle lower voltage distribution and have some losses, but not as significant as the initial high-voltage transformation.\n* **Uninterruptible Power Supplies (UPSs):** UPSs involve battery charging/discharging, which has losses. However, they typically handle a smaller portion of the total power flow compared to the main transformers. \n"
            }
        ]
    },
    {
        "doc_id": 196,
        "doc": {
            "question_id": 129012628,
            "question": "Distributing data replicas across an entire cluster instead of concentrating them within a few machines primarily benefits ______.",
            "options": [
                "Resiliency",
                "Latency",
                "Cost",
                "Power Consumption"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Distributing data replicas across an entire cluster instead of concentrating them within a few machines primarily benefits ______.",
                "Expected Label": "A",
                "Predicted Label": "A) Resiliency \n\n**Explanation:**\n\nDistributing data replicas enhances **resiliency**. If one machine fails, other replicas can still serve the data, preventing a single point of failure. \n\nLet's analyze why the other options are less accurate:\n\n* **Latency (B):** While data locality can improve latency for some requests, distributing replicas might increase latency for others due to data needing to be fetched from farther nodes.\n* **Cost (C):** Distributing replicas usually increases storage costs as you need more storage overall.\n* **Power Consumption (D):** More distributed storage can lead to higher power consumption due to increased data transfer and processing across multiple machines. \n"
            }
        ]
    },
    {
        "doc_id": 197,
        "doc": {
            "question_id": 129012632,
            "question": "_____ is a technique used to manage the aggregate power of a pool of servers in data centers.",
            "options": [
                "Load balancing",
                "Virtualization",
                "Power capping",
                "Overclocking"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ is a technique used to manage the aggregate power of a pool of servers in data centers.",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Load balancing** is the process of distributing network traffic across multiple servers. This ensures no single server is overwhelmed, maximizing responsiveness and uptime for applications. \n\nLet's look at why the other options are incorrect:\n\n* **Virtualization** creates virtual versions of computer resources, but doesn't directly manage power across a server pool.\n* **Power capping** limits the maximum power consumption of individual servers, not the aggregate power of a pool.\n* **Overclocking** increases a component's clock rate to achieve performance gains, potentially increasing power consumption. \n\n**Therefore, the best answer is A) Load balancing.** \n"
            }
        ]
    },
    {
        "doc_id": 198,
        "doc": {
            "question_id": 129012634,
            "question": "Datacenter power utilization can be negatively impacted if servers are installed based on their ____ power consumption.",
            "options": [
                "average",
                "peak",
                "idle",
                "minimum"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Datacenter power utilization can be negatively impacted if servers are installed based on their ____ power consumption.",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 199,
        "doc": {
            "question_id": 129012636,
            "question": "Among Fine-Grained policies, the trend is toward using ____ histories.",
            "options": [
                "longer",
                "shorter",
                "fixed size",
                "variable size"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Among Fine-Grained policies, the trend is toward using ____ histories.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) longer**. Here's why:\n\n* **Fine-grained policies** in computer architecture refer to branch prediction. Modern processors use complex branch prediction mechanisms to improve performance.\n* **Longer histories** provide more information about past branch behavior. This allows the branch predictor to identify more complex patterns and make more accurate predictions. \n\n**In summary:**  While some techniques use shorter or adaptive history lengths, the overall trend in modern high-performance processors is towards utilizing longer histories for more sophisticated and accurate branch prediction in fine-grained policies. \n"
            }
        ]
    },
    {
        "doc_id": 200,
        "doc": {
            "question_id": 129012638,
            "question": "____ is a cache replacement policy that addresses thrashing by evicting new lines to retain old lines.",
            "options": [
                "LRU",
                "MRU",
                "EELRU",
                "FIFO"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a cache replacement policy that addresses thrashing by evicting new lines to retain old lines.",
                "Expected Label": "B",
                "Predicted Label": "C) EELRU \n\n**Explanation:**\n\n* **EELRU (Extended Enhanced Least Recently Used)** is specifically designed to combat thrashing. It achieves this by maintaining a separate queue for recently evicted lines.  When a cache miss occurs, EELRU checks if the requested line is in this \"evicted\" queue. If found, it indicates thrashing (the line was recently used and evicted), so EELRU prioritizes keeping that line and evicts a different one.\n\nLet's analyze the other options:\n\n* **LRU (Least Recently Used):**  Evicts the least recently used item, which can worsen thrashing as it might evict frequently needed data.\n* **MRU (Most Recently Used):** Evicts the most recently used item, which is generally a poor strategy and doesn't address thrashing.\n* **FIFO (First-In, First-Out):** Evicts the oldest item in the cache, regardless of usage, and is also susceptible to thrashing. \n"
            }
        ]
    },
    {
        "doc_id": 201,
        "doc": {
            "question_id": 129012646,
            "question": "The Adaptive Replacement Cache (ARC) policy maintains ____ additional tag directories compared to a baseline cache.",
            "options": [
                "One",
                "Two",
                "Three",
                "Four"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The Adaptive Replacement Cache (ARC) policy maintains ____ additional tag directories compared to a baseline cache.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe ARC policy uses two additional tag directories called the LRU ghost list (B1) and the MRU ghost list (B2). These lists track recently evicted blocks to dynamically adjust the cache size between a virtual T1 and T2 cache. \n"
            }
        ]
    },
    {
        "doc_id": 202,
        "doc": {
            "question_id": 129012650,
            "question": "____ is a technique used in some cache replacement policies to ensure that inaccurate predictions do not permanently pollute the cache.",
            "options": [
                "Prefetching",
                "Pipelining",
                "Bypassing",
                "Aging"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a technique used in some cache replacement policies to ensure that inaccurate predictions do not permanently pollute the cache.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 203,
        "doc": {
            "question_id": 129012652,
            "question": "Hawkeye's OPTgen algorithm determines the optimal caching decisions by analyzing when a line is next ____.",
            "options": [
                "reused",
                "evicted",
                "modified",
                "fetched"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Hawkeye's OPTgen algorithm determines the optimal caching decisions by analyzing when a line is next ____.",
                "Expected Label": "A",
                "Predicted Label": "A) reused \n\nHawkeye's OPTgen algorithm focuses on **reuse**. It aims to predict when a cache line will be needed again in the future, and uses this information to make optimal caching decisions. \n"
            }
        ]
    },
    {
        "doc_id": 204,
        "doc": {
            "question_id": 129012656,
            "question": "In cache replacement policies, ____ is often used as a key metric to determine which lines to evict.",
            "options": [
                "Access frequency",
                "Memory address",
                "Age",
                "Size"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In cache replacement policies, ____ is often used as a key metric to determine which lines to evict.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 205,
        "doc": {
            "question_id": 129012658,
            "question": "CSOPT, as a cache replacement policy, is impractical because it requires knowledge of ____.",
            "options": [
                "Cache size",
                "Block size",
                "Future accesses",
                "Associativity"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "CSOPT, as a cache replacement policy, is impractical because it requires knowledge of ____.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Future accesses**.\n\nHere's why:\n\n* **CSOPT (Clairvoyant OPT)** is a theoretical cache replacement policy. It represents the absolute optimal replacement strategy. CSOPT works by always evicting the block that will be accessed furthest in the future.\n\n* **The problem:**  It's impossible to predict future memory access patterns perfectly in a real-world system. \n\nLet's look at why the other options are incorrect:\n\n* **A) Cache size:** CSOPT doesn't need to know the entire cache size, just which blocks are currently in the cache.\n* **B) Block size:** CSOPT works independently of the block size.\n* **D) Associativity:**  CSOPT's functionality isn't directly tied to the cache's associativity (direct-mapped, set-associative, etc.). \n\n**In summary:** CSOPT is a theoretical best-case scenario used for comparison against real-world policies, but its reliance on knowing the future makes it impractical for implementation. \n"
            }
        ]
    },
    {
        "doc_id": 206,
        "doc": {
            "question_id": 129012662,
            "question": "In Total Store Order (TSO), ____ are used to ensure that a core's memory operations before the instruction are placed in memory order before the core's memory operations after it.",
            "options": [
                "Write buffers",
                "FENCE instructions",
                "Read-modify-write operations",
                "Cache coherence protocols"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In Total Store Order (TSO), ____ are used to ensure that a core's memory operations before the instruction are placed in memory order before the core's memory operations after it.",
                "Expected Label": "B",
                "Predicted Label": "B) FENCE instructions \n\n**Explanation:**\n\n* **FENCE instructions** act as barriers in the instruction stream, enforcing that all memory operations before a FENCE instruction complete before any memory operations after it are initiated. This helps maintain order in Total Store Order (TSO) systems.\n\nLet's look at why the other options are not the best fit:\n\n* **Write buffers (A):** While write buffers help improve performance by buffering write operations, they don't inherently guarantee the strict ordering required by TSO across all memory operations.\n* **Read-modify-write operations (C):** These are atomic operations that ensure data consistency for a single memory location, but they don't enforce global memory ordering across multiple operations. \n* **Cache coherence protocols (D):**  These are crucial for multi-core systems to maintain data consistency across different caches, but they don't directly enforce the specific memory ordering model of TSO within a single core. \n\n**Therefore, FENCE instructions are the primary mechanism to ensure the intended memory order within a core in Total Store Order.** \n"
            }
        ]
    },
    {
        "doc_id": 207,
        "doc": {
            "question_id": 129012664,
            "question": "XC, an example Relaxed Memory Consistency model, requires that all cores insert their loads, stores, and ____ into the order <m.",
            "options": [
                "Cache Coherence Instructions",
                "Atomic Instructions",
                "FENCEs",
                "Branch Instructions"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "XC, an example Relaxed Memory Consistency model, requires that all cores insert their loads, stores, and ____ into the order <m.",
                "Expected Label": "C",
                "Predicted Label": "C) FENCEs \n\n**Explanation:**\n\nRelaxed Memory Consistency (RMC) models like XC allow for out-of-order execution to improve performance. However, they need mechanisms to enforce ordering guarantees when necessary.  \n\n* **FENCE instructions** (also known as memory barriers) act as synchronization points. They ensure that all instructions before a FENCE in program order are globally observed before any instructions after the FENCE. This is crucial for maintaining consistency in multi-threaded scenarios.\n\nLet's look at why the other options are incorrect:\n\n* **Cache Coherence Instructions:** While important for multi-processor systems, they don't directly enforce the global ordering that FENCEs provide.\n* **Atomic Instructions:** These guarantee atomicity (all or nothing execution) for a single memory location but don't inherently enforce ordering on other instructions.\n* **Branch Instructions:** These control program flow and are not directly related to memory consistency ordering. \n"
            }
        ]
    },
    {
        "doc_id": 208,
        "doc": {
            "question_id": 129012666,
            "question": "IBM Power performs stores with respect to ____.",
            "options": [
                "memory",
                "a single core",
                "a register",
                "other cores"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "IBM Power performs stores with respect to ____.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **A) memory**. Here's why:\n\n* **Stores** in computer architecture refer to the act of writing data from a processor into memory. \n* While cores, registers, and inter-core communication are all part of a modern processor's design, the ultimate destination for a store instruction is **main memory**.\n\nLet me know if you'd like more explanations on computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 209,
        "doc": {
            "question_id": 129012668,
            "question": "____ is a transient state that arises when transitions between stable states are not atomic in coherence protocols.",
            "options": [
                "IVD",
                "Get",
                "Put",
                "DataResp"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a transient state that arises when transitions between stable states are not atomic in coherence protocols.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 210,
        "doc": {
            "question_id": 129012670,
            "question": "The MSI protocol, as described, assumes a ____ cache.",
            "options": [
                "write-through",
                "write-back",
                "direct-mapped",
                "fully associative"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The MSI protocol, as described, assumes a ____ cache.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe MSI (Modified, Shared, Invalid) protocol inherently relies on a **write-back** cache strategy. Here's why:\n\n* **Write-Back Caches:** In a write-back cache, modifications to data are initially made only in the cache. The updated data is written back to the main memory only when the cache line needs to be evicted. This strategy is essential for the MSI protocol to function because:\n\n    * **\"M\" (Modified) State:**  The MSI protocol uses an \"M\" state to indicate that a cache block has been modified. This modification is initially local to the cache.\n    * **Reduced Bus Traffic:** Write-back caches minimize bus traffic as writes don't propagate to the main memory immediately. This is important for multi-core systems where bus contention can be a performance bottleneck.\n\n* **Write-Through Caches:** In contrast, write-through caches update both the cache and main memory simultaneously on every write. This eliminates the need for an \"M\" state and is not compatible with the way MSI manages cache coherence.\n\nLet me know if you have any more questions! \n"
            }
        ]
    },
    {
        "doc_id": 211,
        "doc": {
            "question_id": 129012674,
            "question": "In cache coherence protocols, the ____ state indicates that a cache block is valid, read-only, dirty, and owned.",
            "options": [
                "Modified (M)",
                "Exclusive (E)",
                "Shared (S)",
                "Owned (O)"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In cache coherence protocols, the ____ state indicates that a cache block is valid, read-only, dirty, and owned.",
                "Expected Label": "D",
                "Predicted Label": "A) Modified (M) \n\n**Explanation:**\n\n* **Modified (M):** This state signifies that the cache block holds the most up-to-date data, it has been modified (written to) by the local processor, and it is the only copy marked as valid. This implies ownership as well.\n\nLet's analyze why the other options are incorrect:\n\n* **Exclusive (E):** This state means the cache block is valid, clean (not modified), and the only copy in any cache. \n* **Shared (S):** This state indicates the cache block is valid, clean, and possibly present in multiple caches.\n* **Owned (O):** While some protocols might use an \"Owned\" state, it's not as standard as the \"Modified\" state for representing a dirty, owned block.\n\n**Therefore, the \"Modified\" state best encompasses valid, read-only, dirty, and owned characteristics.** \n"
            }
        ]
    },
    {
        "doc_id": 212,
        "doc": {
            "question_id": 129012676,
            "question": "In directory-based cache coherence, ____ evictions of blocks in the shared state can simplify the protocol but may impact performance.",
            "options": [
                "silent",
                "acknowledged",
                "delayed",
                "forwarded"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In directory-based cache coherence, ____ evictions of blocks in the shared state can simplify the protocol but may impact performance.",
                "Expected Label": "A",
                "Predicted Label": "A) silent \n\n**Explanation:**\n\nSilent evictions in directory-based cache coherence mean that when a cache controller evicts a block in the shared state, it doesn't inform the directory. This simplifies the protocol because the directory doesn't need to keep track of every cache with the shared block. However, the downside is a potential performance hit. If another processor needs the evicted block, the directory won't know it's gone and may direct requests to the now-empty cache, leading to unnecessary communication overhead. \n"
            }
        ]
    },
    {
        "doc_id": 213,
        "doc": {
            "question_id": 129012678,
            "question": "One method to mitigate the impact of false sharing is to perform coherence at a ____.",
            "options": [
                "block level",
                "sub-block granularity",
                "transaction level",
                "cache line"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "One method to mitigate the impact of false sharing is to perform coherence at a ____.",
                "Expected Label": "B",
                "Predicted Label": "B) sub-block granularity \n\nHere's why:\n\n* **False sharing** occurs when threads on different processors modify different data that happen to reside within the same cache line. This causes unnecessary cache coherency traffic, even though the threads aren't truly sharing data.\n\n* **Sub-block granularity (B)** addresses this by allowing smaller portions of a cache line to be tracked and invalidated independently.  This means only the modified sub-block needs to be communicated, reducing coherence traffic.\n\nLet's look at the other options:\n\n* **Block level (A)** is the standard cache coherency approach. It's what causes false sharing in the first place, as the entire block gets invalidated even if only a portion is modified.\n* **Transaction level (C)** manages coherence at the level of memory transactions, which is too coarse-grained for addressing false sharing within a single cache line.\n* **Cache line (D)** is synonymous with block level coherence.\n\n**Therefore, sub-block granularity (B) is the most effective method to mitigate false sharing.** \n"
            }
        ]
    },
    {
        "doc_id": 214,
        "doc": {
            "question_id": 129012680,
            "question": "In temporal coherence, each reader brings in a cache block for a finite period of time called the ____, at the end of which time the block is self-invalidated.",
            "options": [
                "scope",
                "lifetime",
                "lease",
                "interval"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In temporal coherence, each reader brings in a cache block for a finite period of time called the ____, at the end of which time the block is self-invalidated.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 215,
        "doc": {
            "question_id": 129012682,
            "question": "Upon an L2 cache miss in a Relaxed Cache Consistency (RCC) protocol, the L2 controller must first ______ before evicting a block in the Owned (O) state.",
            "options": [
                "Invalidate all L1 cache entries",
                "Broadcast a write-back request to all L1 caches",
                "Request a write-back from the current owner of the block",
                "Flush the entire L2 cache"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Upon an L2 cache miss in a Relaxed Cache Consistency (RCC) protocol, the L2 controller must first ______ before evicting a block in the Owned (O) state.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nIn a Relaxed Cache Consistency (RCC) protocol, the \"Owned\" state for a cache block signifies that a particular cache line has been modified and holds the most up-to-date data. Before this block can be evicted (to make space for the L2 cache miss), the system must ensure that the modified data is not lost. \n\nHere's why the other options are incorrect and why 'C' is the best choice:\n\n* **A) Invalidate all L1 cache entries:**  While invalidation is part of cache coherence, invalidating all L1 entries for a single L2 eviction is inefficient and unnecessary.\n* **B) Broadcast a write-back request to all L1 caches:** This is also inefficient. RCC protocols aim to reduce broadcast traffic. We only need to contact the specific L1 cache that owns the block.\n* **D) Flush the entire L2 cache:** Flushing the entire L2 cache is a drastic action and unrelated to the specific need to evict a block in the Owned state.\n\n**In summary:**  Requesting a write-back from the current owner (option C) directly addresses the need to retrieve the modified data before eviction, making it the most efficient and appropriate action. \n"
            }
        ]
    },
    {
        "doc_id": 216,
        "doc": {
            "question_id": 129014147,
            "question": "To improve key-value storage performance, MICA partitions data to exploit _____.",
            "options": [
                "multi-core parallelism",
                "distributed systems",
                "disk bandwidth",
                "key-value pairs"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To improve key-value storage performance, MICA partitions data to exploit _____.",
                "Expected Label": "A",
                "Predicted Label": "A) multi-core parallelism \n\n**Explanation:**\n\nMICA (Multi-core In-memory Computing Architecture) specifically focuses on leveraging the power of **multi-core processors** to enhance the performance of key-value stores. By dividing data and operations across multiple cores, MICA aims to achieve better parallelism and overall speed. \n\nWhile the other options might be relevant in broader data storage contexts, they are not the primary focus of MICA's design for improving key-value storage. \n"
            }
        ]
    },
    {
        "doc_id": 217,
        "doc": {
            "question_id": 129014149,
            "question": "____ is a programming model for parallel computing that supports more sophisticated communication topologies than MapReduce.",
            "options": [
                "Hadoop",
                "Dryad",
                "Spark",
                "Hive"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a programming model for parallel computing that supports more sophisticated communication topologies than MapReduce.",
                "Expected Label": "B",
                "Predicted Label": "**C) Spark** \n\n**Explanation:**\n\n* **Spark** is designed to handle a wider range of parallel computing tasks than MapReduce. It allows for more complex data flows and communication patterns between nodes, going beyond the rigid map-and-reduce phases. This makes it suitable for iterative algorithms, graph processing, and real-time stream processing, where MapReduce can be less efficient.\n\nLet's break down why the other options are less suitable:\n\n* **Hadoop** is a distributed computing framework, and MapReduce is a programming model *within* Hadoop. \n* **Dryad** is a system similar to Spark but was developed by Microsoft and is less widely used.\n* **Hive** is a data warehouse system built on top of Hadoop and is not a programming model itself. \n"
            }
        ]
    },
    {
        "doc_id": 218,
        "doc": {
            "question_id": 129014151,
            "question": "The ______ of a processor should be kept low when considering total cost of ownership for datacenter hardware.",
            "options": [
                "instruction-level parallelism",
                "memory bandwidth",
                "capital cost",
                "clock frequency"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ______ of a processor should be kept low when considering total cost of ownership for datacenter hardware.",
                "Expected Label": "C",
                "Predicted Label": "**D) clock frequency**\n\n**Explanation:**\n\nWhile higher clock frequency generally leads to better performance, it also significantly increases power consumption and heat generation. In a datacenter environment with thousands of servers, the cost of electricity and cooling becomes a major factor in the total cost of ownership. Therefore, keeping the clock frequency at a reasonable level is crucial for cost optimization. \n\nHere's why the other options are less relevant:\n\n* **A) Instruction-level parallelism:** Higher ILP can improve performance, and while it might slightly increase complexity and cost, it's not the primary driver of TCO in datacenters.\n* **B) Memory bandwidth:** Sufficient memory bandwidth is crucial for performance, but it's not directly tied to clock frequency and impacts TCO less than power consumption.\n* **C) Capital cost:** While important, minimizing capital cost alone without considering operational expenses like power is shortsighted for datacenters. \n"
            }
        ]
    },
    {
        "doc_id": 219,
        "doc": {
            "question_id": 129014153,
            "question": "____ enables dynamic voltage and frequency scaling to tailor memory bandwidth to application demands.",
            "options": [
                "LP-DDRx",
                "Kozyrakis",
                "Web index partitioning",
                "Memscale"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ enables dynamic voltage and frequency scaling to tailor memory bandwidth to application demands.",
                "Expected Label": "D",
                "Predicted Label": "A) LP-DDRx \n\n**Explanation:**\n\nWhile all the options relate to computer architecture and performance in some way, the most direct answer to the question is **A) LP-DDRx**. Here's why:\n\n* **LP-DDRx (Low Power Double Data Rate x)** memory technologies are specifically designed for lower power consumption and include features that allow for dynamic voltage and frequency scaling (DVFS). This means the memory controller can adjust the voltage and frequency of the memory bus to match the application's needs, optimizing for both performance and power efficiency.\n\nLet's look at why the other options are not the best fit:\n\n* **B) Kozyrakis:**  Christos Kozyrakis is a renowned computer scientist known for his work on multicore architectures and energy efficiency, but not directly for a technology enabling dynamic voltage/frequency scaling in memory.\n* **C) Web index partitioning:** This refers to techniques for splitting and distributing large search engine indexes across multiple servers, unrelated to memory bandwidth scaling.\n* **D) Memscale:** This term isn't commonly associated with a specific technology for memory bandwidth scaling. \n\n**In summary:** LP-DDRx is the technology that directly addresses the question's requirement of dynamic voltage and frequency scaling for memory bandwidth adaptation. \n"
            }
        ]
    },
    {
        "doc_id": 220,
        "doc": {
            "question_id": 129014155,
            "question": "____ is a key consideration in hardware design for datacenter workloads, matching hardware capability to software intensity.",
            "options": [
                "Balanced design",
                "Memory persistence",
                "Distributed caching",
                "Graph analytics"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key consideration in hardware design for datacenter workloads, matching hardware capability to software intensity.",
                "Expected Label": "A",
                "Predicted Label": "A) Balanced design \n\n**Explanation:**\n\nWhile other options have relevance in datacenter contexts,  **balanced design** directly addresses the core of the question. Here's why:\n\n* **Balanced Design:** This refers to ensuring that CPU, memory, storage, and network capabilities are proportionally aligned with the workload's demands. Datacenter workloads vary greatly (e.g., high-performance computing, web serving, databases). A balanced design optimizes performance by preventing bottlenecks where one component is overloaded while others are underutilized.\n\nLet's look at the other options:\n\n* **Memory Persistence:** Important for data reliability, but not the primary factor in matching hardware to software intensity.\n* **Distributed Caching:** Improves data access speed but doesn't inherently address the core hardware-software intensity balance.\n* **Graph Analytics:**  A specific type of workload, not a universal design consideration. \n\n**Therefore, \"A) Balanced design\" is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 221,
        "doc": {
            "question_id": 129014161,
            "question": "In resource allocation mechanisms, ____ is a property that ensures no user would prefer another user's allocation over their own.",
            "options": [
                "Pareto efficiency",
                "Strategy-proofness",
                "Sharing incentives",
                "Envy-freeness"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In resource allocation mechanisms, ____ is a property that ensures no user would prefer another user's allocation over their own.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 222,
        "doc": {
            "question_id": 129014163,
            "question": "MARSSx86 leverages ____ to handle complex x86 opcodes.",
            "options": [
                "QEMU emulation",
                "PTLsim performance model",
                "DRAMsim performance model",
                "Host Linux Kernel"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "MARSSx86 leverages ____ to handle complex x86 opcodes.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 223,
        "doc": {
            "question_id": 129014169,
            "question": "Which of the following best describes the relationship between query complexity and response time in a search engine?",
            "options": [
                "Unrelated",
                "Inversely proportional",
                "Not applicable",
                "Strongly correlated"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Which of the following best describes the relationship between query complexity and response time in a search engine?",
                "Expected Label": "D",
                "Predicted Label": "D) Strongly correlated \n\n**Explanation:**\n\nWhile not *directly* proportional (a more complex query doesn't *always* mean a longer response time due to optimizations, caching, etc.), query complexity and response time are **strongly correlated**. More complex queries generally require more processing, wider data searches, and potentially more complex ranking algorithms, all of which contribute to longer response times. \n"
            }
        ]
    },
    {
        "doc_id": 224,
        "doc": {
            "question_id": 129014175,
            "question": "In MILP formulations for scheduling problems, ______ constraints are often used to model the condition that one task must finish before another starts.",
            "options": [
                "assignment",
                "network flow",
                "disjunction",
                "convexity"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In MILP formulations for scheduling problems, ______ constraints are often used to model the condition that one task must finish before another starts.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 225,
        "doc": {
            "question_id": 129014179,
            "question": "____ is a metric used to describe the amount of on-chip cache required by an application to achieve a given quality of service in data center resource management.",
            "options": [
                "Memory sensitivity",
                "Memory pressure",
                "Quality of Service",
                "Working set size"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a metric used to describe the amount of on-chip cache required by an application to achieve a given quality of service in data center resource management.",
                "Expected Label": "A",
                "Predicted Label": "D) Working set size \n\n**Explanation:**\n\n* **Working set size** refers to the minimum amount of memory (including cache) that a program needs to remain resident for efficient execution.  A larger working set implies the application needs more cache to avoid frequent accesses to slower main memory.\n\nLet's analyze why the other options are less accurate:\n\n* **Memory sensitivity:**  This broadly describes how much an application's performance is impacted by memory speed or latency, but it doesn't directly quantify cache requirements.\n* **Memory pressure:**  This indicates a shortage of available memory, which might force data out of the cache, but it's not a direct measure of an application's cache needs.\n* **Quality of Service (QoS):**  This refers to the overall performance level guaranteed to an application, encompassing factors like latency, throughput, and availability. While cache size influences QoS, it's not a metric specifically designed to determine cache needs. \n"
            }
        ]
    },
    {
        "doc_id": 226,
        "doc": {
            "question_id": 129014191,
            "question": "____ is a commonly used architectural simulator for evaluating detailed computer architecture models.",
            "options": [
                "CPLEX",
                "SPEC CPU",
                "gem5",
                "MINLP"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a commonly used architectural simulator for evaluating detailed computer architecture models.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **gem5** is a widely recognized and used architectural simulator that provides a flexible platform for simulating various computer architecture designs in detail. \n\nLet's look at the other options:\n\n* **CPLEX** is an optimization software package, not an architecture simulator.\n* **SPEC CPU** is a benchmark suite used to measure computer system performance, not a simulator itself.\n* **MINLP** (Mixed-Integer Nonlinear Programming) refers to a class of optimization problems and is not related to architecture simulation. \n"
            }
        ]
    },
    {
        "doc_id": 227,
        "doc": {
            "question_id": 129014197,
            "question": "A ____ cache hierarchy guarantees that a data block will be found in either the L1 or L2 cache, but not both.",
            "options": [
                "inclusive",
                "exclusive",
                "non-inclusive",
                "write-through"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ cache hierarchy guarantees that a data block will be found in either the L1 or L2 cache, but not both.",
                "Expected Label": "B",
                "Predicted Label": "B) exclusive \n"
            }
        ]
    },
    {
        "doc_id": 228,
        "doc": {
            "question_id": 129014207,
            "question": "Jiang et al.'s mechanism maintains shadow tags for each core and each ____ found in the system to optimize throughput.",
            "options": [
                "cache block size",
                "memory latency",
                "LLC associativity",
                "instruction set"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Jiang et al.'s mechanism maintains shadow tags for each core and each ____ found in the system to optimize throughput.",
                "Expected Label": "C",
                "Predicted Label": "A) cache block size \n\nThe answer must be something that, if tracked individually, would help optimize throughput. Of the above choices, only tracking individual cache block sizes makes sense in this context. \n\n**Therefore, the answer is A.** \n\n**Explanation:**\n\nJiang et al.'s work likely focuses on memory security or coherence protocols.  Tracking information at the cache block level is essential for these purposes. Here's why the other options are less likely:\n\n* **B) memory latency:** Memory latency is a system-wide characteristic and not something you'd track per core or element.\n* **C) LLC associativity:** LLC associativity is a property of the cache design, not individual elements within it.\n* **D) instruction set:**  Instruction sets are unrelated to per-core or per-element tracking for throughput optimization. \n"
            }
        ]
    },
    {
        "doc_id": 229,
        "doc": {
            "question_id": 129014215,
            "question": "______ prefetchers are commonly used in most commercial processors.",
            "options": [
                "Temporal",
                "Spatial",
                "Stride",
                "Pattern-based"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "______ prefetchers are commonly used in most commercial processors.",
                "Expected Label": "C",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 230,
        "doc": {
            "question_id": 129014217,
            "question": "A 5x5 router with four virtual channels per physical channel can have a maximum of ____ incoming packets simultaneously contending for the same output port.",
            "options": [
                "4",
                "5",
                "16",
                "20"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A 5x5 router with four virtual channels per physical channel can have a maximum of ____ incoming packets simultaneously contending for the same output port.",
                "Expected Label": "C",
                "Predicted Label": "D) 20 \n\n**Explanation:**\n\n* A 5x5 router has 5 input ports.\n* Each input port can have a maximum of one packet destined for a particular output port at a time (head-of-line blocking).\n* With four virtual channels per physical channel, each input port can buffer up to 4 packets headed to the same output port.\n* Therefore, a maximum of 5 input ports * 4 virtual channels/port = 20 packets can contend for the same output port simultaneously. \n"
            }
        ]
    },
    {
        "doc_id": 231,
        "doc": {
            "question_id": 129014219,
            "question": "Kerckhoffs's principle suggests that the operation of the TCB should be publicly known and should have no secrets other than the ______.",
            "options": [
                "cryptographic keys",
                "attack vectors",
                "operation algorithms",
                "software bugs"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Kerckhoffs's principle suggests that the operation of the TCB should be publicly known and should have no secrets other than the ______.",
                "Expected Label": "A",
                "Predicted Label": "A) cryptographic keys \n\n**Explanation:**\n\nKerckhoffs's principle, a fundamental concept in cryptography, states that a cryptosystem should be secure even if everything about the system, *except the key*, is public knowledge. \n\nLet's analyze why the other options are incorrect:\n\n* **B) attack vectors:** Knowing attack vectors doesn't compromise the system if the key remains secret. In fact, understanding attack vectors helps strengthen security.\n* **C) operation algorithms:**  Modern cryptography relies on publicly known algorithms.  Security comes from the key, not obscurity of the algorithm.\n* **D) software bugs:** While software bugs can lead to vulnerabilities, they are not the core secret that Kerckhoffs's principle focuses on. \n\n**Therefore, the security of a system should rely solely on the secrecy of the cryptographic keys.** \n"
            }
        ]
    },
    {
        "doc_id": 232,
        "doc": {
            "question_id": 129014221,
            "question": "______ is the prevention of unauthorized modification of protected information without detection.",
            "options": [
                "Confidentiality",
                "Integrity",
                "Availability",
                "Authentication"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "______ is the prevention of unauthorized modification of protected information without detection.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 233,
        "doc": {
            "question_id": 129014227,
            "question": "____ is a key factor in determining the security architecture of a processor, specifying which privilege levels and entities are trusted or untrusted.",
            "options": [
                "Threat model",
                "Execution privilege",
                "Hardware configuration",
                "Software stack"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor in determining the security architecture of a processor, specifying which privilege levels and entities are trusted or untrusted.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe **threat model** defines the security boundaries, potential attackers, and their capabilities.  This directly informs the design of the security architecture within a processor, outlining trusted and untrusted entities and the privilege levels they operate at. \n\nLet's look at why the other options are not the best fit:\n\n* **B) Execution privilege:** While important for security, execution privilege is a *mechanism* implemented based on the security architecture determined by the threat model.\n* **C) Hardware configuration:**  This can influence security, but it's not the key factor in *determining* the architecture.  A single hardware configuration can support different security architectures.\n* **D) Software stack:** The software stack operates *on top* of the processor's security architecture and relies on it for enforcement. \n \n**In short, the threat model lays the foundation for deciding how the security architecture of a processor will be structured.** \n"
            }
        ]
    },
    {
        "doc_id": 234,
        "doc": {
            "question_id": 129014229,
            "question": "The two main security properties that the TCB of a secure processor architecture aims to provide for a TEE are ____ and integrity.",
            "options": [
                "Availability",
                "Confidentiality",
                "Performance",
                "Scalability"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The two main security properties that the TCB of a secure processor architecture aims to provide for a TEE are ____ and integrity.",
                "Expected Label": "B",
                "Predicted Label": "B) Confidentiality \n\n**Explanation:**\n\nThe Trusted Computing Base (TCB) of a secure processor architecture aims to protect the Trusted Execution Environment (TEE) primarily through:\n\n* **Confidentiality:** Ensuring that data and operations within the TEE are kept secret from unauthorized entities, including potentially compromised operating systems or other applications.\n* **Integrity:** Guaranteeing that the code, data, and operations within the TEE are protected from unauthorized modification.\n\nLet me know if you'd like more details on any security aspect of computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 235,
        "doc": {
            "question_id": 129014231,
            "question": "NoHype introduced the idea of eliminating the hypervisor while allowing multiple ____ to share the processor.",
            "options": [
                "operating systems",
                "processors",
                "virtual machines",
                "cores"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "NoHype introduced the idea of eliminating the hypervisor while allowing multiple ____ to share the processor.",
                "Expected Label": "C",
                "Predicted Label": "A) operating systems \n\n**Explanation:**\n\nNoHype aims to achieve the benefits of virtualization (multiple isolated environments on one machine) without the overhead of a traditional hypervisor.  The goal is to allow multiple **operating systems** to share the processor directly, leading to improved performance.\n\nLet's break down why the other options are incorrect:\n\n* **B) processors:**  Processors sharing a processor doesn't make sense in this context.\n* **C) virtual machines:** Virtual machines are the environments created by hypervisors, and NoHype aims to eliminate the hypervisor.\n* **D) cores:** While multiple cores already share a processor in modern systems, NoHype isn't about core management, it's about replacing the hypervisor's role. \n"
            }
        ]
    },
    {
        "doc_id": 236,
        "doc": {
            "question_id": 129014239,
            "question": "In secure symmetric multiprocessor designs, ____ is typically used for encryption due to performance constraints.",
            "options": [
                "Triple DES",
                "RSA",
                "Counter mode AES",
                "Blowfish"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In secure symmetric multiprocessor designs, ____ is typically used for encryption due to performance constraints.",
                "Expected Label": "C",
                "Predicted Label": "C \n\nHere's why:\n\n* **Counter mode AES (Advanced Encryption Standard)** is known for its speed and hardware efficiency, making it suitable for multiprocessor environments where performance is crucial. \n\nLet's examine why the other options are less likely:\n\n* **Triple DES:**  While secure, it's significantly slower than AES, making it less desirable in performance-sensitive scenarios.\n* **RSA:**  Primarily an asymmetric encryption algorithm. While sometimes used in key exchange within secure systems, it's computationally heavier than symmetric algorithms like AES and less suitable for bulk data encryption.\n* **Blowfish:**  Although faster than DES, it's generally not as widely adopted or optimized in hardware as AES, particularly in multiprocessor systems. \n\n**Therefore, Counter mode AES (C) provides the best combination of security and performance for secure symmetric multiprocessor designs.** \n"
            }
        ]
    },
    {
        "doc_id": 237,
        "doc": {
            "question_id": 129014243,
            "question": "Next-line prefetching is most effective in handling ____.",
            "options": [
                "sequential instruction fetches",
                "discontinuities due to loops",
                "discontinuities due to function calls",
                "non-sequential instruction fetches"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Next-line prefetching is most effective in handling ____.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 238,
        "doc": {
            "question_id": 129014245,
            "question": "______ is introduced between the branch predictor and the L1 instruction fetch unit in fetch-directed instruction prefetching to decouple them.",
            "options": [
                "Prefetch instruction queue",
                "Fetch target queue",
                "Branch prediction buffer",
                "Instruction buffer"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "______ is introduced between the branch predictor and the L1 instruction fetch unit in fetch-directed instruction prefetching to decouple them.",
                "Expected Label": "B",
                "Predicted Label": "B) Fetch target queue \n\n**Explanation:**\n\nA **fetch target queue (FTQ)** sits between the branch predictor and the L1 instruction fetch unit in fetch-directed instruction prefetching. Here's why:\n\n* **Decoupling:** The FTQ decouples the prefetching mechanism (which might predict branches) from the actual instruction fetching. This allows the prefetcher to work ahead and speculate on branches without disrupting the main instruction fetch pipeline.\n* **Speculative Prefetching:** The branch predictor can add predicted addresses to the FTQ. Even if the prediction is wrong, the main fetch unit isn't affected until an instruction from the FTQ is actually needed.\n* **Efficiency:**  The FTQ can hold a small queue of prefetched instructions. This helps hide the latency of memory accesses, as the instructions are often available when needed.\n\n**Other options are incorrect because:**\n\n* **Prefetch instruction queue:** This term is not commonly used in computer architecture. \n* **Branch prediction buffer:** This structure stores branch prediction information (like whether a branch is likely taken or not), not fetched instructions.\n* **Instruction buffer:** While it holds instructions, it's typically located after the fetch unit, not before. \n"
            }
        ]
    },
    {
        "doc_id": 239,
        "doc": {
            "question_id": 129014247,
            "question": "TIFS directly predicts future instruction-cache misses by ____ and replaying recurring L1 instruction miss sequences, rather than exploring a program's control flow graph.",
            "options": [
                "identifying",
                "recording",
                "classifying",
                "prioritizing"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "TIFS directly predicts future instruction-cache misses by ____ and replaying recurring L1 instruction miss sequences, rather than exploring a program's control flow graph.",
                "Expected Label": "B",
                "Predicted Label": "B) recording \n\n**Explanation:**\n\nTIFS (Trace-based Instruction Fetch Stream) operates by **recording** sequences of instructions that lead to cache misses. It doesn't try to understand the program's logic (identifying, classifying, prioritizing) but focuses on memorizing and replaying observed patterns of instruction fetches that resulted in misses previously. \n"
            }
        ]
    },
    {
        "doc_id": 240,
        "doc": {
            "question_id": 129014249,
            "question": "Proactive instruction fetch modifies the TIFS design to record the sequence of cache blocks accessed by the ________.",
            "options": [
                "mispredicted branches",
                "predicted branch paths",
                "prefetch unit",
                "committed instruction sequence"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Proactive instruction fetch modifies the TIFS design to record the sequence of cache blocks accessed by the ________.",
                "Expected Label": "D",
                "Predicted Label": "B) predicted branch paths \n\n**Explanation:**\n\nProactive instruction fetch aims to improve performance by fetching instructions ahead of time, even before the branch outcome is known. It does this by following the **predicted branch paths**. This helps to reduce the stall time associated with fetching instructions after a branch is resolved. \n"
            }
        ]
    },
    {
        "doc_id": 241,
        "doc": {
            "question_id": 129014251,
            "question": "____ is a key parameter in stride prefetchers that determines how many blocks to prefetch when a strided stream is detected.",
            "options": [
                "Prefetch degree",
                "Cache size",
                "Stream length",
                "Reference prediction"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key parameter in stride prefetchers that determines how many blocks to prefetch when a strided stream is detected.",
                "Expected Label": "A",
                "Predicted Label": "A) Prefetch degree \n\n**Explanation:**\n\n* **Prefetch degree** directly controls how many blocks ahead the prefetcher will try to bring into the cache when a stride (a repeating pattern of memory accesses) is detected. \n* **Cache size** influences how many prefetched blocks can be stored, but not how many are initially fetched.\n* **Stream length** isn't a parameter of the prefetcher itself; it's a characteristic of the access pattern.\n* **Reference prediction** is a broader concept and might be used in prefetching, but it doesn't specifically define how many blocks to prefetch in a strided scenario. \n"
            }
        ]
    },
    {
        "doc_id": 242,
        "doc": {
            "question_id": 129014253,
            "question": "The number of potential successor addresses fetched by a Markov prefetcher is referred to as its ____.",
            "options": [
                "Depth",
                "Width",
                "Stride",
                "Coverage"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The number of potential successor addresses fetched by a Markov prefetcher is referred to as its ____.",
                "Expected Label": "B",
                "Predicted Label": "B) Width \n\n**Explanation:**\n\n* **Width** in the context of a Markov prefetcher refers to the number of possible successor addresses it fetches simultaneously.  A wider prefetcher can speculate on multiple potential branches or data accesses.\n\nLet's look at why the other options are incorrect:\n\n* **Depth:**  Usually refers to how many levels of future addresses a prefetcher might try to predict (e.g., fetching not just the next instruction but a few instructions ahead).\n* **Stride:**  Refers to a pattern of accessing memory with a fixed interval (e.g., accessing every fourth element). While Markov predictors can detect strides, \"stride\" itself doesn't describe the number of fetched addresses.\n* **Coverage:** A general term related to how effectively a prefetcher anticipates needed data, not specifically the number of addresses fetched. \n"
            }
        ]
    },
    {
        "doc_id": 243,
        "doc": {
            "question_id": 129014257,
            "question": "The ____ prefetcher design splits the correlation table into two structures: a history buffer and an index table.",
            "options": [
                "Markov",
                "tag correlating",
                "dead block correlating",
                "global history buffer"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ prefetcher design splits the correlation table into two structures: a history buffer and an index table.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **(B) tag correlating**. Here's why:\n\n* **Tag correlating prefetchers** improve upon simple stream prefetchers by using a correlation table. This table stores information about previously accessed addresses and their associated \"successor\" addresses (the addresses fetched shortly after). To make this lookup efficient, they often split the table:\n    * **History Buffer:**  Stores recent memory access addresses.\n    * **Index Table:**  Uses the history buffer entries as tags (or indexes) to point to entries that predict the next address to be fetched.\n\nLet's look at the other options:\n\n* **Markov prefetchers (A):** Use a state machine to model different access patterns and predict future accesses. They don't typically employ a history buffer/index table structure.\n* **Dead block correlating prefetchers (C):**  Focus on identifying and prefetching data blocks that are likely to be needed soon, even if they haven't been recently accessed. They don't directly use the history buffer/index table split.\n* **Global history buffer (D):** This is a general concept referring to a buffer storing recent memory accesses, which can be used by various prefetching mechanisms. It doesn't specifically describe the split structure in the question. \n\n**Therefore, the tag correlating prefetcher design is the one that specifically utilizes the history buffer and index table structure for its correlation mechanism.** \n"
            }
        ]
    },
    {
        "doc_id": 244,
        "doc": {
            "question_id": 129014259,
            "question": "Stream chaining in GHB prefetching aims to improve prefetch accuracy by exploiting the ____ correlation among consecutively accessed PC-localized streams.",
            "options": [
                "temporal",
                "spatial",
                "frequency",
                "latency"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Stream chaining in GHB prefetching aims to improve prefetch accuracy by exploiting the ____ correlation among consecutively accessed PC-localized streams.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 245,
        "doc": {
            "question_id": 129014263,
            "question": "The ____ is used as the key stored in the GHB PC/DC index table.",
            "options": [
                "cache miss address",
                "memory access instruction",
                "program counter value",
                "stride sequence"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is used as the key stored in the GHB PC/DC index table.",
                "Expected Label": "C",
                "Predicted Label": "C) program counter value \n\n**Explanation:**\n\nThe Global History Buffer (GHB) is a structure used in some advanced branch predictors.  The program counter (PC) value is used as the key to index into the GHB's PC/DC (Program Counter/Direction Counter) table. This table stores historical information about branches taken from specific PC values. \n"
            }
        ]
    },
    {
        "doc_id": 246,
        "doc": {
            "question_id": 129014265,
            "question": "The simplest way to represent spatial patterns in a code-correlated spatial prefetcher is to use a ____.",
            "options": [
                "linked list of memory addresses",
                "hash table of cache lines",
                "bit vector representing cache lines",
                "queue of data addresses"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The simplest way to represent spatial patterns in a code-correlated spatial prefetcher is to use a ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 247,
        "doc": {
            "question_id": 129014267,
            "question": "_____ is the metric that quantifies the energy usage per computation, useful for both battery-powered and plugged-in devices.",
            "options": [
                "Energy per operation",
                "Performance per Watt",
                "Clock speed",
                "Instructions per cycle"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "_____ is the metric that quantifies the energy usage per computation, useful for both battery-powered and plugged-in devices.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Energy per operation** directly measures the energy consumed for each computation, making it the most relevant metric for quantifying energy usage per computation. \n\nLet's look at why the other options are not the best fit:\n\n* **Performance per Watt** focuses on efficiency (how much work is done per unit of energy) but doesn't directly quantify energy used per computation.\n* **Clock speed** measures the speed of processing cycles but doesn't account for energy consumption per cycle.\n* **Instructions per cycle** indicates processing throughput but doesn't reflect energy usage per instruction. \n"
            }
        ]
    },
    {
        "doc_id": 248,
        "doc": {
            "question_id": 129014271,
            "question": "A machine model provides a developer with an interface to a device or programming language. At the lowest level, this interface represents the ____.",
            "options": [
                "Software libraries",
                "Operating system",
                "Physical hardware",
                "Virtual memory map"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A machine model provides a developer with an interface to a device or programming language. At the lowest level, this interface represents the ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 249,
        "doc": {
            "question_id": 129014275,
            "question": "____ is a technique used in Image Signal Processors to correct for known faulty pixels by interpolating from surrounding values.",
            "options": [
                "Dead-pixel correction",
                "Black-level correction",
                "Demosaicking",
                "Lens shading correction"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in Image Signal Processors to correct for known faulty pixels by interpolating from surrounding values.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 250,
        "doc": {
            "question_id": 129014281,
            "question": "In image processing pipelines, the energy required for ____ often exceeds the energy needed for computation.",
            "options": [
                "cache access",
                "branch prediction",
                "instruction fetch",
                "DRAM access"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In image processing pipelines, the energy required for ____ often exceeds the energy needed for computation.",
                "Expected Label": "D",
                "Predicted Label": "D) DRAM access \n\n**Explanation:**\n\nAccessing data in DRAM is significantly more energy-intensive than operations within the processor itself (like cache access, branch prediction, or instruction fetch). Image processing often involves large images, leading to frequent DRAM accesses and high energy consumption. \n"
            }
        ]
    },
    {
        "doc_id": 251,
        "doc": {
            "question_id": 129014283,
            "question": "Halide represents algorithms in a ____ form, similar to Darkroom but with a slightly different syntax.",
            "options": [
                "pure functional",
                "imperative",
                "object-oriented",
                "aspect-oriented"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Halide represents algorithms in a ____ form, similar to Darkroom but with a slightly different syntax.",
                "Expected Label": "A",
                "Predicted Label": "A) pure functional \n"
            }
        ]
    },
    {
        "doc_id": 252,
        "doc": {
            "question_id": 129241297,
            "question": "Partially homomorphic encryption schemes support the evaluation of circuits consisting of only one type of ____.",
            "options": [
                "operation/gate",
                "function/application",
                "multiplications",
                "depth"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Partially homomorphic encryption schemes support the evaluation of circuits consisting of only one type of ____.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) operation/gate**. Here's why:\n\n* **Partially Homomorphic Encryption (PHE)** allows computations on encrypted data without fully decrypting it.  The \"partially\" comes in because these schemes are typically limited to supporting only one type of operation (like addition OR multiplication) on the encrypted data.\n\nLet's break down the other options:\n\n* **B) function/application:** This is too broad. PHE schemes are designed for specific operations, not entire functions or applications.\n* **C) multiplications:** While some PHE schemes focus on multiplication (like RSA), it's not the only operation they can support. Some might support addition (like Paillier).\n* **D) depth:**  Depth refers to the complexity of a circuit (how many layers of gates). PHE's limitations are about the *type* of operation, not the circuit's depth. \n\n**Therefore, the correct answer is A.** \n"
            }
        ]
    },
    {
        "doc_id": 253,
        "doc": {
            "question_id": 129241309,
            "question": "In the context of ciphertext multiplication, the reduction in bit precision in the underlying plaintext message for a circuit of depth d multiplies messages with \u03b7 bits of precision and computes their product with ____ bits of precision.",
            "options": [
                "\u03b7 - d - 1",
                "\u03b7 + \u2308log d\u2309 -1",
                "\u03b7 - log d - 1",
                "\u03b7 / (log d + 1)"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the context of ciphertext multiplication, the reduction in bit precision in the underlying plaintext message for a circuit of depth d multiplies messages with \u03b7 bits of precision and computes their product with ____ bits of precision.",
                "Expected Label": "C",
                "Predicted Label": "B) \u03b7 + \u2308log d\u2309 -1 \n"
            }
        ]
    },
    {
        "doc_id": 254,
        "doc": {
            "question_id": 129241311,
            "question": "The ____ approach to KeySwitch performs decomposition before modulus switching to control noise growth.",
            "options": [
                "standard",
                "noise-reduction",
                "modular",
                "hybrid"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ approach to KeySwitch performs decomposition before modulus switching to control noise growth.",
                "Expected Label": "D",
                "Predicted Label": "D) hybrid \n\n**Explanation:**\n\nThe hybrid approach to KeySwitch combines elements of decomposition and modulus switching to manage noise growth effectively. It's not solely reliant on decomposition before modulus switching like option C suggests. \n"
            }
        ]
    },
    {
        "doc_id": 255,
        "doc": {
            "question_id": 129241313,
            "question": "The primary objective of the bootstrapping operation in CKKS FHE is to homomorphically evaluate the ____ operation modulo q on the plaintext.",
            "options": [
                "modular reduction",
                "encryption",
                "key switching",
                "noise generation"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The primary objective of the bootstrapping operation in CKKS FHE is to homomorphically evaluate the ____ operation modulo q on the plaintext.",
                "Expected Label": "A",
                "Predicted Label": "A) modular reduction \n"
            }
        ]
    },
    {
        "doc_id": 256,
        "doc": {
            "question_id": 129241315,
            "question": "To handle non-linear operations like the sigmoid function in the CKKS scheme, we need a _____ approximation.",
            "options": [
                "Fourier Series",
                "Polynomial",
                "Taylor Series",
                "Exponential"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To handle non-linear operations like the sigmoid function in the CKKS scheme, we need a _____ approximation.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) Taylor Series**. Here's why:\n\n* **CKKS (Cheon-Kim-Kim-Song)** is a homomorphic encryption scheme that allows computations on encrypted data. It's particularly well-suited for operations on real or complex numbers.\n\n* **Non-linear functions** like the sigmoid function are crucial in many machine learning applications. However, directly performing them on encrypted data within CKKS is challenging.\n\n* **Taylor Series Approximation:**  The Taylor series provides a way to approximate a function using a polynomial. This is highly beneficial in the CKKS context because:\n    * Polynomials can be evaluated directly on encrypted data using homomorphic operations.\n    * By choosing a sufficient number of terms in the Taylor series, we can control the accuracy of the approximation.\n\n**Why other options are less suitable:**\n\n* **Fourier Series (A):**  While powerful for representing periodic functions, Fourier series aren't the most efficient way to approximate non-linear functions like sigmoid, which are typically not periodic. \n* **Polynomial (B):** While we use a polynomial for approximation, the question asks for the *method* of approximation, making this option too general.\n* **Exponential (D):**  The exponential function itself doesn't directly provide a general method for approximating arbitrary non-linear functions. \n"
            }
        ]
    },
    {
        "doc_id": 257,
        "doc": {
            "question_id": 129241317,
            "question": "____ is a technique used to improve the efficiency of fully homomorphic encryption schemes.",
            "options": [
                "Key expansion",
                "Pipelining",
                "Bootstrapping",
                "Branch prediction"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique used to improve the efficiency of fully homomorphic encryption schemes.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 258,
        "doc": {
            "question_id": 129241319,
            "question": "The operational complexity of the NTT/iNTT operations, crucial for switching between polynomial representations in the CKKS scheme, is given by ____.",
            "options": [
                "O(N)",
                "O(log N)",
                "O(N log N)",
                "O(N^2)"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The operational complexity of the NTT/iNTT operations, crucial for switching between polynomial representations in the CKKS scheme, is given by ____.",
                "Expected Label": "C",
                "Predicted Label": "C) O(N log N) \n"
            }
        ]
    },
    {
        "doc_id": 259,
        "doc": {
            "question_id": 129241323,
            "question": "____ is a type of DIMM where both address/command and data signals are first routed to the buffer chip.",
            "options": [
                "RDIMM",
                "LRDIMM",
                "DDR5 DIMM",
                "Standard DIMM"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a type of DIMM where both address/command and data signals are first routed to the buffer chip.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) LRDIMM**. Here's why:\n\n* **LRDIMM (Load-Reduced DIMM)** features a buffer chip between the memory controller and the DRAM chips. This buffer handles both address/command and data signals, reducing the electrical load on the memory controller and allowing for higher capacities and speeds.\n\nLet's look at why the other options are incorrect:\n\n* **RDIMM (Registered DIMM)** also has a buffer, but it primarily buffers the address and command signals. Data signals still flow directly between the controller and DRAM.\n* **DDR5 DIMM** is a generation of DIMM technology but doesn't inherently specify this buffer configuration.  Both RDIMMs and LRDIMMs exist within the DDR5 standard.\n* **Standard DIMM** (also known as UDIMM - Unbuffered DIMM) does not have any buffer chip.  Signals travel directly between the memory controller and DRAM. \n\n**In summary, the defining characteristic of LRDIMMs is the buffering of both address/command and data signals.** \n"
            }
        ]
    },
    {
        "doc_id": 260,
        "doc": {
            "question_id": 129241325,
            "question": "HBM2 and its successor, HBM3, aim to address the growing need for ____ in modern processors.",
            "options": [
                "lower power consumption",
                "reduced heat dissipation",
                "higher memory bandwidth",
                "larger on-chip cache sizes"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "HBM2 and its successor, HBM3, aim to address the growing need for ____ in modern processors.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nHigh Bandwidth Memory (HBM) technologies like HBM2 and HBM3 are specifically designed to address the **memory bandwidth bottleneck** that modern processors face.  They do this through:\n\n* **Vertical stacking:**  Memory dies are stacked vertically, allowing for a wider interface and thus higher data transfer rates. \n* **Closer proximity to the processor:** HBM is placed closer to the processor (often on the same package), reducing data travel distances and latency.\n\nWhile HBM might offer some power and heat dissipation advantages over traditional DDR memory, its primary goal and impact are on **memory bandwidth**. \n"
            }
        ]
    },
    {
        "doc_id": 261,
        "doc": {
            "question_id": 129241327,
            "question": "Attache uses a three-tier predictor to decide ____.",
            "options": [
                "whether to compress the block",
                "if the block should use ECC",
                "the compression algorithm to use",
                "how much of the block to fetch"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Attache uses a three-tier predictor to decide ____.",
                "Expected Label": "D",
                "Predicted Label": "D) how much of the block to fetch \n\n**Explanation:**\n\nAttache is a technique used in computer architecture to improve the efficiency of fetching data from memory. A three-tier predictor in this context would analyze memory access patterns to predict how much data to fetch, optimizing for situations where fetching an entire block might be wasteful. \n"
            }
        ]
    },
    {
        "doc_id": 262,
        "doc": {
            "question_id": 129241331,
            "question": "The Micron HMC utilizes a ____ interface for connecting to processors and other HMCs.",
            "options": [
                "parallel",
                "serial",
                "photonic",
                "quantum"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The Micron HMC utilizes a ____ interface for connecting to processors and other HMCs.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 263,
        "doc": {
            "question_id": 129241333,
            "question": "The combination of low-cost DRAM chips and parallel interface may lead to ____ networks.",
            "options": [
                "serial",
                "distributed",
                "hybrid serial-parallel",
                "centralized"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The combination of low-cost DRAM chips and parallel interface may lead to ____ networks.",
                "Expected Label": "C",
                "Predicted Label": "B) distributed \n"
            }
        ]
    },
    {
        "doc_id": 264,
        "doc": {
            "question_id": 129241337,
            "question": "In DDR4, a single _____ operation refreshes half the cells compared to a regular refresh operation (FGR-1x).",
            "options": [
                "FGR-2x",
                "FGR-4x",
                "REFpb",
                "REFab"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In DDR4, a single _____ operation refreshes half the cells compared to a regular refresh operation (FGR-1x).",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 265,
        "doc": {
            "question_id": 129241339,
            "question": "The primary approach to reduce refresh energy is to ____ refreshes to rows under certain conditions.",
            "options": [
                "schedule",
                "elide",
                "increase",
                "track"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The primary approach to reduce refresh energy is to ____ refreshes to rows under certain conditions.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) elide**. Here's why:\n\n* **Eliding** refreshes means to strategically skip them entirely when possible. This is a direct way to reduce the energy spent on refreshing DRAM. \n\nLet's look at why the other options aren't the best fit:\n\n* **A) schedule:** While scheduling can optimize *when* refreshes happen, it doesn't inherently reduce their overall energy consumption.\n* **C) increase:** Increasing refreshes would actually *increase* energy consumption, which is the opposite of what we want.\n* **D) track:** Tracking refreshes is useful for management but doesn't directly address reducing energy. \n\n**In summary, eliding refreshes is the most effective way to reduce refresh energy in DRAM.** \n"
            }
        ]
    },
    {
        "doc_id": 266,
        "doc": {
            "question_id": 129241341,
            "question": "In a memristor crossbar, the current through a bitline represents a ____ of the vector of input voltages and the vector of conductances for the cells connected to that bitline.",
            "options": [
                "sum",
                "difference",
                "dot-product",
                "cross-product"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a memristor crossbar, the current through a bitline represents a ____ of the vector of input voltages and the vector of conductances for the cells connected to that bitline.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 267,
        "doc": {
            "question_id": 129241343,
            "question": "Moving compute closer to the ____ in solid state drives (SSDs) offers higher bandwidth but introduces challenges in managing frequent errors.",
            "options": [
                "controller",
                "NAND dies",
                "cache",
                "DRAM"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Moving compute closer to the ____ in solid state drives (SSDs) offers higher bandwidth but introduces challenges in managing frequent errors.",
                "Expected Label": "B",
                "Predicted Label": "B) NAND dies \n\n**Explanation:**\n\nMoving compute closer to the NAND dies in SSDs is the essence of computational storage.  While this offers significant bandwidth advantages by processing data near its storage location, NAND flash memory is known for its susceptibility to errors, especially as it ages. Managing these errors effectively becomes a significant challenge in such architectures. \n"
            }
        ]
    },
    {
        "doc_id": 268,
        "doc": {
            "question_id": 129241345,
            "question": "To offer durable storage to numerous applications, storage devices in a WSC are connected to the ____",
            "options": [
                "data center network",
                "server board",
                "accelerator board",
                "disk tray"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To offer durable storage to numerous applications, storage devices in a WSC are connected to the ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nStorage devices in a Warehouse-Scale Computer (WSC) need to be accessible by many servers running various applications. This necessitates a network connection for shared access, making the **data center network (A)** the correct answer.\n\nHere's why the other options are incorrect:\n\n* **B) Server board:**  Storage directly connected to a single server board limits its accessibility to other servers.\n* **C) Accelerator board:** Accelerator boards (like GPUs) are for computation, not storage.\n* **D) Disk tray:** A disk tray is a physical component of a server and doesn't imply network connectivity. \n"
            }
        ]
    },
    {
        "doc_id": 269,
        "doc": {
            "question_id": 129241347,
            "question": "____ is a popular open-source program that orchestrates resource management functions for container-based workloads in large-scale computing environments.",
            "options": [
                "Docker",
                "Kubernetes",
                "VMware",
                "Borg"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a popular open-source program that orchestrates resource management functions for container-based workloads in large-scale computing environments.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 270,
        "doc": {
            "question_id": 129241351,
            "question": "____ latency refers to the latency of the slowest requests in a system.",
            "options": [
                "Median",
                "Average",
                "Mean",
                "Tail"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ latency refers to the latency of the slowest requests in a system.",
                "Expected Label": "D",
                "Predicted Label": "D) Tail \n\n**Explanation:**\n\nTail latency specifically focuses on the higher percentiles of request latencies (e.g., 95th, 99th percentile). These represent the slowest requests that can significantly impact user experience, especially in scenarios where responsiveness is critical. \n"
            }
        ]
    },
    {
        "doc_id": 271,
        "doc": {
            "question_id": 129241353,
            "question": "____ is the ratio between the number of downlinks and uplinks in a Top of Rack (TOR) switch, determining how much the intra-rack fabric is over-provisioned with respect to the data center fabric.",
            "options": [
                "Oversubscription ratio",
                "Bandwidth ratio",
                "Port allocation",
                "Network provisioning factor"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the ratio between the number of downlinks and uplinks in a Top of Rack (TOR) switch, determining how much the intra-rack fabric is over-provisioned with respect to the data center fabric.",
                "Expected Label": "A",
                "Predicted Label": "A) Oversubscription ratio \n"
            }
        ]
    },
    {
        "doc_id": 272,
        "doc": {
            "question_id": 129241355,
            "question": "Each ____  consists of two Tensor cores, each equipped with a systolic array for matrix computations (MXU) and a connection to high bandwidth memory (HBM).",
            "options": [
                "GPU",
                "TPUv2",
                "CPU",
                "ASIC"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Each ____  consists of two Tensor cores, each equipped with a systolic array for matrix computations (MXU) and a connection to high bandwidth memory (HBM).",
                "Expected Label": "B",
                "Predicted Label": "B) TPUv2 \n\n**Explanation:**\n\n* **TPUv2 (Tensor Processing Unit v2)** is known for its Tensor Cores, specifically designed for matrix multiplication and convolution operations crucial for deep learning.  The systolic array within the Tensor Core enables highly parallel processing of these operations.\n\nLet's look at why the other options aren't the best fit:\n\n* **GPUs** also have cores optimized for matrix operations, but they are not typically referred to as \"Tensor Cores\" and don't always have a dedicated systolic array.\n* **CPUs** are general-purpose processors and lack the specialized hardware for efficient matrix computations found in TPUs.\n* **ASICs** are application-specific integrated circuits. While a TPU is a type of ASIC, the description specifically points to the architecture of a TPUv2. \n"
            }
        ]
    },
    {
        "doc_id": 273,
        "doc": {
            "question_id": 129241359,
            "question": "____ are the last layer in the transformation and distribution architecture and route individual circuits to the computer cabinets.",
            "options": [
                "UPS systems",
                "Diesel generators",
                "Switchgear",
                "PDUs"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are the last layer in the transformation and distribution architecture and route individual circuits to the computer cabinets.",
                "Expected Label": "D",
                "Predicted Label": "D) PDUs \n\n**PDUs (Power Distribution Units)** are the final step in delivering power to equipment within a rack or cabinet. They take power from the UPS or generator (via switchgear) and distribute it to individual servers and devices.\n\nHere's why the other options are incorrect:\n\n* **UPS systems (Uninterruptible Power Supply)** provide temporary backup power during outages.\n* **Diesel generators** are a longer-term backup power source.\n* **Switchgear** manages and protects the flow of high-voltage electricity, typically further upstream in the power distribution system. \n"
            }
        ]
    },
    {
        "doc_id": 274,
        "doc": {
            "question_id": 129241361,
            "question": "CRAC units maintain positive pressure in the raised floor plenum by forcing ____ air into the underfloor space, which then escapes through perforated tiles in front of the server racks.",
            "options": [
                "cold",
                "hot",
                "ambient",
                "outside"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "CRAC units maintain positive pressure in the raised floor plenum by forcing ____ air into the underfloor space, which then escapes through perforated tiles in front of the server racks.",
                "Expected Label": "A",
                "Predicted Label": "A) cold \n"
            }
        ]
    },
    {
        "doc_id": 275,
        "doc": {
            "question_id": 129241363,
            "question": "Power usage effectiveness (PUE) reflects the quality of the data center ______.",
            "options": [
                "IT equipment",
                "building infrastructure",
                "server performance",
                "network efficiency"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Power usage effectiveness (PUE) reflects the quality of the data center ______.",
                "Expected Label": "B",
                "Predicted Label": "B \n\nPower usage effectiveness (PUE) specifically measures how efficiently the **building infrastructure** of a data center uses energy. It compares total facility power to IT equipment power, indicating how much energy is lost to cooling, lighting, etc. \n"
            }
        ]
    },
    {
        "doc_id": 276,
        "doc": {
            "question_id": 129241369,
            "question": "In systems with traditional block-based storage media, operating systems accelerate access by caching recently accessed pages in ____.",
            "options": [
                "DRAM",
                "HDD",
                "SSD",
                "PM"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In systems with traditional block-based storage media, operating systems accelerate access by caching recently accessed pages in ____.",
                "Expected Label": "A",
                "Predicted Label": "A) DRAM \n\n**Explanation:**\n\nDRAM (Dynamic Random Access Memory) is the primary memory used for caching recently accessed pages by the operating system. This is because DRAM is significantly faster than other storage media options like HDDs and SSDs, even though it is volatile (loses data when powered off). \n\nLet's break down why the other options are less suitable:\n\n* **HDD (Hard Disk Drive):** HDDs are much slower than DRAM and using them for caching would defeat the purpose of accelerating access.\n* **SSD (Solid State Drive):**  While faster than HDDs, SSDs are still considerably slower than DRAM. They are more suitable for secondary storage.\n* **PM (Persistent Memory):** PM technologies are an emerging area, but they are not yet commonly used as the primary caching mechanism due to cost and maturity. \n\n**Therefore, DRAM is the most efficient and widely used solution for caching recently accessed pages to improve system performance.** \n"
            }
        ]
    },
    {
        "doc_id": 277,
        "doc": {
            "question_id": 129241373,
            "question": "The ARMv8.2 ISA introduced the ____ instruction for cache flush operations to the point of persistence.",
            "options": [
                "CLWB",
                "PCOMMIT",
                "DC CVAP",
                "SFENCE"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ARMv8.2 ISA introduced the ____ instruction for cache flush operations to the point of persistence.",
                "Expected Label": "C",
                "Predicted Label": "B) PCOMMIT \n"
            }
        ]
    },
    {
        "doc_id": 278,
        "doc": {
            "question_id": 129241377,
            "question": "____ is an ordering relation over memory operations prescribed by the memory persistency model.",
            "options": [
                "Persist Memory Order (PMO)",
                "Volatile Memory Order (VMO)",
                "Strict Persistency Order (SPO)",
                "Buffered Strict Order (BSO)"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is an ordering relation over memory operations prescribed by the memory persistency model.",
                "Expected Label": "A",
                "Predicted Label": "C \n\n**C) Strict Persistency Order (SPO)** \n\n**Explanation:**\n\nWhile options like PMO, VMO, and BSO might sound plausible, they are not standard terms related to memory consistency models. \n\n* **Strict Persistency Order (SPO)** refers to the order in which memory operations, particularly writes, become visible to other processors or devices in a system. It's a fundamental concept in defining the memory consistency model and ensuring data coherence. \n"
            }
        ]
    },
    {
        "doc_id": 279,
        "doc": {
            "question_id": 129241379,
            "question": "The size of the ____ determines the maximum number of instructions that must be replayed in case of a system failure in the batch persistency model.",
            "options": [
                "Write buffer",
                "Batch",
                "Cache line",
                "Log"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The size of the ____ determines the maximum number of instructions that must be replayed in case of a system failure in the batch persistency model.",
                "Expected Label": "B",
                "Predicted Label": "B) Batch \n\nHere's why:\n\n* **Batch Persistency Model:**  This model ensures data consistency by grouping operations into \"batches.\" Only when an entire batch is successfully completed are the changes made persistent (meaning they survive crashes). If a failure occurs during a batch, the entire batch is replayed. \n\nLet's analyze why the other options are incorrect:\n\n* **Write Buffer (A):** A write buffer temporarily holds data before it's written to main memory. While it plays a role in data consistency, it doesn't directly determine the number of instructions replayed in the batch persistency model.\n* **Cache Line (C):** Cache lines are units of data transfer between cache and main memory. They are unrelated to the scope of instruction replay in a batch.\n* **Log (D):**  Logs record operations and can be used for recovery after a failure. However, the log size doesn't inherently dictate the maximum instructions replayed in the batch persistency model.\n\n**In summary, the size of the batch itself defines the maximum amount of work that might need to be redone if a failure happens before the batch completes.** \n"
            }
        ]
    },
    {
        "doc_id": 280,
        "doc": {
            "question_id": 129241381,
            "question": "In buffered epoch persistency, persists must drain to PM in their ____ order.",
            "options": [
                "visibility",
                "buffer",
                "epoch",
                "execution"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In buffered epoch persistency, persists must drain to PM in their ____ order.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(A) visibility**. Here's why:\n\n* **Buffered epoch persistency** is a technique used in persistent memory systems to improve performance. Instead of writing data directly to persistent memory on every update, changes are first buffered and then written in batches. \n\n* The **visibility order** refers to the order in which writes made by different threads or processes become visible to other threads/processes.  \n\n* It's crucial to drain persists (writes) to persistent memory in their **visibility order** to maintain data consistency. If a write that was supposed to be visible *after* another is flushed to persistent memory first, it could lead to inconsistent states in the case of a crash.\n\nLet me know if you'd like more explanation on any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 281,
        "doc": {
            "question_id": 129241383,
            "question": "In buffered epoch persistency models, ____ ensures that persists are performed to persistent memory in the correct order.",
            "options": [
                "The processor",
                "The operating system",
                "The application software",
                "The cache subsystem"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In buffered epoch persistency models, ____ ensures that persists are performed to persistent memory in the correct order.",
                "Expected Label": "D",
                "Predicted Label": "**B) The operating system**\n\n**Explanation:**\n\nIn buffered epoch persistency models, the operating system plays a crucial role in ensuring the correct order of persists to persistent memory. Here's why:\n\n* **Buffer Management:** The OS manages a buffer cache for persistent memory.  Writes are often first directed to this buffer. \n* **Flush Ordering:** The OS is responsible for flushing the buffer to persistent memory. It guarantees that the flushes, and therefore the persists, happen in the correct order to maintain data consistency.\n\n**The other options are not primarily responsible:**\n\n* **Processor:** Executes instructions but doesn't inherently manage persist ordering at a high level.\n* **Application Software:**  Issues write requests but relies on the underlying system for persistence guarantees.\n* **Cache Subsystem:** Primarily focused on performance optimization and doesn't inherently guarantee persist ordering across different operations. \n"
            }
        ]
    },
    {
        "doc_id": 282,
        "doc": {
            "question_id": 129241387,
            "question": "The ____ primitive ensures that prior persists complete before any subsequent persists are issued.",
            "options": [
                "new strand",
                "join strand",
                "persist barrier",
                "conflicting barrier"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ primitive ensures that prior persists complete before any subsequent persists are issued.",
                "Expected Label": "B",
                "Predicted Label": "C \n\n**Explanation:**\n\nA **persist barrier (C)** acts as a synchronization point within a processor or between the processor and memory system.  It guarantees that all previous store (write) instructions will be visible to other threads or devices before any subsequent store instructions are initiated.\n\nLet's look at why the other options are incorrect:\n\n* **A) new strand:** This refers to creating a new thread of execution and doesn't directly relate to ensuring persistence order.\n* **B) join strand:** This refers to waiting for a specific thread to complete its execution. While it relates to synchronization, it doesn't guarantee the persistence order of memory operations.\n* **D) conflicting barrier:** This term is not a standard concept in computer architecture. Barriers typically deal with synchronization in a broader sense, not specifically persistence ordering. \n\n**Therefore, the persist barrier is the most appropriate primitive to ensure the completion of prior persists before new ones.** \n"
            }
        ]
    },
    {
        "doc_id": 283,
        "doc": {
            "question_id": 129241389,
            "question": "The ____ is responsible for managing the order in which persists drain to persistent memory in the StrandWeaver architecture.",
            "options": [
                "persist queue",
                "store queue",
                "strand buffer unit",
                "write-back buffer"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is responsible for managing the order in which persists drain to persistent memory in the StrandWeaver architecture.",
                "Expected Label": "C",
                "Predicted Label": "**A) persist queue** \n\nThe persist queue is specifically designed in StrandWeaver to manage the order of persistent data writes. \n"
            }
        ]
    },
    {
        "doc_id": 284,
        "doc": {
            "question_id": 129241393,
            "question": "______ is a liveness property that insists that threads not execute forever without making forward progress.",
            "options": [
                "Livelock freedom",
                "Starvation freedom",
                "Deadlock freedom",
                "Fairness"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is a liveness property that insists that threads not execute forever without making forward progress.",
                "Expected Label": "A",
                "Predicted Label": "B) Starvation freedom \n"
            }
        ]
    },
    {
        "doc_id": 285,
        "doc": {
            "question_id": 129241397,
            "question": "The ____ algorithm is an example of a deadlock avoidance strategy.",
            "options": [
                "Peterson's",
                "Lamport's bakery",
                "Banker's",
                "Dining Philosophers'"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ algorithm is an example of a deadlock avoidance strategy.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 286,
        "doc": {
            "question_id": 129241409,
            "question": "The ____ method in a seqlock implementation allows a reader to check if its critical section execution overlapped with a writer.",
            "options": [
                "reader validate",
                "become writer",
                "writer acquire",
                "writer release"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ method in a seqlock implementation allows a reader to check if its critical section execution overlapped with a writer.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe **reader validate** method is crucial in seqlock implementations. Here's why:\n\n* **Seqlocks** are lightweight synchronization primitives that optimize for the case where reads are much more frequent than writes. They allow concurrent readers even if a writer is active, potentially sacrificing data consistency for speed.\n* To ensure some level of consistency, readers use a sequence counter. They record this counter before and after accessing the protected data. \n* The **reader validate** method compares these two recorded sequence numbers. If they match, the read was consistent (no writer interfered). If they differ, a writer modified the data during the read, indicating a potential inconsistency, and the reader may need to retry.\n\n**Let's analyze the other options:**\n\n* **become writer, writer acquire, writer release:** These relate to the writer's interaction with the seqlock, not the reader's validation process. \n\nLet me know if you would like a deeper dive into seqlocks or other synchronization mechanisms! \n"
            }
        ]
    },
    {
        "doc_id": 287,
        "doc": {
            "question_id": 129241411,
            "question": "If we let C equal _____, the semaphore functions as a mutual exclusion lock.",
            "options": [
                "0",
                "1",
                "N",
                "C+1"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If we let C equal _____, the semaphore functions as a mutual exclusion lock.",
                "Expected Label": "B",
                "Predicted Label": "**B) 1**\n\n**Explanation:**\n\nA semaphore initialized to 1 functions as a mutual exclusion lock. \n\nHere's why:\n\n* **Initialization to 1:**  This means only one process can acquire the lock initially.\n* **Wait (P) Operation:** Decrements the semaphore. If the semaphore value becomes 0, the process acquires the lock. If the value is less than 0, the process blocks.\n* **Signal (V) Operation:** Increments the semaphore. If there are waiting processes, one is unblocked and allowed to acquire the lock.\n\nTherefore, a semaphore initialized to 1 ensures that only one process can access a critical section at a time, effectively acting as a mutual exclusion lock. \n"
            }
        ]
    },
    {
        "doc_id": 288,
        "doc": {
            "question_id": 129241413,
            "question": "____ is a barrier generalization that allows threads to dynamically join or leave, and participate as signalers, waiters, or both.",
            "options": [
                "Fuzzy barrier",
                "Topological barrier",
                "Phaser",
                "Forall loop"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a barrier generalization that allows threads to dynamically join or leave, and participate as signalers, waiters, or both.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 289,
        "doc": {
            "question_id": 129241415,
            "question": "The Harris & Michael (H&M) list algorithm leverages a two-step process for node deletion to primarily address the ____ problem.",
            "options": [
                "Memory leak",
                "Deadlock ",
                "ABA ",
                "Cache coherence"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Harris & Michael (H&M) list algorithm leverages a two-step process for node deletion to primarily address the ____ problem.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C\n\n**Explanation:**\n\nThe Harris & Michael (H&M) algorithm is specifically designed to solve the ABA problem in lock-free data structures. \n\nHere's why the other options are incorrect:\n\n* **A) Memory Leak:** Memory leaks occur when allocated memory is not properly released. H&M doesn't directly address this.\n* **B) Deadlock:** Deadlocks happen when processes are stuck waiting for each other to release resources. H&M doesn't primarily target this.\n* **D) Cache Coherence:** Cache coherence is related to maintaining data consistency across multiple caches in a multi-processor system. This is not the focus of the H&M algorithm. \n"
            }
        ]
    },
    {
        "doc_id": 290,
        "doc": {
            "question_id": 129241421,
            "question": "The process of replicating input feature map values into a structured matrix for convolution computation is referred to as creating a ____ matrix.",
            "options": [
                "Hessian",
                "Toeplitz",
                "Fourier",
                "Identity"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The process of replicating input feature map values into a structured matrix for convolution computation is referred to as creating a ____ matrix.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 291,
        "doc": {
            "question_id": 129241423,
            "question": "The spatial reuse distance is defined as the maximum number of data accesses between any pair of ____ that access the same data value.",
            "options": [
                "memory levels",
                "NoC channels",
                "L2 caches",
                "consumers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The spatial reuse distance is defined as the maximum number of data accesses between any pair of ____ that access the same data value.",
                "Expected Label": "D",
                "Predicted Label": "D) consumers \n\n**Explanation:**\n\nSpatial reuse distance refers to the distance (measured in data accesses) between different parts of a system that use the same data. The term \"consumers\" is the most general and accurate way to describe these different parts, which could be:\n\n* **Processing cores:** In a multi-core processor, different cores might access the same data.\n* **Threads:** Within a core, different threads might access the same data.\n* **Instructions:** Even within a single thread, different instructions might access the same data.\n\nThe other options are not as accurate:\n\n* **Memory levels:**  While data is transferred between memory levels, spatial reuse distance focuses on consumers of the data, not just its location.\n* **NoC channels:** NoC channels are communication pathways, not consumers of data.\n* **L2 caches:** L2 caches are a specific type of memory hierarchy level, not a general representation of data consumers. \n"
            }
        ]
    },
    {
        "doc_id": 292,
        "doc": {
            "question_id": 129241425,
            "question": "____ is a technique used in DNN accelerators to efficiently handle data movement without requiring runtime routing.",
            "options": [
                "Circuit-switched routing",
                "Packet-switched routing",
                "Store-and-forward routing",
                "Wormhole routing"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in DNN accelerators to efficiently handle data movement without requiring runtime routing.",
                "Expected Label": "A",
                "Predicted Label": "D) Wormhole routing \n\n**Explanation:**\n\nWormhole routing is a network routing technique often employed in DNN accelerators. It allows data packets to be divided into smaller units (flits) and transmitted across the network.  Here's why it fits best:\n\n* **Efficiency:** Wormhole routing minimizes latency by establishing a direct path between source and destination for the entire data packet (even if it's divided into flits), reducing the overhead of traditional routing decisions at each intermediate node.\n\nLet's look at why the other options are not as suitable:\n\n* **Circuit-switched routing:** This establishes a dedicated path for the entire duration of transmission. While it offers guaranteed bandwidth, it's less flexible and efficient for the dynamic data patterns in DNNs.\n* **Packet-switched routing:**  Involves routing decisions at each node, leading to higher latency, especially for the large data volumes in DNNs.\n* **Store-and-forward routing:**  Similar to packet-switched, it stores the entire packet at each node before forwarding, adding significant latency.\n\n**In essence, wormhole routing's ability to efficiently transmit data without complex runtime routing decisions makes it a preferred choice for DNN accelerators.** \n"
            }
        ]
    },
    {
        "doc_id": 293,
        "doc": {
            "question_id": 129241429,
            "question": "A wide data range may result in higher ____. ",
            "options": [
                "latency",
                "bandwidth consumption",
                "quantization error",
                "power consumption"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Benchmarking and Measurement"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A wide data range may result in higher ____. ",
                "Expected Label": "C",
                "Predicted Label": "D) power consumption \n\n**Explanation:**\n\nA wider data range means representing numbers with more bits.  More bits:\n\n* **Increased Power Consumption:** Processing and storing more bits requires more transistors switching, leading to higher power consumption. \n\nLet's look at why the other options are less likely:\n\n* **A) Latency:** While wider data paths can slightly impact latency due to more complex circuits, it's not the primary effect.\n* **B) Bandwidth Consumption:** This is more related to the amount of data being transferred, not the data width itself.\n* **C) Quantization Error:** Wider data ranges actually *reduce* quantization error by allowing for finer-grained representation of values. \n"
            }
        ]
    },
    {
        "doc_id": 294,
        "doc": {
            "question_id": 129241431,
            "question": "____ can be used to indicate which pixels or activations are temporally correlated between consecutive frames in video processing.",
            "options": [
                "Frame interpolation",
                "Pixel shuffling",
                "Temporal dithering",
                "Motion vectors"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ can be used to indicate which pixels or activations are temporally correlated between consecutive frames in video processing.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 295,
        "doc": {
            "question_id": 129241433,
            "question": "In the tree-based representation of a sparse tensor, a ____ represents a collection of coordinates at a particular level.",
            "options": [
                "fiber",
                "rank",
                "node",
                "coordinate"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In the tree-based representation of a sparse tensor, a ____ represents a collection of coordinates at a particular level.",
                "Expected Label": "A",
                "Predicted Label": "C) node \n\n**Explanation:**\n\nIn the tree-based representation of sparse tensors, each **node** typically corresponds to a specific coordinate value at a particular level of the tree. The path from the root to a node defines a sequence of coordinates, and the node itself can store a non-zero value or point to further nodes at deeper levels. \n\nLet's break down why other options are incorrect:\n\n* **A) fiber:** A fiber represents all the non-zero elements along a specific dimension in a tensor. \n* **B) rank:** Rank refers to the number of dimensions in a tensor.\n* **D) coordinate:** A coordinate is a single value that specifies a position within a dimension of a tensor.\n\nTherefore, \"**node**\" is the most accurate term for representing a collection of coordinates at a particular level in a tree-based sparse tensor representation. \n"
            }
        ]
    },
    {
        "doc_id": 296,
        "doc": {
            "question_id": 129241437,
            "question": "In sparse dataflows, ____ can occur when splitting fibers uniformly in coordinate space, potentially leading to underutilization of processing elements.",
            "options": [
                "Data duplication",
                "Memory overflow",
                "Load imbalance",
                "Cache coherence issues"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In sparse dataflows, ____ can occur when splitting fibers uniformly in coordinate space, potentially leading to underutilization of processing elements.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 297,
        "doc": {
            "question_id": 129241439,
            "question": "In NVM-based processing-in-memory, the ______ of a resistive memory element (memristor) is used to represent a filter weight.",
            "options": [
                "voltage",
                "current",
                "power consumption",
                "conductance"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In NVM-based processing-in-memory, the ______ of a resistive memory element (memristor) is used to represent a filter weight.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 298,
        "doc": {
            "question_id": 129666970,
            "question": "____ is typically the most significant bottleneck in hardware design for deep learning platforms.",
            "options": [
                "Compute units",
                "Main memory bandwidth",
                "Number of transistors",
                "Power consumption"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically the most significant bottleneck in hardware design for deep learning platforms.",
                "Expected Label": "B",
                "Predicted Label": "B) Main memory bandwidth \n"
            }
        ]
    },
    {
        "doc_id": 299,
        "doc": {
            "question_id": 129666972,
            "question": "The primary advantage of using a pooling layer in a convolutional neural network is that it allows filters to have a larger ____ on the input.",
            "options": [
                "number of channels",
                "receptive field",
                "computational complexity",
                "memory footprint"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The primary advantage of using a pooling layer in a convolutional neural network is that it allows filters to have a larger ____ on the input.",
                "Expected Label": "B",
                "Predicted Label": "B) receptive field \n"
            }
        ]
    },
    {
        "doc_id": 300,
        "doc": {
            "question_id": 129666974,
            "question": "Deep Learning Recommendation Model (DLRM) uses ________ to predict event probabilities.",
            "options": [
                "fully connected layers",
                "convolutional layers",
                "recurrent layers",
                "embedding vectors and post-processed dense features"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Deep Learning Recommendation Model (DLRM) uses ________ to predict event probabilities.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 301,
        "doc": {
            "question_id": 129666976,
            "question": "______ is a technique used in neural machine translation to select output words during decoding.",
            "options": [
                "Beam search",
                "Hidden Markov model",
                "Recurrent neural network",
                "Convolutional neural network"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is a technique used in neural machine translation to select output words during decoding.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 302,
        "doc": {
            "question_id": 129666980,
            "question": "___ dynamically turns on or off the adaptive learning rate depending on the variance confidence.",
            "options": [
                "LAMB",
                "SWA",
                "RAdam",
                "SGDM"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "___ dynamically turns on or off the adaptive learning rate depending on the variance confidence.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **RAdam (Rectified Adam)** specifically addresses the issue of high variance in the adaptive learning rate early in training. It introduces a \"rectification\" term that dynamically switches between a constant learning rate and the adaptive learning rate based on variance confidence.\n\nLet's break down why the other options are incorrect:\n\n* **LAMB (Layer-wise Adaptive Moments for Batch Training)** is an optimizer designed for large batch training and focuses on layer-wise adaptive learning rates, but doesn't have the dynamic variance-based switching mechanism of RAdam.\n* **SWA (Stochastic Weight Averaging)** is a technique for improving generalization by averaging model weights from different points during training. It doesn't directly control the learning rate.\n* **SGDM (Stochastic Gradient Descent with Momentum)** uses momentum to smooth out the gradient updates but doesn't have the adaptive learning rate mechanism found in RAdam. \n"
            }
        ]
    },
    {
        "doc_id": 303,
        "doc": {
            "question_id": 129666986,
            "question": "____ is a technique where an ensemble of students collaboratively learn and teach others by sharing their softmax outputs.",
            "options": [
                "Knowledge Distillation",
                "Deep Mutual Learning",
                "Teacher Assistant",
                "Softmax Temperature"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique where an ensemble of students collaboratively learn and teach others by sharing their softmax outputs.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 304,
        "doc": {
            "question_id": 129666988,
            "question": "____ cache type provides a compromise between access time and conflicts.",
            "options": [
                "Fully Associative",
                "Direct Mapped",
                "N-way Set-Associative",
                "Hybrid"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ cache type provides a compromise between access time and conflicts.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Fully Associative:** Offers the lowest conflict rate (and potential for best hit rate) but is the most complex and slowest to access. \n* **Direct Mapped:**  Simplest and fastest access but suffers from the highest conflict rate.\n* **N-way Set-Associative:**  Strikes a balance. It divides the cache into sets, allowing a limited number (N) of possible locations for data within each set. This reduces conflicts compared to direct-mapped while being faster than fully associative.\n* **Hybrid:**  Usually refers to combining multiple levels of cache (e.g., L1, L2) rather than a single cache organization type. \n\n**Therefore, N-way Set-Associative (C) best fits the description of a compromise between access time and conflicts.** \n"
            }
        ]
    },
    {
        "doc_id": 305,
        "doc": {
            "question_id": 129666990,
            "question": "The theoretical peak bandwidth per direction for PCIe 3.0 with 16 lanes is ____ GB/s.",
            "options": [
                "8",
                "16",
                "31.5",
                "63"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The theoretical peak bandwidth per direction for PCIe 3.0 with 16 lanes is ____ GB/s.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) 16**. Here's why:\n\n* **PCIe 3.0 data rate per lane:** 8 GT/s (GigaTransfers per second)\n* **Encoding scheme:** 128b/130b encoding (meaning 130 bits transmitted for every 128 bits of data)\n* **Number of lanes:** 16\n\n**Calculation:**\n\n(8 GT/s/lane) * (128 data bits / 130 transmitted bits) * 16 lanes = 15.75 GB/s/direction \u2248 **16 GB/s/direction** \n"
            }
        ]
    },
    {
        "doc_id": 306,
        "doc": {
            "question_id": 129666992,
            "question": "The typical split of virtual address space between user and kernel memory in 32-bit Linux is _____ for user space and _____ for kernel space.",
            "options": [
                "3 GB, 1 GB",
                "2 GB, 2 GB",
                "1 GB, 3 GB",
                "4 GB, 0 GB"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The typical split of virtual address space between user and kernel memory in 32-bit Linux is _____ for user space and _____ for kernel space.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 307,
        "doc": {
            "question_id": 129666996,
            "question": "The process of traversing the levels of a page table is known as a ____.",
            "options": [
                "page table translation",
                "page table walk",
                "page table hierarchy",
                "page table mapping"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The process of traversing the levels of a page table is known as a ____.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 308,
        "doc": {
            "question_id": 129666998,
            "question": "A commonly employed approach to improve performance with hashed inverted page tables is to use a ____.",
            "options": [
                "Collision chain",
                "Dynamically sized page table",
                "Multi-level radix page table",
                "Hash Anchor Table"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A commonly employed approach to improve performance with hashed inverted page tables is to use a ____.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nHash Anchor Tables are specifically designed to improve the performance of hashed inverted page tables. Here's why:\n\n* **Hash collisions:** Inverted page tables suffer from collisions where multiple virtual addresses map to the same hash table entry.\n* **Hash Anchor Table solution:** These tables mitigate collision chains (which slow down lookups) by providing additional hash functions or indexing schemes to quickly resolve collisions.\n\nLet's look at why the other options are less suitable:\n\n* **A) Collision chain:** This is the problem Hash Anchor Tables aim to solve, not a solution itself.\n* **B) Dynamically sized page table:** While helpful for memory management, it doesn't directly address the performance issues of hashed tables.\n* **C) Multi-level radix page table:** This is a technique used in traditional page tables, not specifically with hashed inverted page tables. \n\n**Therefore, D) Hash Anchor Table is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 309,
        "doc": {
            "question_id": 129667000,
            "question": "____ bits in page table entries are used to identify memory-resident pages with data that needs to be written back to the stale backing-store.",
            "options": [
                "Dirty",
                "Accessed",
                "Global",
                "Permission"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ bits in page table entries are used to identify memory-resident pages with data that needs to be written back to the stale backing-store.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Dirty bits** are specifically designed to track modifications. If a page has been written to (modified), its dirty bit is set. This signals that the page in memory differs from the backing store, and the data needs to be written back before the page can be evicted.\n\nLet's look at why the other options are incorrect:\n\n* **Accessed bits** track whether a page has been read, used for page replacement algorithms (like LRU).\n* **Global bits** typically relate to whether a page is shared across multiple processes.\n* **Permission bits** define the type of access allowed (read, write, execute) for a page. \n"
            }
        ]
    },
    {
        "doc_id": 310,
        "doc": {
            "question_id": 129667002,
            "question": "In Linux, _____ data structures are used to implement VMA trees for efficient lookup operations.",
            "options": [
                "Hash Table",
                "Red-Black Tree",
                "Linked List",
                "Queue"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In Linux, _____ data structures are used to implement VMA trees for efficient lookup operations.",
                "Expected Label": "B",
                "Predicted Label": "B) Red-Black Tree \n\nHere's why:\n\n* **Red-Black Trees** are self-balancing binary search trees. This self-balancing property ensures logarithmic time complexity for search, insertion, and deletion operations, making them efficient for managing VMAs (Virtual Memory Areas) which need frequent lookups and modifications.\n\nLet's look at why the other options are not suitable:\n\n* **Hash Tables**  while offering fast average-case lookups, don't handle ordered operations well, and their worst-case performance can be linear.\n* **Linked Lists** have linear search time, making them inefficient for large VMA sets.\n* **Queues** are linear data structures meant for first-in, first-out operations and are unsuitable for the dynamic lookup needs of VMA management. \n"
            }
        ]
    },
    {
        "doc_id": 311,
        "doc": {
            "question_id": 129667004,
            "question": "____ refers to holes in memory that are not reusable because any single one is smaller than the size of a new memory region being allocated.",
            "options": [
                "Internal fragmentation",
                "Segmentation fault",
                "External fragmentation",
                "Memory leak"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ refers to holes in memory that are not reusable because any single one is smaller than the size of a new memory region being allocated.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 312,
        "doc": {
            "question_id": 129667008,
            "question": "A ____ connects code to functions in dynamically linked libraries and is often lazily updated, resulting in instruction memory writes.",
            "options": [
                "procedure linkage table (PLT)",
                "translation lookaside buffer (TLB)",
                "instruction register (IR)",
                "memory management unit (MMU)"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ connects code to functions in dynamically linked libraries and is often lazily updated, resulting in instruction memory writes.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 313,
        "doc": {
            "question_id": 129667012,
            "question": "____ are used to accelerate nested page table lookups by caching guest physical page to system physical page translations.",
            "options": [
                "TLBs",
                "MMU caches",
                "nTLBs",
                "L1 caches"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ are used to accelerate nested page table lookups by caching guest physical page to system physical page translations.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C) nTLBs \n\n**Explanation:**\n\n* **nTLBs (Nested Translation Lookaside Buffers)** are specifically designed to handle the nested page table walks required in virtualized environments. They cache mappings between guest physical addresses and the underlying system physical addresses, significantly speeding up the translation process.\n\nLet's look at why the other options are not the best fit:\n\n* **TLBs (Translation Lookaside Buffers)**:  These cache page table entries for a single address space. In a virtualized system, they would only cache the mapping from the guest virtual address to the guest physical address, requiring further lookups.\n* **MMU caches**: This is too general. While nTLBs are part of the MMU, the term \"MMU cache\" doesn't specifically refer to the mechanism for nested page table acceleration.\n* **L1 caches**: These are CPU caches for general data and instructions, not involved in address translation. \n"
            }
        ]
    },
    {
        "doc_id": 314,
        "doc": {
            "question_id": 129667014,
            "question": "TLB ____ leverages patterns in page table entries to improve TLB reach without increasing TLB size.",
            "options": [
                "clustering",
                "sub-blocking",
                "deduplication",
                "coalescing"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "TLB ____ leverages patterns in page table entries to improve TLB reach without increasing TLB size.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **(A) clustering**. Here's why:\n\n* **TLB Clustering** exploits the principle of locality. If a program accesses a particular page, it's likely to access other nearby pages soon. TLB clustering mechanisms group related page table entries together, increasing the chance that multiple entries needed by the program will reside in the TLB simultaneously. \n\nLet's break down why the other options are less suitable:\n\n* **Sub-blocking** is a technique used in caches to divide larger cache lines into smaller blocks, improving efficiency but not directly related to TLB reach.\n* **Deduplication** focuses on eliminating redundant data, which isn't a primary concern within the TLB's function.\n* **Coalescing** typically refers to merging operations or requests to reduce overhead, and it doesn't have a direct connection to improving TLB reach. \n"
            }
        ]
    },
    {
        "doc_id": 315,
        "doc": {
            "question_id": 129667020,
            "question": "Fine-grained multithreading enables a processor to switch between several active threads every _____.",
            "options": [
                "Instruction",
                "Cycle",
                "Millisecond",
                "Context Switch"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Fine-grained multithreading enables a processor to switch between several active threads every _____.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Cycle**. \n\nHere's why:\n\n* **Fine-grained multithreading** is designed to maximize thread interleaving. It aims to switch threads rapidly, even within the execution of a single instruction.\n* **Cycle** refers to the basic time unit of a processor's operation. Switching threads every cycle allows for the highest degree of interleaving.\n\nLet me know if you'd like more explanation on multithreading concepts! \n"
            }
        ]
    },
    {
        "doc_id": 316,
        "doc": {
            "question_id": 129667024,
            "question": "____ is a memory technology that enables future Niagara chips to continue increasing the number of on-chip threads without running out of pins.",
            "options": [
                "FB-DIMM",
                "DDR2 SDRAM",
                "SRAM",
                "GDDR"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a memory technology that enables future Niagara chips to continue increasing the number of on-chip threads without running out of pins.",
                "Expected Label": "A",
                "Predicted Label": "A) FB-DIMM \n\n**Explanation:**\n\nWhile the other options are memory technologies, **Fully Buffered DIMM (FB-DIMM)** was specifically designed to address the pin limitations of increasing memory density and bandwidth requirements.  Here's why:\n\n* **FB-DIMMs use a serial interface** instead of the parallel interface found in other DIMM technologies. This serial approach dramatically reduces the number of pins needed on the chip and memory module, allowing for higher memory capacities and faster data transfer rates.\n\nLet me know if you'd like a deeper dive into FB-DIMMs or any of the other memory technologies! \n"
            }
        ]
    },
    {
        "doc_id": 317,
        "doc": {
            "question_id": 129667026,
            "question": "A chip design with a ______ number of cores can lead to lower overall performance due to contention for shared resources like caches and interconnects.",
            "options": [
                "small",
                "large",
                "moderate",
                "balanced"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A chip design with a ______ number of cores can lead to lower overall performance due to contention for shared resources like caches and interconnects.",
                "Expected Label": "B",
                "Predicted Label": "B) large \n\n**Explanation:**\n\nWhile more cores generally increase processing power, a very large number of cores can lead to performance bottlenecks.  Here's why:\n\n* **Resource Contention:**  A high core count increases competition for shared resources like cache memory and interconnection bandwidth. This contention can slow down data access and communication between cores.\n* **Amdahl's Law:** This law states that the speedup of a program is limited by the portion of the program that cannot be parallelized.  Even with many cores, portions of a program that must run sequentially will create a performance ceiling.\n\nTherefore, a **large** number of cores has the highest potential to create performance issues due to resource limitations. \n"
            }
        ]
    },
    {
        "doc_id": 318,
        "doc": {
            "question_id": 129667030,
            "question": "In chip multiprocessor designs, ____ is often used as the point of communication between processors to balance complexity and latency.",
            "options": [
                "Register file",
                "Primary cache",
                "Main memory",
                "Secondary cache"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In chip multiprocessor designs, ____ is often used as the point of communication between processors to balance complexity and latency.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Register file (A):** Too small and processor-specific for inter-processor communication.\n* **Primary cache (B):** While fast, direct access by multiple processors introduces significant complexity in maintaining data consistency (cache coherence).\n* **Main memory (C):**  Too slow for frequent communication between processors.\n* **Secondary cache (D):** Strikes a balance. Larger and slower than the primary cache, but still faster than main memory. Sharing at this level is more manageable in terms of complexity and offers a reasonable compromise for latency. \n\nLet me know if you'd like more detail on any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 319,
        "doc": {
            "question_id": 129667032,
            "question": "In a TLS system like Hydra, the process of retiring speculative writes in the correct order is managed by ____.",
            "options": [
                "Separate secondary cache buffers for each thread",
                "A centralized speculation queue",
                "The primary cache write buffers",
                "The operating system's thread scheduler"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a TLS system like Hydra, the process of retiring speculative writes in the correct order is managed by ____.",
                "Expected Label": "A",
                "Predicted Label": "**B) A centralized speculation queue** \n\n**Explanation:**\n\nHydra and similar TLS (Transactional Memory Light) systems rely on speculation for performance. A centralized speculation queue plays a crucial role in these systems by:\n\n* **Tracking speculative writes:** It records all writes made by threads speculatively, preventing them from immediately updating the main memory.\n* **Ensuring order during commit:** When a thread successfully completes its transaction, the queue is used to apply the speculative writes in the correct order to ensure consistency.\n* **Rollback on conflicts:** If a conflict is detected, the queue allows for easy rollback of the speculative writes, leaving the main memory unaffected. \n\nLet me know if you'd like a deeper dive into any aspect of TLS or Hydra! \n"
            }
        ]
    },
    {
        "doc_id": 320,
        "doc": {
            "question_id": 129667036,
            "question": "The average thread size in thread-level speculation is typically at least ____ instructions.",
            "options": [
                "10",
                "50",
                "100",
                "1000"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The average thread size in thread-level speculation is typically at least ____ instructions.",
                "Expected Label": "C",
                "Predicted Label": "C) 100 \n\n**Explanation:**\n\nThread-level speculation works best with reasonably large threads to amortize the overhead of speculation. Threads with only a few instructions wouldn't benefit much and could even perform worse than traditional execution. While there's no fixed minimum, 100 instructions is a commonly cited ballpark figure for thread size in this context. \n"
            }
        ]
    },
    {
        "doc_id": 321,
        "doc": {
            "question_id": 129667038,
            "question": "Final sorting in a heap is conducted by removing the ____ node and iteratively filling the vacancy.",
            "options": [
                "leaf",
                "child",
                "branch",
                "top"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Final sorting in a heap is conducted by removing the ____ node and iteratively filling the vacancy.",
                "Expected Label": "D",
                "Predicted Label": "D) top \n\n**Explanation:**\n\nHeaps maintain a partial ordering where the largest (or smallest, depending on the type of heap) element is always at the top. Final sorting in a heap involves repeatedly:\n\n1. **Removing the top (root) node**  (which contains the current maximum/minimum value).\n2. **Filling the vacancy** at the top with the last element in the heap.\n3. **Re-heapifying:** \"Sifting down\" the newly placed element to its correct position to maintain the heap property. \n\nThis process is repeated until the heap is empty, resulting in a sorted sequence. \n"
            }
        ]
    },
    {
        "doc_id": 322,
        "doc": {
            "question_id": 129667040,
            "question": "3D integration technology can help improve processor performance by reducing ____. ",
            "options": [
                "wire length",
                "cache size",
                "number of cores",
                "instruction size"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "3D integration technology can help improve processor performance by reducing ____. ",
                "Expected Label": "A",
                "Predicted Label": "A) wire length \n\n**Explanation:**\n\n3D integration stacks components vertically, significantly reducing the distance signals need to travel between them.  Shorter wires mean:\n\n* **Faster communication:** Signals arrive quicker.\n* **Lower latency:** Operations complete faster.\n* **Reduced power consumption:** Less energy is wasted on signal transmission. \n\nLet me know if you'd like more details on 3D integration! \n"
            }
        ]
    },
    {
        "doc_id": 323,
        "doc": {
            "question_id": 129667042,
            "question": "In 3D cache design, ____ refers to the strategy where wordlines in a sub-array are divided and mapped onto different active device layers.",
            "options": [
                "3D Divided Bitline",
                "3D Divided Wordline",
                "3D Partitioned Cache",
                "3D Stacked Memory"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In 3D cache design, ____ refers to the strategy where wordlines in a sub-array are divided and mapped onto different active device layers.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 324,
        "doc": {
            "question_id": 129667044,
            "question": "A significant reduction in _____ can be achieved when wordlines are divided along the third dimension in 3D processor design.",
            "options": [
                "bitline delays",
                "routing delays",
                "critical global wiring lengths",
                "sense amplifier activation"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A significant reduction in _____ can be achieved when wordlines are divided along the third dimension in 3D processor design.",
                "Expected Label": "C",
                "Predicted Label": "C) critical global wiring lengths \n\nHere's why:\n\n* **3D processor design** aims to reduce the distance signals need to travel within a chip. By stacking layers of transistors vertically, connections can be made more directly. \n* **Critical global wiring lengths** refer to the longest wires that span across the chip, often limiting performance.  3D design significantly shortens these lengths by allowing for vertical connections between layers.\n\nLet's look at why the other options are less accurate:\n\n* **Bitline delays:** While 3D design might indirectly improve bitline delays, its primary impact isn't focused there.\n* **Routing delays:** 3D design primarily addresses global routing, not the general concept of routing delays.\n* **Sense amplifier activation:** 3D design doesn't directly target sense amplifier activation. \n\n**Therefore, the most significant reduction achieved by 3D design concerning the given options is in critical global wiring lengths.** \n"
            }
        ]
    },
    {
        "doc_id": 325,
        "doc": {
            "question_id": 129667046,
            "question": "The 3D cache organization exhibits the greatest reduction in global bank-level routing latency for ____ caches.",
            "options": [
                "16KB",
                "64KB",
                "512KB",
                "2-4 MB"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The 3D cache organization exhibits the greatest reduction in global bank-level routing latency for ____ caches.",
                "Expected Label": "D",
                "Predicted Label": "D) 2-4 MB \n\n**Explanation:**\n\n3D cache organizations are most effective at reducing global bank-level routing latency for **larger cache sizes**. Here's why:\n\n* **Bank-Level Routing:**  In traditional caches, accessing different banks within the cache can introduce significant latency. \n* **3D Organization:** 3D caches, often implemented using technologies like Through-Silicon Vias (TSVs), stack cache layers vertically. This allows for shorter interconnects between banks, reducing latency.\n* **Larger Caches, Greater Benefit:** The latency reduction offered by 3D organization becomes more pronounced as the cache size increases. Smaller caches have fewer banks to begin with, so the impact of 3D organization is less significant. \n\nTherefore, while 3D organization helps all cache sizes, the greatest reduction in bank-level routing latency is observed in larger caches like 2-4MB. \n"
            }
        ]
    },
    {
        "doc_id": 326,
        "doc": {
            "question_id": 129667048,
            "question": "Quantum teleportation requires the application of a combination of one bit-flip gate and one ______ gate to correct any errors.",
            "options": [
                "phase-flip",
                "swap",
                "identity",
                "Hadamard"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Quantum teleportation requires the application of a combination of one bit-flip gate and one ______ gate to correct any errors.",
                "Expected Label": "A",
                "Predicted Label": "**A) phase-flip**\n\n**Explanation:**\n\nQuantum teleportation utilizes entanglement and classical communication to transmit quantum information. During this process, errors can occur, potentially flipping the qubit's state (bit-flip error) or introducing a phase shift (phase-flip error).  \n\nTo correct these errors, a combination of gates is used:\n\n* **Bit-flip gate (X gate):** Corrects bit-flip errors.\n* **Phase-flip gate (Z gate):** Corrects phase-flip errors. \n\nTherefore, the correct answer is **A) phase-flip**. \n"
            }
        ]
    },
    {
        "doc_id": 327,
        "doc": {
            "question_id": 129667056,
            "question": "____ is considered one of the most resource-intensive operations in quantum computing due to the high volatility of quantum data.",
            "options": [
                "Error correction",
                "Data compression",
                "Encryption",
                "Memory allocation"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered one of the most resource-intensive operations in quantum computing due to the high volatility of quantum data.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 328,
        "doc": {
            "question_id": 129667060,
            "question": "In Shor's algorithm, the modular exponentiation procedure is a series of quantum multiplication steps divided into a series of quantum ______.",
            "options": [
                "comparators",
                "subtractions",
                "adders",
                "dividers"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Shor's algorithm, the modular exponentiation procedure is a series of quantum multiplication steps divided into a series of quantum ______.",
                "Expected Label": "C",
                "Predicted Label": "C) adders \n\n**Explanation:**\n\nShor's algorithm utilizes modular exponentiation, which can be broken down into a series of multiplications.  These multiplications, in turn, are implemented using quantum adders as their fundamental building blocks. \n"
            }
        ]
    },
    {
        "doc_id": 329,
        "doc": {
            "question_id": 129667062,
            "question": "____ is used to find the failure probability of stabilizer circuits in quantum error correction simulations.",
            "options": [
                "Deterministic analysis",
                "Analytical modeling",
                "Exhaustive enumeration",
                "Monte-Carlo simulation"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is used to find the failure probability of stabilizer circuits in quantum error correction simulations.",
                "Expected Label": "D",
                "Predicted Label": "D) Monte-Carlo simulation \n\nHere's why:\n\n* **Monte-Carlo simulations** are particularly well-suited for estimating probabilities in complex systems where direct calculation is difficult. In quantum error correction simulations, they involve randomly introducing errors based on a noise model and observing how often the stabilizer code successfully corrects them.  \n\nLet's look at why the other options aren't the best fit:\n\n* **Deterministic analysis:** This involves analyzing a system based on fixed inputs and known behaviors, not ideal for the probabilistic nature of errors in quantum systems.\n* **Analytical modeling:** While useful for understanding general behavior, deriving precise analytical models for error probability in complex quantum systems can be extremely challenging or even impossible.\n* **Exhaustive enumeration:** This would involve testing every possible error combination, computationally infeasible for larger quantum systems. \n\n**Therefore, Monte-Carlo simulation (D) is the most practical and commonly used method for determining the failure probability of stabilizer circuits in quantum error correction simulations.** \n"
            }
        ]
    },
    {
        "doc_id": 330,
        "doc": {
            "question_id": 129667064,
            "question": "The efficiency of quantum search algorithms can be diminished by the need for ____, especially when dealing with large databases.",
            "options": [
                "error correction",
                "parallel processing",
                "caching",
                "pipelining"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The efficiency of quantum search algorithms can be diminished by the need for ____, especially when dealing with large databases.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 331,
        "doc": {
            "question_id": 129667066,
            "question": "QASM-TOOLS were used to empirically compute ____ at Level 2 for the QLA logical qubit.",
            "options": [
                "resource overhead",
                "pth",
                "EPR pair creation time",
                "error correction latency"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "QASM-TOOLS were used to empirically compute ____ at Level 2 for the QLA logical qubit.",
                "Expected Label": "B",
                "Predicted Label": "D) error correction latency \n"
            }
        ]
    },
    {
        "doc_id": 332,
        "doc": {
            "question_id": 129667068,
            "question": "In quantum architectures, ____ are typically more error-prone than memory operations.",
            "options": [
                "Teleportation operations",
                "Error correction cycles",
                "Gate operations",
                "Qubit initialization procedures"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In quantum architectures, ____ are typically more error-prone than memory operations.",
                "Expected Label": "C",
                "Predicted Label": "C) Gate operations \n\n**Explanation:**\n\nGate operations are the fundamental building blocks of quantum computations, and they are inherently more susceptible to errors than memory operations. Here's why:\n\n* **Sensitivity to environment:** Gate operations involve manipulating the delicate quantum states of qubits, making them highly sensitive to noise from the environment.\n* **Complexity:**  Gate operations often require precise control of electromagnetic fields or other physical parameters, increasing the chance of errors. \n* **Cascading errors:** Errors in one gate operation can propagate through subsequent operations, compounding the overall error rate.\n\nWhile other options might have their own error probabilities, gate operations are generally recognized as the most error-prone aspect of quantum architectures. \n"
            }
        ]
    },
    {
        "doc_id": 333,
        "doc": {
            "question_id": 129667082,
            "question": "Stereo vision systems primarily aim to extract ____ information from a scene.",
            "options": [
                "color",
                "3D structure",
                "texture",
                "object tracking"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Stereo vision systems primarily aim to extract ____ information from a scene.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 334,
        "doc": {
            "question_id": 129667084,
            "question": "The KITTI dataset project for autonomous driving was initiated in _____.",
            "options": [
                "2001",
                "2005",
                "2012",
                "2015"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The KITTI dataset project for autonomous driving was initiated in _____.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 335,
        "doc": {
            "question_id": 129667086,
            "question": "In a vision-based localization framework, the ____ establishes feature correspondences and is always activated.",
            "options": [
                "backend",
                "fusion filter",
                "mapping module",
                "frontend"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a vision-based localization framework, the ____ establishes feature correspondences and is always activated.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Frontend:** This component is responsible for continuously processing raw sensor data (in this case, images) to extract relevant features and match them with existing map data or previous frames. This feature correspondence is fundamental for localization.\n\nLet's analyze why the other options are incorrect:\n\n* **Backend:** This component typically deals with higher-level tasks like trajectory optimization or global map updates, not constant feature matching.\n* **Fusion Filter:** This component is responsible for merging data from multiple sensors, not solely focused on feature correspondences.\n* **Mapping Module:**  This component focuses on building and updating the map itself, rather than the real-time localization task. \n\n**Therefore, the frontend is the component constantly active in establishing feature correspondences for vision-based localization.** \n"
            }
        ]
    },
    {
        "doc_id": 336,
        "doc": {
            "question_id": 129667092,
            "question": "In motion planning for robotics systems, ____ typically consumes the majority of computational time and CPU instructions.",
            "options": [
                "Path finding",
                "Trajectory optimization",
                "Collision detection",
                "Kinematic solving"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In motion planning for robotics systems, ____ typically consumes the majority of computational time and CPU instructions.",
                "Expected Label": "C",
                "Predicted Label": "C) Collision detection \n"
            }
        ]
    },
    {
        "doc_id": 337,
        "doc": {
            "question_id": 130052965,
            "question": "_____ involves multiple processors executing different instructions on different data elements.",
            "options": [
                "Multiple Instruction Multiple Data (MIMD)",
                "Single Program Multiple Data (SPMD)",
                "Single Instruction Multiple Data (SIMD)",
                "Vector Processing"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "_____ involves multiple processors executing different instructions on different data elements.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 338,
        "doc": {
            "question_id": 130052967,
            "question": "____ is a compiler optimization technique that aims to reduce the overhead of cache misses.",
            "options": [
                "Strength reduction",
                "Tiling",
                "Function inlining",
                "Register allocation"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a compiler optimization technique that aims to reduce the overhead of cache misses.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Tiling** is a loop transformation technique that breaks down large data blocks into smaller chunks (tiles) that can fit in the cache. This aims to improve cache locality and reduce cache misses. \n\nLet's look at why the other options are incorrect:\n\n* **Strength reduction:** Replaces expensive operations with computationally cheaper ones (e.g., replacing multiplication with repeated addition).  It doesn't directly address cache misses.\n* **Function inlining:** Replaces function calls with the actual code of the function, reducing call overhead. It doesn't directly target cache misses.\n* **Register allocation:** Assigns frequently used variables to registers for faster access. While it improves performance, it's not directly related to reducing cache misses. \n"
            }
        ]
    },
    {
        "doc_id": 339,
        "doc": {
            "question_id": 130052969,
            "question": "In the context of dataflow analysis in computer architecture, a function's impact on the overall program state can be summarized using ____.",
            "options": [
                "Control Flow Graphs (CFGs)",
                "GEN and KILL sets",
                "Semi-lattices",
                "Data Dependency Graphs"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the context of dataflow analysis in computer architecture, a function's impact on the overall program state can be summarized using ____.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 340,
        "doc": {
            "question_id": 130052971,
            "question": "The distance of a dependence is defined as ____.",
            "options": [
                "the index of the loop nest containing the dependence",
                "the direction of data flow between iterations",
                "the number of iterations between accesses",
                "the difference between the loop index values of the source and sink"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The distance of a dependence is defined as ____.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D**.\n\nHere's why:\n\n* **Dependence distance** quantifies how far apart (in terms of iterations) a dependent instruction must be from the instruction it depends on.  \n\nLet's break down the options:\n\n* **A) the index of the loop nest containing the dependence** - This describes the nesting level of the loop, not the distance between dependent instructions.\n* **B) the direction of data flow between iterations** - This refers to dependence type (flow, anti, output) but doesn't quantify the distance.\n* **C) the number of iterations between accesses** - This is close but not as precise. Dependence distance is specifically calculated using the difference in loop index values.\n\n**D) the difference between the loop index values of the source and sink** - This accurately defines dependence distance. The \"source\" is the instruction producing the data, and the \"sink\" is the instruction consuming it.  The difference in their loop index values tells you how many iterations apart they are. \n"
            }
        ]
    },
    {
        "doc_id": 341,
        "doc": {
            "question_id": 130052973,
            "question": "____ is a graph that displays both data and control dependences found in a program.",
            "options": [
                "Program dependence graph",
                "Control flow graph",
                "Data flow graph",
                "Directed acyclic graph"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a graph that displays both data and control dependences found in a program.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 342,
        "doc": {
            "question_id": 130052977,
            "question": "To vectorize an inner loop, the transformation of putting strongly connected components in separate loops is applied and then ____ or strip mining is used.",
            "options": [
                "loop fusion",
                "loop tiling",
                "loop blocking",
                "loop peeling"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To vectorize an inner loop, the transformation of putting strongly connected components in separate loops is applied and then ____ or strip mining is used.",
                "Expected Label": "C",
                "Predicted Label": "C) loop blocking \n\n**Explanation:**\n\nHere's why loop blocking (also often called loop tiling) is the correct answer in this context:\n\n* **Vectorization:** The goal is to break down operations so they can be executed in parallel by vector processors (SIMD).\n* **Strongly Connected Components:** When these are in separate loops, it often means there's a data dependency preventing straightforward vectorization. \n* **Loop Blocking:** Divides the iterations of a loop into smaller chunks or blocks. This helps in:\n    * **Improving Cache Utilization:** Smaller blocks of data are more likely to fit into cache, reducing memory accesses.\n    * **Enabling Vectorization:**  Within a block, operations may become independent, allowing for vector instructions.\n\n**Why other options are less suitable:**\n\n* **Loop Fusion:**  Combines loops to reduce overhead, not directly related to vectorization after dependency separation.\n* **Loop Peeling:** Removes a small number of iterations at the beginning or end, primarily used for boundary condition handling.\n* **Loop Tiling:**  While technically correct (it's a synonym for loop blocking), the question already mentions putting components in separate loops, making \"loop blocking\" a clearer and more specific answer. \n"
            }
        ]
    },
    {
        "doc_id": 343,
        "doc": {
            "question_id": 130052979,
            "question": "____ is a key challenge in parallelizing while loops compared to counted loops.",
            "options": [
                "Loop unrolling",
                "Branch prediction",
                "Cache coherence",
                "Unknown iteration count"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a key challenge in parallelizing while loops compared to counted loops.",
                "Expected Label": "D",
                "Predicted Label": "D) Unknown iteration count \n\n**Explanation:**\n\nWhile loops iterate based on a condition rather than a predetermined count. This dynamic nature makes it difficult to divide the work among processors beforehand, as the total number of iterations is unknown.  \n\nLet's break down why the other options are less relevant:\n\n* **A) Loop unrolling:** This technique improves performance within a loop but doesn't inherently address the challenge of parallelizing while loops with unknown iteration counts.\n* **B) Branch prediction:** Branch prediction helps optimize conditional statements within loops but doesn't solve the fundamental issue of task division in parallelization when the iteration count is unknown.\n* **C) Cache coherence:**  Cache coherence is essential in multi-core systems but is a general concern for parallel processing, not specifically a key challenge posed by while loops with unknown iteration counts. \n"
            }
        ]
    },
    {
        "doc_id": 344,
        "doc": {
            "question_id": 130052981,
            "question": "Loop ________ is a form of skewing that simplifies the formation of dependence equations by transforming a loop to start at zero or one.",
            "options": [
                "normalization",
                "interchange",
                "unrolling",
                "tiling"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Loop ________ is a form of skewing that simplifies the formation of dependence equations by transforming a loop to start at zero or one.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 345,
        "doc": {
            "question_id": 130052983,
            "question": "Reductions exploit ____ to both break dependences and enable code motion and parallelization.",
            "options": [
                "commutativity",
                "associativity",
                "concurrency",
                "synchronization"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Reductions exploit ____ to both break dependences and enable code motion and parallelization.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 346,
        "doc": {
            "question_id": 130052985,
            "question": "Loop ____ is a transformation that increases the temporal and spatial locality of array accesses.",
            "options": [
                "unrolling",
                "fusion",
                "interchange",
                "peeling"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Loop ____ is a transformation that increases the temporal and spatial locality of array accesses.",
                "Expected Label": "C",
                "Predicted Label": "C\n \n**Loop interchange** reorders the execution order of nested loops to improve memory access patterns. By changing the nesting order, it can make array accesses stride through memory more sequentially, which improves both temporal and spatial locality.\n\nLet me know if you'd like more explanation on any of the loop transformation techniques! \n"
            }
        ]
    },
    {
        "doc_id": 347,
        "doc": {
            "question_id": 130052991,
            "question": "______ accelerators aim to balance performance gains with ease of programming by focusing on substantial computational units that can be reused across applications.",
            "options": [
                "Application-level",
                "Kernel-level",
                "Instruction-level",
                "Pipeline-integrated"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "______ accelerators aim to balance performance gains with ease of programming by focusing on substantial computational units that can be reused across applications.",
                "Expected Label": "B",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 348,
        "doc": {
            "question_id": 130052993,
            "question": "Cache-coupled accelerators typically require their own mechanisms for ____.",
            "options": [
                "Instruction fetching",
                "Branch prediction",
                "Address translation",
                "Register renaming"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Cache-coupled accelerators typically require their own mechanisms for ____.",
                "Expected Label": "C",
                "Predicted Label": "C) Address translation \n\nHere's why:\n\n* **Cache-coupled accelerators** are specialized processing units that share the main processor's cache hierarchy. \n\nLet's analyze the options:\n\n* **A) Instruction fetching:**  While accelerators execute specialized instructions, the primary instruction fetching mechanism is usually still managed by the main CPU.\n* **B) Branch prediction:** Similar to instruction fetching, branch prediction is typically handled by the main CPU's pipeline.\n* **C) Address translation:** Accelerators often work on data subsets within a larger address space. They might require their own address translation mechanisms to map between the accelerator's view of data and the main memory addresses.\n* **D) Register renaming:** Register renaming is an optimization technique primarily within the main CPU's pipeline to improve instruction-level parallelism. \n\n**Therefore, address translation (C) is the most likely mechanism that a cache-coupled accelerator would require independently.** \n"
            }
        ]
    },
    {
        "doc_id": 349,
        "doc": {
            "question_id": 130052997,
            "question": "____ is a key benefit of high-level synthesis in hardware design.",
            "options": [
                "Better design",
                "Increased power consumption",
                "Lower clock speeds",
                "Larger chip area"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key benefit of high-level synthesis in hardware design.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 350,
        "doc": {
            "question_id": 130053003,
            "question": "____ is typically used to capture switching activity at the gate level for power estimation in RTL-based design flows.",
            "options": [
                "DDDG",
                "ModelSim",
                "Aladdin",
                "SAIF"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is typically used to capture switching activity at the gate level for power estimation in RTL-based design flows.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 351,
        "doc": {
            "question_id": 130053019,
            "question": "____ eliminates re-compaction overhead by allowing sub-blocks of a block to be allocated anywhere in the data array.",
            "options": [
                "VSC",
                "SC2",
                "DCC",
                "IIC-C"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ eliminates re-compaction overhead by allowing sub-blocks of a block to be allocated anywhere in the data array.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **B) SC2**. Here's why:\n\n* **SC2 (Segmented Copying with Compaction 2)** is a garbage collection technique designed to reduce compaction overhead. It achieves this by dividing blocks into smaller sub-blocks. These sub-blocks can be moved independently during compaction, allowing for more efficient use of memory space and eliminating the need to move entire blocks. \n\nLet's look at why the other options are incorrect:\n\n* **VSC (Variable Sized Chunking)** is a memory allocation strategy, not directly related to garbage collection and compaction.\n* **DCC (Distributed Cycle Collection)** is a distributed garbage collection algorithm, not focused on the specific compaction technique described.\n* **IIC-C** is not a standard term related to garbage collection or memory management. \n\n**Therefore, SC2 is the most fitting answer as it directly addresses the concept of reducing recompaction overhead by allowing flexible sub-block allocation.** \n"
            }
        ]
    },
    {
        "doc_id": 352,
        "doc": {
            "question_id": 130053021,
            "question": "____ is a technique used in some compressed cache designs to maintain the mapping between tags and data while reducing metadata overhead.",
            "options": [
                "Back pointers",
                "Forward pointers",
                "Skewed indexing",
                "Super-block tags"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in some compressed cache designs to maintain the mapping between tags and data while reducing metadata overhead.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **(C) Skewed indexing**. Here's why:\n\n* **Skewed indexing** manipulates how indexes are calculated for cache lines within a set. This allows compressed cache designs to store tags more efficiently, often by grouping them contiguously. \n\nLet's break down why the other options are incorrect:\n\n* **Back pointers (A)** are used in data structures like linked lists, not typically in cache design for tag-to-data mapping.\n* **Forward pointers (B)**  are also more common in data structures and don't directly address metadata overhead reduction in caches.\n* **Super-block tags (D)** While the term sounds plausible, it's not a standard technique in cache compression. Super-blocks usually refer to larger memory blocks and wouldn't directly solve the metadata problem. \n"
            }
        ]
    },
    {
        "doc_id": 353,
        "doc": {
            "question_id": 130053023,
            "question": "Skewed associative caches, like SCC, tend to have ____ conflicts than a conventional set-associative cache with the same number of ways.",
            "options": [
                "more",
                "fewer",
                "the same number of",
                "unpredictably more or fewer"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Skewed associative caches, like SCC, tend to have ____ conflicts than a conventional set-associative cache with the same number of ways.",
                "Expected Label": "B",
                "Predicted Label": "B) fewer \n"
            }
        ]
    },
    {
        "doc_id": 354,
        "doc": {
            "question_id": 130053025,
            "question": "____ compression algorithm stores and accesses only non-zero bytes of a block to reduce dynamic power consumption.",
            "options": [
                "Significance-compression",
                "Effective Capacity Maximizer (ECM)",
                "Dynamic zero compression (DZC)",
                "Frequent value locality"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ compression algorithm stores and accesses only non-zero bytes of a block to reduce dynamic power consumption.",
                "Expected Label": "C",
                "Predicted Label": "C) Dynamic zero compression (DZC) \n"
            }
        ]
    },
    {
        "doc_id": 355,
        "doc": {
            "question_id": 130053029,
            "question": "The IBM MXT proposal organizes compressed memory into sub-blocks of ____ size.",
            "options": [
                "256 bytes",
                "1 KB",
                "64 bytes",
                "Variable"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The IBM MXT proposal organizes compressed memory into sub-blocks of ____ size.",
                "Expected Label": "A",
                "Predicted Label": "C) 64 bytes \n"
            }
        ]
    },
    {
        "doc_id": 356,
        "doc": {
            "question_id": 130053039,
            "question": "In CUDA, ____ is used to halt the current thread until all previous writes to shared and global memory are visible by other threads.",
            "options": [
                "atomic_add()",
                "_threadfence()",
                "mem_fence()",
                "sync_threads()"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In CUDA, ____ is used to halt the current thread until all previous writes to shared and global memory are visible by other threads.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 357,
        "doc": {
            "question_id": 130053041,
            "question": "Multiple banks and ____ are two techniques used in GPUs to achieve high register file bandwidth.",
            "options": [
                "cache coherence protocols",
                "speculative execution",
                "operand buffering",
                "instruction reordering"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Multiple banks and ____ are two techniques used in GPUs to achieve high register file bandwidth.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Operand Buffering:** GPUs utilize large register files to manage the massive parallelism of threads. To supply these registers efficiently, they use operand buffering. This technique fetches and buffers operands (data values used in instructions) in advance, ensuring rapid availability when instructions need them.\n\nLet's examine why the other options are incorrect:\n\n* **Cache Coherence Protocols:**  Deal with maintaining consistency of shared data across multiple caches. While relevant to GPU architecture, it's not directly a technique for high register file bandwidth.\n* **Speculative Execution:**  Aims to improve performance by executing instructions before it's certain they'll be needed. It doesn't directly address register file bandwidth.\n* **Instruction Reordering:**  Optimizes instruction execution order to minimize stalls and improve pipeline efficiency. While beneficial, it's not the primary mechanism for high register file bandwidth. \n"
            }
        ]
    },
    {
        "doc_id": 358,
        "doc": {
            "question_id": 130053045,
            "question": "____ architecture executes native x86 ISAs for high-throughput computing.",
            "options": [
                "Intel's Many Integrated Core (MIC)",
                "NVIDIA's CUDA",
                "AMD's Fusion",
                "IBM's Cell"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ architecture executes native x86 ISAs for high-throughput computing.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Intel's Many Integrated Core (MIC)** architecture was specifically designed for highly parallel workloads and used x86-based processors. \n\nLet's look at why the other options are incorrect:\n\n* **NVIDIA's CUDA** is a parallel computing platform and programming model, but it relies on NVIDIA GPUs and their own instruction set, not x86.\n* **AMD's Fusion** primarily focuses on integrating CPU and GPU on a single die for improved graphics and general-purpose computing, not specifically high-throughput x86 execution.\n* **IBM's Cell** architecture, while powerful for its time, used a specialized PowerPC-based core along with several synergistic processing elements (SPEs), not traditional x86 cores. \n"
            }
        ]
    },
    {
        "doc_id": 359,
        "doc": {
            "question_id": 130053047,
            "question": "The ratio of work to depth (W(n)/D(n)) in a parallel computation provides an indication of the ______.",
            "options": [
                "algorithm's complexity",
                "average available parallelism",
                "total number of operations",
                "critical path length"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ratio of work to depth (W(n)/D(n)) in a parallel computation provides an indication of the ______.",
                "Expected Label": "B",
                "Predicted Label": "B) average available parallelism \n\n**Explanation:**\n\n* **Work (W(n))** represents the total number of operations in an algorithm.\n* **Depth (D(n))** represents the longest chain of dependent operations (the critical path).\n\nThe ratio W(n)/D(n) indicates how much work can be done in parallel on average. A higher ratio suggests more opportunities for parallelization and potentially greater speedup. \n\nLet's look at why the other options are incorrect:\n\n* **A) algorithm's complexity:** While related to work, the ratio doesn't directly represent complexity (e.g., Big O notation).\n* **C) total number of operations:** This is represented by work (W(n)) alone.\n* **D) critical path length:** This is represented by depth (D(n)) alone. \n"
            }
        ]
    },
    {
        "doc_id": 360,
        "doc": {
            "question_id": 130053049,
            "question": "The intensity of a blocked matrix multiply algorithm, assuming three blocks can fit in fast memory at once, is ____.",
            "options": [
                "O(n2)",
                "O(n3)",
                "O(\u221aZ)",
                "O(1)"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The intensity of a blocked matrix multiply algorithm, assuming three blocks can fit in fast memory at once, is ____.",
                "Expected Label": "C",
                "Predicted Label": "D) O(1) \n\n**Explanation:**\n\nThe intensity of an algorithm is a measure of its data reuse. It's calculated as the ratio of operations to data movement. A higher intensity implies better data locality and potentially better performance.\n\nHere's why the answer is **D) O(1)**:\n\n* **Blocked Matrix Multiplication:**  This technique breaks down large matrices into smaller blocks that fit into fast memory (cache). This significantly reduces data movement between fast memory and slower main memory.\n* **Three Blocks in Fast Memory:**  If we can hold three blocks (two for input matrices and one for the result) simultaneously in fast memory, we perform a large chunk of computations with minimal data movement from the main memory. \n\nSince the block size is fixed, and we're assuming we can always fit three blocks in fast memory, the data movement becomes independent of the matrix size (n). We perform a constant amount of data movement for a much larger amount of computation. This leads to an intensity that approaches a constant value, hence **O(1)**. \n"
            }
        ]
    },
    {
        "doc_id": 361,
        "doc": {
            "question_id": 130053051,
            "question": "In computer architecture, ____ is a principle that relates to the average degree of available memory-level parallelism.",
            "options": [
                "Amdahl's Law",
                "Kung's balance principle",
                "Gustafson's Law",
                "Little's Law"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ is a principle that relates to the average degree of available memory-level parallelism.",
                "Expected Label": "D",
                "Predicted Label": "B \n\n**Kung's balance principle** is specifically about balancing the bandwidths of different levels in the memory hierarchy to match the processor's requirements and thus optimize performance. \n\nLet's break down why the other options are incorrect:\n\n* **Amdahl's Law:** Focuses on the potential speedup of a program based on the parallelizable and non-parallelizable portions.\n* **Gustafson's Law:** Addresses the scalability of parallel algorithms, stating that the speedup achievable increases with problem size.\n* **Little's Law:**  Relates average number of items in a queuing system to the average waiting time. \n\n**Therefore, B is the most relevant answer to the question.** \n"
            }
        ]
    },
    {
        "doc_id": 362,
        "doc": {
            "question_id": 130053053,
            "question": "In the context of the fast multipole method (FMM), the ____ phase involves calculations between a leaf node and its neighboring leaf nodes.",
            "options": [
                "U-list",
                "V-list",
                "W-list",
                "X-list"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In the context of the fast multipole method (FMM), the ____ phase involves calculations between a leaf node and its neighboring leaf nodes.",
                "Expected Label": "A",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 363,
        "doc": {
            "question_id": 130053057,
            "question": "In GPU architectures, ____ is a key factor in determining the visibility of SFU instruction execution costs.",
            "options": [
                "Cache hit rate",
                "Register file size",
                "Ratio of SFU instructions to other instructions",
                "Number of active warps"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In GPU architectures, ____ is a key factor in determining the visibility of SFU instruction execution costs.",
                "Expected Label": "C",
                "Predicted Label": "C) Ratio of SFU instructions to other instructions \n\n**Explanation:**\n\nWhile all options can influence performance to some degree, the ratio of SFU (Special Function Unit) instructions to other instructions is the most direct factor in determining how much their execution cost impacts overall performance.  Here's why:\n\n* **High Ratio:** If a program heavily relies on SFU instructions (e.g., for complex mathematical computations common in graphics), the execution time of these instructions becomes a significant bottleneck. Even with optimizations like caching or large register files, the sheer volume of SFU operations will be prominent.\n* **Low Ratio:** Conversely, if SFU instructions are infrequent, their cost is less likely to be the limiting factor. Other parts of the pipeline, like memory access or ALU operations, might dominate the performance profile.\n\n**Let's consider why the other options are less directly related:**\n\n* **A) Cache hit rate:**  While important for overall performance, a good cache hit rate benefits all instruction types, not just SFUs.\n* **B) Register file size:** Larger register files can help reduce memory access latency, which benefits all instructions.\n* **D) Number of active warps:** More warps generally increase throughput, but the impact of SFU instruction latency depends on their prevalence in those warps, not just the warp count itself. \n"
            }
        ]
    },
    {
        "doc_id": 364,
        "doc": {
            "question_id": 130053061,
            "question": "One fundamental roadblock to efficiency in modern computing systems and DNN accelerators is _____.",
            "options": [
                "data movement",
                "computation power",
                "memory size",
                "network latency"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "One fundamental roadblock to efficiency in modern computing systems and DNN accelerators is _____.",
                "Expected Label": "A",
                "Predicted Label": "A) data movement \n\n**Explanation:**\n\nWhile all options can play a role in limiting efficiency, **data movement** is the most fundamental roadblock in modern computing and DNN accelerators. Here's why:\n\n* **Computation vs. Data Movement:** Modern processors and accelerators have immense computational power. However, fetching data from memory (especially for large datasets used in DNNs) is significantly slower than performing computations. \n* **Memory Wall:**  The gap between processor speeds and memory access times continues to widen, creating a bottleneck known as the \"memory wall.\" \n* **Power Consumption:** Data movement is energy-intensive, consuming more power than actual computation in many cases.\n\nTherefore, optimizing data movement is a critical aspect of designing efficient computing systems and DNN accelerators. \n"
            }
        ]
    },
    {
        "doc_id": 365,
        "doc": {
            "question_id": 130053063,
            "question": "In computer architecture, ____ refers to combinations of loop transformations that affect data movement and reuse patterns.",
            "options": [
                "Loop nests",
                "Dataflows",
                "Convolutions",
                "Tensor operations"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ refers to combinations of loop transformations that affect data movement and reuse patterns.",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 366,
        "doc": {
            "question_id": 130053067,
            "question": "In EDDO approaches, traditional reusable buffer organizations include ____.",
            "options": [
                "DMA engines",
                "address generators",
                "cache hierarchies",
                "hardware FIFOs"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In EDDO approaches, traditional reusable buffer organizations include ____.",
                "Expected Label": "D",
                "Predicted Label": "D) hardware FIFOs \n\n**Explanation:**\n\nEDDO (Explicit Data Dependency Optimization) techniques often use hardware FIFOs (First-In, First-Out) as reusable buffers to manage data dependencies between different processing elements. \n\nLet's break down why the other options are less suitable:\n\n* **A) DMA engines:** While DMA engines handle data transfers, they are not typically considered core components of reusable buffer organizations within the context of EDDO.\n* **B) Address generators:** Address generators are crucial for memory access but don't inherently function as reusable data buffers.\n* **C) Cache hierarchies:** Caches optimize memory access patterns but are not the primary mechanism for managing explicit data dependencies in EDDO approaches. \n\n**Therefore, hardware FIFOs (D) are the most fitting answer as they provide a structured way to store and manage data flow between dependent operations in an EDDO system.** \n"
            }
        ]
    },
    {
        "doc_id": 367,
        "doc": {
            "question_id": 130053071,
            "question": "____ routing in NoCs ensures a fixed path for data traversal, preventing deadlocks but potentially sacrificing throughput optimization.",
            "options": [
                "Adaptive",
                "Deterministic",
                "Dynamic",
                "Flexible"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ routing in NoCs ensures a fixed path for data traversal, preventing deadlocks but potentially sacrificing throughput optimization.",
                "Expected Label": "B",
                "Predicted Label": "B) Deterministic \n"
            }
        ]
    },
    {
        "doc_id": 368,
        "doc": {
            "question_id": 130053073,
            "question": "In a systolic array, each PE is guaranteed to produce a partial sum within ____.",
            "options": [
                "one microsecond",
                "a few cycles",
                "one cycle",
                "one millisecond"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a systolic array, each PE is guaranteed to produce a partial sum within ____.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) one cycle**. Here's why:\n\n* **Systolic arrays** are designed for high-throughput parallel processing.  A key characteristic is that data flows rhythmically (like a heartbeat - hence \"systolic\") through the array. \n\n* **Each Processing Element (PE)** in a systolic array performs a simple operation on the data it receives and passes the result to its neighbors in a synchronized manner. This synchronous operation ensures each PE contributes to the computation within a single clock cycle.\n\nLet me know if you'd like more explanation on systolic arrays! \n"
            }
        ]
    },
    {
        "doc_id": 369,
        "doc": {
            "question_id": 130053075,
            "question": "In DNN accelerator design, ____ is often used as the stationary tensor due to its high memory footprint and ample reuse opportunities.",
            "options": [
                "input data",
                "activation values",
                "partial sums",
                "weight matrices"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In DNN accelerator design, ____ is often used as the stationary tensor due to its high memory footprint and ample reuse opportunities.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Weight matrices** in Deep Neural Networks (DNNs) are typically much larger than the input data or activation values.  They also see significant reuse across multiple input samples and layers. Therefore, storing weights as the stationary tensor in a specialized memory hierarchy optimizes data movement and improves energy efficiency in DNN accelerators.\n\nLet's analyze the other options:\n\n* **A) Input data:** Input data changes frequently, making it less suitable as a stationary tensor.\n* **B) Activation values:**  Similar to input data, activation values have a limited lifetime and change frequently.\n* **C) Partial sums:**  Partial sums are intermediate results and are not stored persistently like weights. \n\n**Therefore, the best answer is D) weight matrices.** \n"
            }
        ]
    },
    {
        "doc_id": 370,
        "doc": {
            "question_id": 130053077,
            "question": "The processing element at the ____ of a systolic array typically determines the overall runtime of the array.",
            "options": [
                "bottom right",
                "top left",
                "center",
                "any edge"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The processing element at the ____ of a systolic array typically determines the overall runtime of the array.",
                "Expected Label": "A",
                "Predicted Label": "D) any edge \n\n**Explanation:**\n\nSystolic arrays achieve their efficiency through parallel processing and data flow. Data enters the array at the edges and propagates through the processing elements.  The processing element that takes the longest for data to pass through effectively sets the overall runtime, and this can be located on *any* edge of the array. \n"
            }
        ]
    },
    {
        "doc_id": 371,
        "doc": {
            "question_id": 130053079,
            "question": "Eyeriss v2 uses a ____ topology optimized for multicast-based distribution of inputs and weights to PE clusters.",
            "options": [
                "fat binary-tree",
                "Hierarchical Mesh",
                "conventional mesh",
                "Network-on-Package"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Eyeriss v2 uses a ____ topology optimized for multicast-based distribution of inputs and weights to PE clusters.",
                "Expected Label": "B",
                "Predicted Label": "B) Hierarchical Mesh \n"
            }
        ]
    },
    {
        "doc_id": 372,
        "doc": {
            "question_id": 130053081,
            "question": "In computer architecture modeling, ____ represents the incremental data that must be transferred between levels for a temporal loop with partial reuse.",
            "options": [
                "An empty delta",
                "A multicasting opportunity",
                "A non-empty delta",
                "A forwarding opportunity"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In computer architecture modeling, ____ represents the incremental data that must be transferred between levels for a temporal loop with partial reuse.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 373,
        "doc": {
            "question_id": 130053085,
            "question": "An FPGA is composed of approximately ____ programmable interconnects.",
            "options": [
                "90%",
                "50%",
                "10%",
                "30%"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "An FPGA is composed of approximately ____ programmable interconnects.",
                "Expected Label": "A",
                "Predicted Label": "B) 50% \n"
            }
        ]
    },
    {
        "doc_id": 374,
        "doc": {
            "question_id": 130053095,
            "question": "A significant disadvantage of current FPGAs compared to ASIC CPUs is their lower ____.",
            "options": [
                "power consumption",
                "clock speed",
                "reliability",
                "cost"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A significant disadvantage of current FPGAs compared to ASIC CPUs is their lower ____.",
                "Expected Label": "B",
                "Predicted Label": "B) clock speed \n"
            }
        ]
    },
    {
        "doc_id": 375,
        "doc": {
            "question_id": 130053099,
            "question": "When designing algorithms for FPGAs, ____ is a key consideration for achieving significant computational gains.",
            "options": [
                "Maximizing conditional statements",
                "Increasing I/O operations",
                "Deepening nested loops",
                "Exploiting data parallelism"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When designing algorithms for FPGAs, ____ is a key consideration for achieving significant computational gains.",
                "Expected Label": "D",
                "Predicted Label": "D) Exploiting data parallelism \n\n**Explanation:**\n\nFPGAs excel at data parallelism because their architecture allows for the creation of massively parallel custom hardware pipelines.  Here's why the other options are less relevant:\n\n* **A) Maximizing conditional statements:** Conditional statements can introduce branching, which can hinder performance in FPGAs due to potential pipeline stalls.\n* **B) Increasing I/O operations:** While I/O is important, it's not the primary factor for achieving computational gains within the FPGA itself.\n* **C) Deepening nested loops:** Similar to conditional statements, deeply nested loops might not map efficiently to the parallel nature of FPGAs.\n\n**In essence, leveraging the ability to perform the same operation on multiple data points simultaneously is how algorithms on FPGAs achieve significant speedups.** \n"
            }
        ]
    },
    {
        "doc_id": 376,
        "doc": {
            "question_id": 130053101,
            "question": "Simulink, a graphical language, is primarily used in the fields of ____ and digital signal analysis.",
            "options": [
                "control theory",
                "database management",
                "artificial intelligence",
                "operating systems"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Simulink, a graphical language, is primarily used in the fields of ____ and digital signal analysis.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 377,
        "doc": {
            "question_id": -1,
            "question": "The ____ format is usually used to store data.",
            "options": [
                "BCD",
                "Decimal",
                "Hexadecimal",
                "Octal"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ format is usually used to store data.",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 378,
        "doc": {
            "question_id": -1,
            "question": "The 8-bit encoding format used to store data in a computer is ____",
            "options": [
                "ASCII",
                "EBCDIC",
                "ANCI",
                "USCII"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The 8-bit encoding format used to store data in a computer is ____",
                "Expected Label": "B",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 379,
        "doc": {
            "question_id": -1,
            "question": "A source program is usually in ____",
            "options": [
                "Assembly language",
                "Machine level language",
                "High-level language",
                "Natural language"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A source program is usually in ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 380,
        "doc": {
            "question_id": -1,
            "question": "Which memory device is generally made of semiconductors?",
            "options": [
                "RAM",
                "Hard-disk",
                "Floppy disk",
                "Cd disk"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which memory device is generally made of semiconductors?",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 381,
        "doc": {
            "question_id": -1,
            "question": "The small extremely fast, RAM\u2019s are called as ____",
            "options": [
                "Cache",
                "Heaps",
                "Accumulators",
                "Stacks"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The small extremely fast, RAM\u2019s are called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 382,
        "doc": {
            "question_id": -1,
            "question": "The ALU makes use of ____ to store the intermediate results.",
            "options": [
                "Accumulators",
                "Registers",
                "Heap",
                "Stack"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ALU makes use of ____ to store the intermediate results.",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 383,
        "doc": {
            "question_id": -1,
            "question": "The control unit controls other units by generating ____",
            "options": [
                "Control signals",
                "Timing signals",
                "Transfer signals",
                "Command Signals"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The control unit controls other units by generating ____",
                "Expected Label": "B",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 384,
        "doc": {
            "question_id": -1,
            "question": "____ are numbers and encoded characters, generally used as operands.",
            "options": [
                "Input",
                "Data",
                "Information",
                "Stored Values"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ are numbers and encoded characters, generally used as operands.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 385,
        "doc": {
            "question_id": -1,
            "question": "The Input devices can send information to the processor.",
            "options": [
                "When the SIN status flag is set",
                "When the data arrives regardless of the SIN flag",
                "Neither of the cases",
                "Either of the cases"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The Input devices can send information to the processor.",
                "Expected Label": "A",
                "Predicted Label": "**A) When the SIN status flag is set**\n\n**Explanation:**\n\nInput devices typically use a status flag (like SIN) to signal to the processor that they have data ready to be read. The processor checks this flag and, if set, initiates the data transfer from the input device. \n"
            }
        ]
    },
    {
        "doc_id": 386,
        "doc": {
            "question_id": -1,
            "question": "____ bus structure is usually used to connect I/O devices.",
            "options": [
                "Single bus",
                "Multiple bus",
                "Star bus",
                "Rambus"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ bus structure is usually used to connect I/O devices.",
                "Expected Label": "A",
                "Predicted Label": "B) Multiple bus \n\n**Explanation:**\n\nWhile simpler systems might use a single bus for everything, I/O devices often benefit from dedicated buses due to their different speed and data transfer requirements. This leads to a multiple bus architecture where:\n\n* **High-speed components** (like CPU and memory) share a fast bus.\n* **Slower I/O devices** are connected via separate, often specialized buses (e.g., PCI, USB).\n\n**Let's analyze why other options are less suitable:**\n\n* **A) Single bus:**  Can lead to bottlenecks with diverse I/O speeds.\n* **C) Star bus:** More common for networking, not typically used within a computer system for connecting I/O.\n* **D) Rambus:** A specific memory technology, not a general bus structure for I/O. \n"
            }
        ]
    },
    {
        "doc_id": 387,
        "doc": {
            "question_id": -1,
            "question": "The I/O interface required to connect the I/O device to the bus consists of ____",
            "options": [
                "Address decoder and registers",
                "Control circuits",
                "Address decoder, registers and Control circuits",
                "Only Control circuits"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The I/O interface required to connect the I/O device to the bus consists of ____",
                "Expected Label": "C",
                "Predicted Label": "C"
            }
        ]
    },
    {
        "doc_id": 388,
        "doc": {
            "question_id": -1,
            "question": "To reduce the memory access time we generally make use of  ____",
            "options": [
                "Heaps",
                "Higher capacity RAM\u2019s",
                "SDRAM\u2019s",
                "Cache\u2019s"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "To reduce the memory access time we generally make use of  ____",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 389,
        "doc": {
            "question_id": -1,
            "question": "____ is generally used to increase the apparent size of physical memory.",
            "options": [
                "Secondary memory",
                "Virtual memory",
                "Hard-disk",
                "Disks"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is generally used to increase the apparent size of physical memory.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 390,
        "doc": {
            "question_id": -1,
            "question": "MFC stands for ____",
            "options": [
                "Memory Format Caches",
                "Memory Function Complete",
                "Memory Find Command",
                "Mass Format Command"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "MFC stands for ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 391,
        "doc": {
            "question_id": -1,
            "question": "The time delay between two successive initiations of memory operation ____",
            "options": [
                "Memory access time",
                "Memory search time",
                "Memory cycle time",
                "Instruction delay"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The time delay between two successive initiations of memory operation ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Memory cycle time** is the time required between successive memory operations (read or write). \n\nHere's why the other options are incorrect:\n\n* **Memory access time:**  The time to access a specific memory location after it's been selected.\n* **Memory search time:**  Not a standard term in computer architecture.\n* **Instruction delay:** Refers to the time an instruction is stalled due to pipeline hazards or other factors. \n"
            }
        ]
    },
    {
        "doc_id": 392,
        "doc": {
            "question_id": -1,
            "question": "The decoded instruction is stored in ____",
            "options": [
                "IR",
                "PC",
                "Registers",
                "MDR"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The decoded instruction is stored in ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **IR (Instruction Register):** Holds the currently fetched instruction, which has been decoded for the CPU to understand and execute. \n* **PC (Program Counter):** Stores the memory address of the *next* instruction to be fetched.\n* **Registers:** General-purpose storage locations within the CPU for quick data access. While decoded instructions might temporarily use registers for operands, they aren't *stored* there.\n* **MDR (Memory Data Register):**  Acts as a buffer between the CPU and memory, holding data being transferred to or from memory.\n\n**Therefore, the decoded instruction is stored in the IR (Instruction Register).** \n"
            }
        ]
    },
    {
        "doc_id": 393,
        "doc": {
            "question_id": -1,
            "question": "The instruction -> Add LOCA, R0 does ____",
            "options": [
                "Adds the value of LOCA to R0 and stores in the temp register",
                "Adds the value of R0 to the address of LOCA",
                "Adds the values of both LOCA and R0 and stores it in R0",
                "Adds the value of LOCA with a value in accumulator and stores it in R0"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instruction -> Add LOCA, R0 does ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe instruction `Add LOCA, R0` is a typical assembly language instruction. Here's how it breaks down:\n\n* **Add:** This is the operation being performed \u2013 addition.\n* **LOCA:** This represents a memory location (likely a variable).\n* **R0:** This is a register (a small, fast storage location within the CPU).\n\nThe instruction fetches the value stored at the memory location \"LOCA\" and adds it to the value currently held in register \"R0.\" The result of this addition is then stored back into register \"R0.\" \n"
            }
        ]
    },
    {
        "doc_id": 394,
        "doc": {
            "question_id": -1,
            "question": "Which registers can interact with the secondary storage?",
            "options": [
                "MAR",
                "PC",
                "IR",
                "R0"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which registers can interact with the secondary storage?",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **MAR (Memory Address Register)** holds the address of the memory location being accessed, which is crucial for interacting with secondary storage. Data is transferred to/from RAM from secondary storage, and the MAR is essential in this process.\n\nLet's look at why the other options are incorrect:\n\n* **PC (Program Counter)** holds the address of the next instruction to be fetched, dealing primarily with instruction execution flow.\n* **IR (Instruction Register)** holds the currently executing instruction, again related to instruction execution.\n* **R0** (typically a general-purpose register) is used for temporary data storage within the CPU and doesn't directly interact with secondary storage. \n"
            }
        ]
    },
    {
        "doc_id": 395,
        "doc": {
            "question_id": -1,
            "question": "During the execution of a program which gets initialized first?",
            "options": [
                "MDR",
                "IR",
                "PC",
                "MAR"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "During the execution of a program which gets initialized first?",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe **Program Counter (PC)** is initialized first. It holds the address of the next instruction to be fetched.  Here's why the other options are incorrect:\n\n* **MDR (Memory Data Register):**  Holds data being transferred to or from memory. It's not initialized at the very start of program execution.\n* **IR (Instruction Register):** Holds the current instruction being decoded and executed. It's loaded after the PC fetches the instruction.\n* **MAR (Memory Address Register):** Holds the address of the memory location being accessed. Like the MDR, it's used during instruction fetching and execution but not initialized at the very beginning. \n"
            }
        ]
    },
    {
        "doc_id": 396,
        "doc": {
            "question_id": -1,
            "question": "Which of the register/s of the processor is/are connected to Memory Bus?",
            "options": [
                "PC",
                "MAR",
                "IR",
                "Both PC and MAR"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Which of the register/s of the processor is/are connected to Memory Bus?",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **MAR (Memory Address Register):** Holds the address of the memory location that the processor wants to read from or write to. This register is directly connected to the memory bus to transmit the address.\n\nLet's look at why the other options are incorrect:\n\n* **PC (Program Counter):** Holds the address of the *next* instruction to be fetched. While it plays a role in instruction fetching, it's not directly connected to the memory bus.\n* **IR (Instruction Register):** Holds the instruction that is currently being decoded and executed. It's not directly involved in memory access. \n\n**In summary:** Only the MAR needs a direct connection to the memory bus to specify the address for data transfer. \n"
            }
        ]
    },
    {
        "doc_id": 397,
        "doc": {
            "question_id": -1,
            "question": "ISP stands for ____",
            "options": [
                "Instruction Set Processor",
                "Information Standard Processing",
                "Interchange Standard Protocol",
                "Interrupt Service Procedure"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "ISP stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\nThe answer is **A) Instruction Set Processor**. \n\nLet's break down why:\n\n* **Instruction Set Processor** refers to the part of the CPU that understands and executes the instructions a program is made of. It defines the fundamental operations the processor can perform.\n\nThe other options are incorrect:\n\n* **Information Standard Processing** is not a standard term in computer architecture.\n* **Interchange Standard Protocol** sounds plausible but is usually used in the context of data exchange formats, not hardware.\n* **Interrupt Service Procedure** is a software routine triggered by hardware interrupts, not directly related to the term \"ISP\". \n"
            }
        ]
    },
    {
        "doc_id": 398,
        "doc": {
            "question_id": -1,
            "question": "The internal components of the processor are connected by ____",
            "options": [
                "Processor intra-connectivity circuitry",
                "Processor bus",
                "Memory bus",
                "Rambus"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The internal components of the processor are connected by ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 399,
        "doc": {
            "question_id": -1,
            "question": "____ is used to choose between incrementing the PC or performing ALU operations.",
            "options": [
                "Conditional codes",
                "Multiplexer",
                "Control unit",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is used to choose between incrementing the PC or performing ALU operations.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **(C) Control unit**. Here's why:\n\n* **Control Unit:** The control unit is the \"brain\" of the processor. It fetches instructions from memory, decodes them, and then generates the necessary control signals to orchestrate other components, including whether to increment the program counter (PC) for the next instruction or to perform an ALU operation based on the current instruction.\n\nLet's look at the other options:\n\n* **Conditional Codes (Flags):** These are set by the ALU based on the result of the previous operation (like zero, negative, overflow). While they influence decisions, they don't directly choose between PC incrementing and ALU operations.\n* **Multiplexer (MUX):** A multiplexer selects one of several input signals and forwards it to a single output. While used in the datapath controlled by the control unit, it doesn't make the high-level decision of PC vs. ALU. \n* **None of the mentioned:**  The control unit is the core component responsible for this decision. \n"
            }
        ]
    },
    {
        "doc_id": 400,
        "doc": {
            "question_id": -1,
            "question": "The registers, ALU and the interconnection between them are collectively called as ____",
            "options": [
                "process route",
                "information trail",
                "information path",
                "data path"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The registers, ALU and the interconnection between them are collectively called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 401,
        "doc": {
            "question_id": -1,
            "question": "____ is used to store data in registers.",
            "options": [
                "D flip flop",
                "JK flip flop",
                "RS flip flop",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is used to store data in registers.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 402,
        "doc": {
            "question_id": -1,
            "question": "The main virtue for using single Bus structure is ____",
            "options": [
                "Fast data transfers",
                "Cost effective connectivity and speed",
                "Cost effective connectivity and ease of attaching peripheral devices",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The main virtue for using single Bus structure is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile single bus structures are simple and inexpensive, they are not known for speed. Their primary advantage is their **cost-effectiveness** and the **ease** with which **peripheral devices** can be added or removed. \n\nLet me know if you'd like to explore other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 403,
        "doc": {
            "question_id": -1,
            "question": "____ are used to overcome the difference in data transfer speeds of various devices.",
            "options": [
                "Speed enhancing circuitory",
                "Bridge circuits",
                "Multiple Buses",
                "Buffer registers"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are used to overcome the difference in data transfer speeds of various devices.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 404,
        "doc": {
            "question_id": -1,
            "question": "To extend the connectivity of the processor bus we use ____",
            "options": [
                "PCI bus",
                "SCSI bus",
                "Controllers",
                "Multiple bus"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To extend the connectivity of the processor bus we use ____",
                "Expected Label": "A",
                "Predicted Label": "D) Multiple bus \n\n**Explanation:**\n\nWhile the other options have relevance to connectivity, here's why \"Multiple bus\" is the most accurate answer:\n\n* **Multiple bus architectures** are fundamentally how modern systems extend processor bus connectivity. They use hierarchies of buses (e.g., local bus, system bus, expansion bus) to connect the processor to various components like memory, peripherals, and expansion slots.\n\nLet's break down why the other options aren't the best fit:\n\n* **A) PCI bus:** PCI (Peripheral Component Interconnect) is a type of expansion bus, but it's not the overarching method for extending processor bus connectivity.\n* **B) SCSI bus:** SCSI (Small Computer System Interface) is designed for high-speed peripherals, typically storage devices. It doesn't directly address the broader concept of extending the processor bus.\n* **C) Controllers:** Controllers manage the communication between the processor/bus and specific devices. They are essential but don't represent the core mechanism of extending bus connectivity. \n"
            }
        ]
    },
    {
        "doc_id": 405,
        "doc": {
            "question_id": -1,
            "question": "IBM developed a bus standard for their line of computers \u2018PC AT\u2019 called ____",
            "options": [
                "IB bus",
                "M-bus",
                "ISA",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "IBM developed a bus standard for their line of computers \u2018PC AT\u2019 called ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 406,
        "doc": {
            "question_id": -1,
            "question": "ANSI stands for ____",
            "options": [
                "American National Standards Institute",
                "American National Standard Interface",
                "American Network Standard Interfacing",
                "American Network Security Interrupt"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "ANSI stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 407,
        "doc": {
            "question_id": -1,
            "question": "____ register Connected to the Processor bus is a single-way transfer capable.",
            "options": [
                "PC",
                "IR",
                "Temp",
                "Z"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ register Connected to the Processor bus is a single-way transfer capable.",
                "Expected Label": "D",
                "Predicted Label": "**B) IR**\n\n**Explanation:**\n\n* **IR (Instruction Register)** is directly connected to the processor bus and receives the instruction fetched from memory. This transfer is one-way, meaning data flows from the bus to the IR.\n\nLet's look at why the other options are incorrect:\n\n* **PC (Program Counter)** stores the address of the next instruction to be fetched. It needs to both send and receive data (updating itself and providing addresses), so it's not single-way.\n* **Temp** registers are internal to the processor and might not be directly connected to the main bus. Even if they are, their communication is likely bi-directional.\n* **Z (Zero) flag** is a single bit within a status register, not a standalone register connected to the bus. \n\n**Therefore, the best answer is B.** \n"
            }
        ]
    },
    {
        "doc_id": 408,
        "doc": {
            "question_id": -1,
            "question": "In multiple Bus organisation, the registers are collectively placed and referred as ____",
            "options": [
                "Set registers",
                "Register file",
                "Register Block",
                "Map registers"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In multiple Bus organisation, the registers are collectively placed and referred as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 409,
        "doc": {
            "question_id": -1,
            "question": "The main advantage of multiple bus organisation over a single bus is ____",
            "options": [
                "Reduction in the number of cycles for execution",
                "Increase in size of the registers",
                "Better Connectivity",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The main advantage of multiple bus organisation over a single bus is ____",
                "Expected Label": "A",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 410,
        "doc": {
            "question_id": -1,
            "question": "The ISA standard Buses are used to connect ____",
            "options": [
                "RAM and processor",
                "GPU and processor",
                "Harddisk and Processor",
                "CD/DVD drives and Processor"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ISA standard Buses are used to connect ____",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile buses *can* technically connect all the components listed to the processor in some manner, the ISA (Industry Standard Architecture) bus was specifically designed for connections like:\n\n* **RAM and processor:** This is the primary function of a system bus like ISA. It allows the CPU to read data from and write data to the system memory (RAM).\n* **Expansion cards and processor:** ISA was widely used for adding functionality via expansion cards (like sound cards, network cards, etc.) which need to communicate with the processor.\n\nOptions B, C, and D often use different, more specialized bus standards for faster and more efficient communication. \n"
            }
        ]
    },
    {
        "doc_id": 411,
        "doc": {
            "question_id": -1,
            "question": "During the execution of the instructions, a copy of the instructions is placed in the ____",
            "options": [
                "Register",
                "RAM",
                "System heap",
                "Cache"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "During the execution of the instructions, a copy of the instructions is placed in the ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Cache** is a small, fast memory that stores frequently used instructions and data for quick access by the CPU. When the CPU needs to execute an instruction, it first checks the cache. If the instruction is found (a cache hit), it can be fetched much faster than from RAM. \n\nLet's look at why the other options are not the best fit:\n\n* **Registers** are even faster than cache but much smaller. They hold data and instructions that are actively being processed.\n* **RAM** is the main memory where programs and data are stored, but it's slower to access than cache.\n* **System heap** is a region of memory used for dynamic memory allocation and isn't directly involved in storing copies of instructions for execution. \n"
            }
        ]
    },
    {
        "doc_id": 412,
        "doc": {
            "question_id": -1,
            "question": "Two processors A and B have clock frequencies of 700 Mhz and 900 Mhz respectively. Suppose A can execute an instruction with an average of 3 steps and B can execute with an average of 5 steps. For the execution of the same instruction which processor is faster?",
            "options": [
                "A",
                "B",
                "Both take the same time",
                "Insufficient information"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Two processors A and B have clock frequencies of 700 Mhz and 900 Mhz respectively. Suppose A can execute an instruction with an average of 3 steps and B can execute with an average of 5 steps. For the execution of the same instruction which processor is faster?",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 413,
        "doc": {
            "question_id": -1,
            "question": "A processor performing fetch or decoding of different instruction during the execution of another instruction is called ____",
            "options": [
                "Super-scaling",
                "Pipe-lining",
                "Parallel Computation",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A processor performing fetch or decoding of different instruction during the execution of another instruction is called ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 414,
        "doc": {
            "question_id": -1,
            "question": "For a given FINITE number of instructions to be executed, which architecture of the processor provides for a faster execution?",
            "options": [
                "ISA",
                "ANSA",
                "Super-scalar",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "For a given FINITE number of instructions to be executed, which architecture of the processor provides for a faster execution?",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Super-scalar**. Here's why:\n\n* **ISA (Instruction Set Architecture):** Defines the instructions a processor can execute but doesn't dictate the underlying implementation or speed.\n* **ANSA (Advanced Network System Architecture):** This is a networking architecture, not a processor architecture.\n* **Super-scalar:** These architectures can execute multiple instructions concurrently, leading to faster execution for a given set of instructions.\n\n**Therefore, for a finite number of instructions, a super-scalar architecture will generally result in faster execution due to its parallel processing capabilities.** \n"
            }
        ]
    },
    {
        "doc_id": 415,
        "doc": {
            "question_id": -1,
            "question": "The clock rate of the processor can be improved by ____",
            "options": [
                "Improving the IC technology of the logic circuits",
                "Reducing the amount of processing done in one step",
                "By using the overclocking method",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The clock rate of the processor can be improved by ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 416,
        "doc": {
            "question_id": -1,
            "question": "An optimizing Compiler does ____",
            "options": [
                "Better compilation of the given piece of code",
                "Takes advantage of the type of processor and reduces its process time",
                "Does better memory management",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "An optimizing Compiler does ____",
                "Expected Label": "B",
                "Predicted Label": "B) Takes advantage of the type of processor and reduces its process time \n"
            }
        ]
    },
    {
        "doc_id": 417,
        "doc": {
            "question_id": -1,
            "question": "The ultimate goal of a compiler is to ____",
            "options": [
                "Reduce the clock cycles for a programming task",
                "Reduce the size of the object code",
                "Be versatile",
                "Be able to detect even the smallest of errors"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ultimate goal of a compiler is to ____",
                "Expected Label": "A",
                "Predicted Label": "A) Reduce the clock cycles for a programming task \n\nHere's why:\n\nWhile compilers aim to produce efficient code (potentially smaller size), their **primary goal** is to translate high-level code into machine code that the processor can understand and execute.  Optimizing this translation process directly impacts how many clock cycles the processor needs to complete the task.\n\nLet's break down why the other options aren't the ultimate goal:\n\n* **B) Reduce the size of the object code:** Code size optimization is a factor, but not the ultimate goal. Efficiency in execution time often takes precedence.\n* **C) Be versatile:** Versatility (supporting various languages/architectures) is a desirable compiler trait, not the ultimate goal.\n* **D) Be able to detect even the smallest of errors:** Error detection is primarily the role of a compiler's front-end and static analysis phases. While important, it's a step towards the goal, not the goal itself. \n\n**Therefore, A is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 418,
        "doc": {
            "question_id": -1,
            "question": "SPEC stands for ____",
            "options": [
                "Standard Performance Evaluation Code",
                "System Processing Enhancing Code",
                "System Performance Evaluation Corporation",
                "Standard Processing Enhancement Corporation"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "SPEC stands for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nThe correct answer is **C) System Performance Evaluation Corporation**. \n\nSPEC is a well-known organization that develops standardized benchmarks to measure and compare computer system performance. \n"
            }
        ]
    },
    {
        "doc_id": 419,
        "doc": {
            "question_id": -1,
            "question": "As of 2000, the reference system to find the performance of a system is ____",
            "options": [
                "Ultra SPARC 10",
                "SUN SPARC",
                "SUN II",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "As of 2000, the reference system to find the performance of a system is ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\nWhile options A and B mention real systems (with Ultra SPARC being a more powerful successor), none of them were ever considered THE reference system for performance comparisons in 2000 or any other period. \n\nPerformance benchmarks are used to compare systems, and there are many of them, each with different focuses and workloads.  Using a single system as a universal reference point isn't a standard practice. \n"
            }
        ]
    },
    {
        "doc_id": 420,
        "doc": {
            "question_id": -1,
            "question": "When Performing a looping operation, the instruction gets stored in the ____",
            "options": [
                "Registers",
                "Cache",
                "System Heap",
                "System stack"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When Performing a looping operation, the instruction gets stored in the ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **A) Registers**. Here's why:\n\n* **Registers** are the fastest memory locations within a CPU. When a loop executes, the loop counter and frequently accessed variables are usually stored in registers for quick access. \n\nLet's examine why the other options are incorrect:\n\n* **Cache:** Cache stores frequently accessed data from main memory to speed up access, but it doesn't specifically hold instructions during a loop.\n* **System Heap:** The heap is used for dynamic memory allocation, not for storing loop instructions.\n* **System Stack:** The stack manages function calls and local variables, but it's not the primary location for storing loop instructions themselves. \n\n**In summary:** Registers are the most efficient place to store instructions and data that need to be accessed repeatedly within a loop. \n"
            }
        ]
    },
    {
        "doc_id": 421,
        "doc": {
            "question_id": -1,
            "question": "The average number of steps taken to execute the set of instructions can be made to be less than one by following ____",
            "options": [
                "ISA",
                "Pipe-lining",
                "Super-scaling",
                "Sequential"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The average number of steps taken to execute the set of instructions can be made to be less than one by following ____",
                "Expected Label": "C",
                "Predicted Label": "B) Pipe-lining \n\nHere's why:\n\n* **Pipelining** breaks down instruction execution into stages and allows multiple instructions to be processed in different stages simultaneously. This overlap makes it possible to complete, on average, more than one instruction per clock cycle, effectively reducing the average steps per instruction to less than one.\n\nLet's look at why the other options are incorrect:\n\n* **ISA (Instruction Set Architecture):**  ISA defines the instructions a processor can execute but doesn't inherently reduce the average steps per instruction.\n* **Super-scaling:**  Super-scaling focuses on increasing the number of instructions executed per cycle by having multiple execution units, but it doesn't necessarily push the average steps per instruction below one.\n* **Sequential:** Sequential execution implies processing one instruction completely before starting the next, making it impossible to achieve less than one average step per instruction. \n"
            }
        ]
    },
    {
        "doc_id": 422,
        "doc": {
            "question_id": -1,
            "question": "If the instruction, Add R1, R2, R3 is executed in a system that is pipe-lined, then the value of S is (Where S is a term of the Basic performance equation)?",
            "options": [
                "3",
                "~2",
                "~1",
                "6"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "If the instruction, Add R1, R2, R3 is executed in a system that is pipe-lined, then the value of S is (Where S is a term of the Basic performance equation)?",
                "Expected Label": "C",
                "Predicted Label": "**C) ~1**\n\n**Explanation:**\n\nIn a pipelined system, the execution of instructions is overlapped.  While the ideal scenario aims for one instruction completion per clock cycle (S=1), there are always pipeline overheads (stalls, hazards) that prevent achieving a perfect \"1\".  Hence, the performance is slightly less than 1 instruction per cycle. \n"
            }
        ]
    },
    {
        "doc_id": 423,
        "doc": {
            "question_id": -1,
            "question": "CISC stands for ____",
            "options": [
                "Complete Instruction Sequential Compilation",
                "Computer Integrated Sequential Compiler",
                "Complex Instruction Set Computer",
                "Complex Instruction Sequential Compilation"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "CISC stands for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 424,
        "doc": {
            "question_id": -1,
            "question": "As of 2000, the reference system to find the SPEC rating are built with ____ Processor.",
            "options": [
                "Intel Atom SParc 300Mhz",
                "Ultra SPARC -IIi 300MHZ",
                "Amd Neutrino series",
                "ASUS A series 450 Mhz"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "As of 2000, the reference system to find the SPEC rating are built with ____ Processor.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Ultra SPARC -IIi 300MHZ**. \n\nHere's why:\n\n* **SPEC (Standard Performance Evaluation Corporation)** uses standardized benchmarks to measure computer system performance.\n* Around the year 2000, the dominant processor used in SPEC reference systems was the **UltraSPARC -IIi** running at 300MHz. This was a high-performance processor common in workstations and servers of that era.\n\nLet me know if you'd like more information on SPEC benchmarks or computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 425,
        "doc": {
            "question_id": -1,
            "question": "The instruction, Add #45,R1 does ____",
            "options": [
                "Adds the value of 45 to the address of R1 and stores 45 in that address",
                "Adds 45 to the value of R1 and stores it in R1",
                "Finds the memory location 45 and adds that content to that of R1",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The instruction, Add #45,R1 does ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 426,
        "doc": {
            "question_id": -1,
            "question": "In the case of, Zero-address instruction method the operands are stored in ____",
            "options": [
                "Registers",
                "Accumulators",
                "Push down stack",
                "Cache"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the case of, Zero-address instruction method the operands are stored in ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nZero-address instructions rely on a stack-based architecture. Operands are implicitly pulled from the top of the pushdown stack, and results are pushed back onto the stack. \n"
            }
        ]
    },
    {
        "doc_id": 427,
        "doc": {
            "question_id": -1,
            "question": "Add #45, when this instruction is executed the following happen/s ____",
            "options": [
                "The processor raises an error and requests for one more operand",
                "The value stored in memory location 45 is retrieved and one more operand is requested",
                "The value 45 gets added to the value on the stack and is pushed onto the stack",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Add #45, when this instruction is executed the following happen/s ____",
                "Expected Label": "B",
                "Predicted Label": "**D) None of the mentioned**\n\n**Explanation:**\n\nThe instruction \"Add #45\" is incomplete. In most assembly languages, arithmetic instructions like \"Add\" require two operands. The instruction needs to specify what to add #45 to. \n\nHere's why the other options are incorrect:\n\n* **A) The processor raises an error and requests for one more operand:**  While some processors might handle this with an error, a more likely scenario is an invalid instruction error.\n* **B) The value stored in memory location 45 is retrieved and one more operand is requested:** This assumes the instruction implies adding a value from a memory address, which isn't clear from the given instruction.\n* **C) The value 45 gets added to the value on the stack and is pushed onto the stack:** This assumes the instruction is meant to operate with a stack, which is not specified.\n\n**A complete instruction would look like:**\n\n* **Add R1, #45**  (Add 45 to the value in register R1)\n* **Add [memory_address], #45** (Add 45 to the value at the given memory address) \n"
            }
        ]
    },
    {
        "doc_id": 428,
        "doc": {
            "question_id": -1,
            "question": "The addressing mode which makes use of in-direction pointers is ____",
            "options": [
                "Indirect addressing mode",
                "Index addressing mode",
                "Relative addressing mode",
                "Offset addressing mode"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The addressing mode which makes use of in-direction pointers is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 429,
        "doc": {
            "question_id": -1,
            "question": "In the following indexed addressing mode instruction, MOV 5(R1), LOC the effective address is ____",
            "options": [
                "EA = 5+R1",
                "EA = R1",
                "EA = [R1]",
                "EA = 5+[R1]"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In the following indexed addressing mode instruction, MOV 5(R1), LOC the effective address is ____",
                "Expected Label": "D",
                "Predicted Label": "D) EA = 5+[R1] \n\n**Explanation:**\n\nIndexed addressing modes calculate the effective address (EA) by adding the content of a register (R1 in this case) to a constant offset (5 in this case). The brackets [] typically denote \"the content of\" in computer architecture. \n\nTherefore, the effective address is the value stored at the memory location pointed to by the sum of the offset 5 and the content of register R1. \n"
            }
        ]
    },
    {
        "doc_id": 430,
        "doc": {
            "question_id": -1,
            "question": "The addressing mode/s, which uses the PC instead of a general purpose register is  ____",
            "options": [
                "Indexed with offset",
                "Relative",
                "Direct",
                "Both Indexed with offset and direct"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The addressing mode/s, which uses the PC instead of a general purpose register is  ____",
                "Expected Label": "B",
                "Predicted Label": "**B) Relative** \n\n**Explanation:**\n\n* **Relative addressing** calculates the address of the target data by adding an offset to the current value of the Program Counter (PC). \n\nLet's analyze why the other options are incorrect:\n\n* **Indexed with offset** uses a base address from a general-purpose register and adds an index (often from another register) and an optional offset.\n* **Direct addressing** directly specifies the memory address of the operand within the instruction itself. \n* **Both Indexed with offset and direct**  is incorrect as we've already established that neither solely uses the PC. \n"
            }
        ]
    },
    {
        "doc_id": 431,
        "doc": {
            "question_id": -1,
            "question": "The addressing mode, where you directly specify the operand value is ____",
            "options": [
                "Immediate",
                "Direct",
                "Definite",
                "Relative"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The addressing mode, where you directly specify the operand value is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Immediate Addressing:** The operand value is directly specified within the instruction. \n* **Direct Addressing:** The operand's memory address is directly specified in the instruction.\n* **Definite Addressing:**  Not a standard addressing mode in computer architecture.\n* **Relative Addressing:** The operand's address is calculated relative to a base address (usually the program counter). \n"
            }
        ]
    },
    {
        "doc_id": 432,
        "doc": {
            "question_id": -1,
            "question": "The effective address of the following instruction is MUL 5(R1,R2).",
            "options": [
                "5+R1+R2",
                "5+(R1*R2)",
                "5+[R1]+[R2]",
                "5*([R1]+[R2])"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The effective address of the following instruction is MUL 5(R1,R2).",
                "Expected Label": "C",
                "Predicted Label": "C) 5+[R1]+[R2] \n\n**Explanation:**\n\nIn computer architecture, addressing modes determine how the CPU calculates the memory location (effective address) to access data. \n\nThe given instruction `MUL 5(R1,R2)` uses a common addressing mode often called **base displacement addressing** or **indexed displacement addressing.** Here's how it breaks down:\n\n* **MUL:**  This is the opcode indicating a multiplication operation.\n* **5:** This is the displacement value (a constant offset).\n* **(R1, R2):** This part indicates that the contents of registers R1 and R2 are used in calculating the address.\n\nThe effective address is calculated as follows:\n\n**Effective Address = Displacement + [Value in R1] + [Value in R2]**\n\nTherefore, the correct answer is **C) 5+[R1]+[R2]**. \n"
            }
        ]
    },
    {
        "doc_id": 433,
        "doc": {
            "question_id": -1,
            "question": "____ addressing mode is most suitable to change the normal sequence of execution of instructions.",
            "options": [
                "Relative",
                "Indirect",
                "Index with Offset",
                "Immediate"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ addressing mode is most suitable to change the normal sequence of execution of instructions.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 434,
        "doc": {
            "question_id": -1,
            "question": "Which method/s of representation of numbers occupies a large amount of memory than others?",
            "options": [
                "Sign-magnitude",
                "1\u2019s complement",
                "2\u2019s complement",
                "1\u2019s & 2\u2019s compliment"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which method/s of representation of numbers occupies a large amount of memory than others?",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 435,
        "doc": {
            "question_id": -1,
            "question": "Which representation is most efficient to perform arithmetic operations on the numbers?",
            "options": [
                "Sign-magnitude",
                "1\u2019s complement",
                "2\u2019S complement",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Which representation is most efficient to perform arithmetic operations on the numbers?",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 436,
        "doc": {
            "question_id": -1,
            "question": "Which method of representation has two representations for \u20180\u2019?",
            "options": [
                "Sign-magnitude",
                "1\u2019s complement",
                "2\u2019s complement",
                "Both Sign-magnitude and 1\u2019s complement"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Which method of representation has two representations for \u20180\u2019?",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 437,
        "doc": {
            "question_id": -1,
            "question": "When we perform subtraction on -7 and 1 the answer in 2\u2019s complement form is ____",
            "options": [
                "1010",
                "1110",
                "0110",
                "1000"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When we perform subtraction on -7 and 1 the answer in 2\u2019s complement form is ____",
                "Expected Label": "D",
                "Predicted Label": "B \n\nHere's why:\n\n* **Two's Complement Representation:** In two's complement, negative numbers are represented by inverting the bits of the positive representation and adding 1. \n\n* **Steps**\n    1. **Represent 7 in binary:** 0111\n    2. **Find the two's complement of 7 (-7):** \n        * Invert the bits: 1000\n        * Add 1:       1001\n    3. **Represent 1 in binary:** 0001\n    4. **Perform the binary addition: (-7) + 1**\n       ```\n        1001\n      + 0001\n      -------\n        1010 \n       ```\n\n* **Result:** The two's complement representation of -8 (the result of -7 - 1) is **1010**. \n"
            }
        ]
    },
    {
        "doc_id": 438,
        "doc": {
            "question_id": -1,
            "question": "When we perform subtraction on -7 and -5 the answer in 2\u2019s complement form is ____",
            "options": [
                "11110",
                "1110",
                "1010",
                "0010"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When we perform subtraction on -7 and -5 the answer in 2\u2019s complement form is ____",
                "Expected Label": "B",
                "Predicted Label": "B) 1110 \n\nHere's why:\n\n* **2's Complement Representation:**  Negative numbers in binary are represented using 2's complement. To find the 2's complement of a number, you invert all the bits and add 1.\n* **Calculation:**\n    1. **-7 in 2's complement (assuming 4 bits):**  7 in binary is 0111. Invert: 1000. Add 1: 1001.\n    2. **-5 in 2's complement (assuming 4 bits):** 5 in binary is 0101. Invert: 1010. Add 1: 1011.\n    3. **Subtraction is addition of the first number and the 2's complement of the second:** 1001 + 1011 = 10100.  \n    4. **Discard the carry bit (leftmost 1) as we are working with 4 bits:** The answer is 0100.\n    5. **Represent the answer in 2's complement for negative results:** Invert: 1011. Add 1: **1110**. \n"
            }
        ]
    },
    {
        "doc_id": 439,
        "doc": {
            "question_id": -1,
            "question": "When we subtract -3 from 2 , the answer in 2\u2019s complement form is ____",
            "options": [
                "0001",
                "1101",
                "0101",
                "1001"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "When we subtract -3 from 2 , the answer in 2\u2019s complement form is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n1. **2's complement of -3:**\n   * Start with the binary representation of 3: 0011\n   * Invert the bits:         1100\n   * Add 1:                1101  (-3 in 2's complement)\n\n2. **Add 2 to the 2's complement of -3:**\n   *  0010  (+2 in binary)\n   + 1101  (-3 in 2's complement)\n   ------\n    0101  (+5 in binary) \n\nLet me know if you'd like to work through more examples! \n"
            }
        ]
    },
    {
        "doc_id": 440,
        "doc": {
            "question_id": -1,
            "question": "The processor keeps track of the results of its operations using flags called ____",
            "options": [
                "Conditional code flags",
                "Test output flags",
                "Type flags",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The processor keeps track of the results of its operations using flags called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Conditional code flags** (also known as status flags) are used by the processor to store information about the outcome of the most recent instruction. These flags indicate things like if the result was zero, negative, or involved an overflow. \n"
            }
        ]
    },
    {
        "doc_id": 441,
        "doc": {
            "question_id": -1,
            "question": "The register used to store the flags is called as ____",
            "options": [
                "Flag register",
                "Status register",
                "Test register",
                "Log register"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The register used to store the flags is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 442,
        "doc": {
            "question_id": -1,
            "question": "The Flag \u2018V\u2019 is set to 1 indicates that ____",
            "options": [
                "The operation is valid",
                "The operation is validated",
                "The operation has resulted in an overflow",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Flag \u2018V\u2019 is set to 1 indicates that ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 443,
        "doc": {
            "question_id": -1,
            "question": "In some pipelined systems, a different instruction is used to add to numbers which can affect the flags upon execution. That instruction is ____",
            "options": [
                "AddSetCC",
                "AddCC",
                "Add++",
                "SumSetCC"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In some pipelined systems, a different instruction is used to add to numbers which can affect the flags upon execution. That instruction is ____",
                "Expected Label": "A",
                "Predicted Label": "**B) AddCC**\n\n**Explanation:**\n\n* **AddCC** (Add with Condition Codes) is a common instruction in pipelined architectures that performs addition while also updating condition code flags (e.g., zero, carry, overflow). \n\nThe other options are not standard instructions in computer architectures:\n\n* **AddSetCC** combines setting condition codes and addition but isn't a typical single instruction.\n* **Add++** implies incrementing after addition, which is usually handled separately.\n* **SumSetCC**  is not a standard instruction. \n"
            }
        ]
    },
    {
        "doc_id": 444,
        "doc": {
            "question_id": -1,
            "question": "The most efficient method followed by computers to multiply two unsigned numbers is ____",
            "options": [
                "Booth algorithm",
                "Bit pair recording of multipliers",
                "Restoring algorithm",
                "Non restoring algorithm"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The most efficient method followed by computers to multiply two unsigned numbers is ____",
                "Expected Label": "B",
                "Predicted Label": "B) Bit pair recording of multipliers \n"
            }
        ]
    },
    {
        "doc_id": 445,
        "doc": {
            "question_id": -1,
            "question": "For the addition of large integers, most of the systems make use of ____",
            "options": [
                "Fast adders",
                "Full adders",
                "Carry look-ahead adders",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "For the addition of large integers, most of the systems make use of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 446,
        "doc": {
            "question_id": -1,
            "question": "In a normal n-bit adder, to find out if an overflow as occurred we make use of ____",
            "options": [
                "And gate",
                "Nand gate",
                "Nor gate",
                "Xor gate"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a normal n-bit adder, to find out if an overflow as occurred we make use of ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nOverflow in adders is detected by comparing the carry-in to the most significant bit (MSB) with the carry-out from the MSB. An XOR gate effectively checks if these two carries are different, indicating an overflow:\n\n* **Carry-in to MSB = Carry-out from MSB:** No overflow (XOR output is 0)\n* **Carry-in to MSB \u2260 Carry-out from MSB:** Overflow (XOR output is 1) \n"
            }
        ]
    },
    {
        "doc_id": 447,
        "doc": {
            "question_id": -1,
            "question": "In the implementation of a Multiplier circuit in the system we make use of ____",
            "options": [
                "Counter",
                "Flip flop",
                "Shift register",
                "Push down stack"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the implementation of a Multiplier circuit in the system we make use of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nMultipliers in digital circuits heavily rely on **shift registers**. Here's why:\n\n* **Shifting for Binary Multiplication:** The core of binary multiplication involves shifting and adding. Each bit in the multiplier represents a potential shift of the multiplicand, followed by an addition if the bit is a '1'.\n* **Shift Register Role:** Shift registers efficiently handle the shifting operation. They store the intermediate results and allow for controlled shifting of bits, which is crucial for aligning the multiplicand correctly during each step of the multiplication process.\n\n**Why other options are less suitable:**\n\n* **Counters:** While counters track iterations, they don't inherently handle the shifting and storage needed for multiplication.\n* **Flip-flops:**  Flip-flops are basic memory elements and don't provide the sequential shifting capability required.\n* **Push-down stacks:**  Stacks operate on a LIFO (Last-In, First-Out) principle, which doesn't align well with the sequential shifting and adding nature of multiplication. \n"
            }
        ]
    },
    {
        "doc_id": 448,
        "doc": {
            "question_id": -1,
            "question": "When 1101 is used to divide 100010010 the remainder is ____",
            "options": [
                "101",
                "11",
                "0",
                "1"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When 1101 is used to divide 100010010 the remainder is ____",
                "Expected Label": "D",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 449,
        "doc": {
            "question_id": -1,
            "question": "The smallest entity of memory is called ____",
            "options": [
                "Cell",
                "Block",
                "Instance",
                "Unit"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The smallest entity of memory is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 450,
        "doc": {
            "question_id": -1,
            "question": "The collection of the above mentioned entities where data is stored is called ____",
            "options": [
                "Block",
                "Set",
                "Word",
                "Byte"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The collection of the above mentioned entities where data is stored is called ____",
                "Expected Label": "A",
                "Predicted Label": "The question is incomplete. You did not provide the \"above mentioned entities\". \n\nPlease provide the full question so I can give you the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 451,
        "doc": {
            "question_id": -1,
            "question": "If a system is 64 bit machine, then the length of each word will be ____",
            "options": [
                "4 bytes",
                "8 bytes",
                "16 bytes",
                "12 bytes"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If a system is 64 bit machine, then the length of each word will be ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 452,
        "doc": {
            "question_id": -1,
            "question": "The type of memory assignment used in Intel processors is ____",
            "options": [
                "Little Endian",
                "Big Endian",
                "Medium Endian",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The type of memory assignment used in Intel processors is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 453,
        "doc": {
            "question_id": -1,
            "question": "When using the Big Endian assignment to store a number, the sign bit of the number is stored in ____",
            "options": [
                "The higher order byte of the word",
                "The lower order byte of the word",
                "Can\u2019t say",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When using the Big Endian assignment to store a number, the sign bit of the number is stored in ____",
                "Expected Label": "A",
                "Predicted Label": "A) The higher order byte of the word \n\n**Explanation:**\n\nBig Endian systems store the most significant byte (MSB) at the lowest memory address. Since the sign bit is typically the most significant bit within the MSB, it resides in the higher-order byte of the word. \n"
            }
        ]
    },
    {
        "doc_id": 454,
        "doc": {
            "question_id": -1,
            "question": "To get the physical address from the logical address generated by CPU we use ____",
            "options": [
                "MAR",
                "MMU",
                "Overlays",
                "TLB"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To get the physical address from the logical address generated by CPU we use ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 455,
        "doc": {
            "question_id": -1,
            "question": "____ method is used to map logical addresses of variable length onto physical memory.",
            "options": [
                "Paging",
                "Overlays",
                "Segmentation",
                "Paging with segmentation"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ method is used to map logical addresses of variable length onto physical memory.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 456,
        "doc": {
            "question_id": -1,
            "question": "During the transfer of data between the processor and memory we use  ____",
            "options": [
                "Cache",
                "TLB",
                "Buffers",
                "Registers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "During the transfer of data between the processor and memory we use  ____",
                "Expected Label": "D",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile all the options relate to data storage and transfer within a computer system, **buffers** are specifically designed for temporary storage during data transfers between components with different speeds or priorities. This is precisely the situation during processor-memory communication.\n\nHere's why the other options are less suitable:\n\n* **Cache:** Cache memory speeds up data access by storing frequently used data closer to the processor. It's not directly involved in the transfer process itself.\n* **TLB (Translation Lookaside Buffer):** The TLB caches page table entries to speed up virtual-to-physical address translation, a crucial part of memory management but not the core transfer mechanism.\n* **Registers:** Registers are extremely fast storage locations within the processor itself. While they hold data being actively processed, they don't buffer data during processor-memory transfers. \n"
            }
        ]
    },
    {
        "doc_id": 457,
        "doc": {
            "question_id": -1,
            "question": "Physical memory is divided into sets of finite size called as ____",
            "options": [
                "Frames",
                "Pages",
                "Blocks",
                "Vectors"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Physical memory is divided into sets of finite size called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 458,
        "doc": {
            "question_id": -1,
            "question": "Add #%01011101,R1 , when this instruction is executed then ____",
            "options": [
                "The binary addition between the operands takes place",
                "The Numerical value represented by the binary value is added to the value of R1",
                "The addition doesn\u2019t take place, whereas this is similar to a MOV instruction",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Add #%01011101,R1 , when this instruction is executed then ____",
                "Expected Label": "A",
                "Predicted Label": "**B) The Numerical value represented by the binary value is added to the value of R1**\n\n**Explanation:**\n\nThe instruction is likely assembly code (though the specific syntax might vary). Here's a breakdown:\n\n* **#%01011101:** This indicates an immediate operand. The \"#\" symbol often denotes an immediate value, and \"% \" suggests a binary representation.  So, the instruction works with the numerical value represented by the binary \"01011101\".\n* **R1:**  This refers to a register named R1. Registers store data within the CPU.\n* **Add:** This is the operation to be performed \u2013 addition.\n\nTherefore, the instruction adds the numerical value represented by the binary \"01011101\" to the current value stored in register R1. \n"
            }
        ]
    },
    {
        "doc_id": 459,
        "doc": {
            "question_id": -1,
            "question": "If we want to perform memory or arithmetic operations on data in Hexa-decimal mode then we use ____ symbol before the operand.",
            "options": [
                "~",
                "!",
                "$",
                "*"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "If we want to perform memory or arithmetic operations on data in Hexa-decimal mode then we use ____ symbol before the operand.",
                "Expected Label": "C",
                "Predicted Label": "**C) $**\n\nThe dollar sign ($) is commonly used as a prefix to denote hexadecimal numbers in computer architecture and assembly languages. \n"
            }
        ]
    },
    {
        "doc_id": 460,
        "doc": {
            "question_id": -1,
            "question": "When generating physical addresses from a logical address the offset is stored in ____",
            "options": [
                "Translation look-aside buffer",
                "Relocation register",
                "Page table",
                "Shift register"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When generating physical addresses from a logical address the offset is stored in ____",
                "Expected Label": "B",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe **page table** maps logical addresses to physical addresses.  A logical address is divided into two parts:\n\n* **Page Number:** Used to index into the page table and find the corresponding physical page frame.\n* **Offset:** This value indicates the relative position of the desired byte within that page frame.  \n\n**The offset remains the same in both the logical and physical address.**\n\n**Why other options are incorrect:**\n\n* **A) Translation look-aside buffer (TLB):** The TLB caches recently used page table entries for faster address translation, but it doesn't directly store the offset.\n* **B) Relocation register:** This register holds a base address used to relocate an entire program, not a specific offset within a page.\n* **D) Shift register:** Shift registers are used for bit manipulation, not memory address translation. \n"
            }
        ]
    },
    {
        "doc_id": 461,
        "doc": {
            "question_id": -1,
            "question": "The technique used to store programs larger than the memory is ____",
            "options": [
                "Overlays",
                "Extension registers",
                "Buffers",
                "Both Extension registers and Buffers"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The technique used to store programs larger than the memory is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 462,
        "doc": {
            "question_id": -1,
            "question": "The unit which acts as an intermediate agent between memory and backing store to reduce process time is ____",
            "options": [
                "TLB\u2019s",
                "Registers",
                "Page tables",
                "Cache"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The unit which acts as an intermediate agent between memory and backing store to reduce process time is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Cache** acts as a high-speed buffer between the CPU and main memory. It stores frequently accessed data and instructions to reduce the time the CPU spends waiting for data from the slower main memory. \n\nLet's look at why the other options are incorrect:\n\n* **TLBs (Translation Lookaside Buffers):**  Speed up address translation between virtual and physical addresses, a part of memory management but not directly involved in buffering between memory and backing store.\n* **Registers:** Smallest and fastest memory locations within the CPU itself, used for holding data that the CPU is actively working on.\n* **Page tables:** Data structures used in virtual memory management to map virtual addresses to physical addresses.\n\n**Therefore, the best answer is D) Cache.** \n"
            }
        ]
    },
    {
        "doc_id": 463,
        "doc": {
            "question_id": -1,
            "question": "Does the Load instruction do the following operation/s?",
            "options": [
                "Loads the contents of a disc onto a memory location",
                "Loads the contents of a location onto the accumulators",
                "Load the contents of the PCB onto the register",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Does the Load instruction do the following operation/s?",
                "Expected Label": "B",
                "Predicted Label": "**B) Loads the contents of a location onto the accumulators**\n\n**Explanation:**\n\nA Load instruction in computer architecture typically copies data from a memory location into a CPU register, often the accumulator. \n\nLet's break down why the other options are incorrect:\n\n* **A) Loads the contents of a disc onto a memory location:** This describes a disk read operation, usually handled by the operating system and involving multiple instructions.\n* **C) Load the contents of the PCB onto the register:**  PCB refers to the Process Control Block, a data structure used by the OS. Loading this into a register is not a standard Load instruction function.\n* **D) None of the mentioned:** Option B accurately describes a common Load instruction. \n\n**Therefore, the correct answer is B.** \n"
            }
        ]
    },
    {
        "doc_id": 464,
        "doc": {
            "question_id": -1,
            "question": "Complete the following analogy:- Registers are to RAM\u2019s as Cache\u2019s are to ____",
            "options": [
                "System stacks",
                "Overlays",
                "Page Table",
                "TLB"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Complete the following analogy:- Registers are to RAM\u2019s as Cache\u2019s are to ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nHere's why the analogy holds:\n\n* **Registers : RAM :: Cache : TLB**\n\n* **Registers** are the fastest memory locations, holding data immediately used by the CPU.  \n* **RAM** is larger but slower, holding data and instructions currently in use.\n* **Cache** is a smaller, faster memory that sits between the CPU and RAM, holding frequently accessed data from RAM to speed up access.\n* **TLB (Translation Lookaside Buffer)** is a small, specialized cache that sits between the CPU and the page table (which manages virtual memory). It stores recently used virtual-to-physical address translations, speeding up memory access in systems using virtual memory.\n\n**In essence:**\n\n* Registers are a fast, limited storage area within the CPU for data used *right now*.\n* RAM is a larger but slower storage for data in *active use*.\n* Cache is a fast intermediary between the CPU and RAM for *frequently used data*.\n* TLB is a fast intermediary between the CPU and the page table for *frequently used address translations*. \n"
            }
        ]
    },
    {
        "doc_id": 465,
        "doc": {
            "question_id": -1,
            "question": "The BOOT sector files of the system are stored in ____",
            "options": [
                "Harddisk",
                "ROM",
                "RAM",
                "Fast solid state chips in the motherboard"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The BOOT sector files of the system are stored in ____",
                "Expected Label": "B",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe BOOT sector (or bootstrap sector) is a small program located at the beginning of a hard drive (or other bootable media). When a computer starts, the BIOS (Basic Input/Output System) reads the boot sector and executes the instructions within it to load the operating system. \n\nLet's look at why the other options are incorrect:\n\n* **B) ROM (Read-Only Memory):** ROM stores the BIOS itself, which is responsible for initially finding and loading the boot sector from the designated boot device.\n* **C) RAM (Random Access Memory):** RAM is volatile memory, meaning it loses its contents when the computer is powered off.  The boot process needs to access the boot sector even when the system is off.\n* **D) Fast solid state chips in the motherboard:** While some motherboards may have integrated storage, the boot sector is typically located on the primary hard drive or SSD designated as the boot device. \n"
            }
        ]
    },
    {
        "doc_id": 466,
        "doc": {
            "question_id": -1,
            "question": "The transfer of large chunks of data with the involvement of the processor is done by ____",
            "options": [
                "DMA controller",
                "Arbitrator",
                "User system programs",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The transfer of large chunks of data with the involvement of the processor is done by ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 467,
        "doc": {
            "question_id": -1,
            "question": "Which of the following techniques used to effectively utilize main memory?",
            "options": [
                "Address binding",
                "Dynamic linking",
                "Dynamic loading",
                "Both  Dynamic linking and loading"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Which of the following techniques used to effectively utilize main memory?",
                "Expected Label": "C",
                "Predicted Label": "**D) Both Dynamic linking and loading**\n\n**Explanation:**\n\n* **Dynamic Linking:** Delays the linking of external libraries until runtime. This means that code is loaded only when needed, reducing initial memory usage. \n* **Dynamic Loading:**  Allows parts of a program to be loaded into memory only when they are required. Unused parts remain on disk, conserving memory.\n\nAddress binding is related to mapping logical addresses to physical addresses, and while important for memory management, it doesn't directly focus on effective utilization in the same way dynamic linking and loading do. \n"
            }
        ]
    },
    {
        "doc_id": 468,
        "doc": {
            "question_id": -1,
            "question": "RTN stands for ____",
            "options": [
                "Register Transfer Notation",
                "Register Transmission Notation",
                "Regular Transmission Notation",
                "Regular Transfer Notation"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "RTN stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 469,
        "doc": {
            "question_id": -1,
            "question": "The instruction, Add Loc,R1 in RTN is ____",
            "options": [
                "AddSetCC Loc+R1",
                "R1=Loc+R1",
                "Not possible to write in RTN",
                "R1<-[Loc]+[R1]"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The instruction, Add Loc,R1 in RTN is ____",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 470,
        "doc": {
            "question_id": -1,
            "question": "Can you perform an addition on three operands simultaneously in ALN using Add instruction?",
            "options": [
                "Yes",
                "Not possible using Add, we\u2019ve to use AddSetCC",
                "Not permitted",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Can you perform an addition on three operands simultaneously in ALN using Add instruction?",
                "Expected Label": "C",
                "Predicted Label": "**C) Not permitted**\n\n**Explanation:**\n\nALN (Advanced Logic Network) typically refers to arithmetic logic units (ALUs) or similar components within a processor. Standard ALUs, and most instruction sets, are designed for binary operations, meaning they operate on **two** operands at a time.  Trying to add three operands simultaneously with a single instruction is generally not permitted in common architectures. \n"
            }
        ]
    },
    {
        "doc_id": 471,
        "doc": {
            "question_id": -1,
            "question": "The instruction, Add R1,R2,R3 in RTN is ____",
            "options": [
                "R3=R1+R2+R3",
                "R3<-[R1]+[R2]+[R3]",
                "R3=[R1]+[R2]",
                "R3<-[R1]+[R2]"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The instruction, Add R1,R2,R3 in RTN is ____",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 472,
        "doc": {
            "question_id": -1,
            "question": "In a system, which has 32 registers the register id is ____ long.",
            "options": [
                "16 bit",
                "8 bits",
                "5 bits",
                "6 bits"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a system, which has 32 registers the register id is ____ long.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 473,
        "doc": {
            "question_id": -1,
            "question": "The two phases of executing an instruction are ____",
            "options": [
                "Instruction decoding and storage",
                "Instruction fetch and instruction execution",
                "Instruction execution and storage",
                "Instruction fetch and Instruction processing"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The two phases of executing an instruction are ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 474,
        "doc": {
            "question_id": -1,
            "question": "The Instruction fetch phase ends with ____",
            "options": [
                "Placing the data from the address in MAR into MDR",
                "Placing the address of the data into MAR",
                "Completing the execution of the data and placing its storage address into MAR",
                "Decoding the data in MDR and placing it in IR"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The Instruction fetch phase ends with ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 475,
        "doc": {
            "question_id": -1,
            "question": "While using the iterative construct (Branching) in execution ____ instruction is used to check the condition.",
            "options": [
                "TestAndSet",
                "Branch",
                "TestCondn",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "While using the iterative construct (Branching) in execution ____ instruction is used to check the condition.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Branch instructions** are fundamental for implementing iterative constructs (loops). They evaluate a condition (often stored in status flags based on previous calculations) and redirect execution to a different part of the code (the loop's beginning) if the condition is true.\n\nLet's look at why the other options are incorrect:\n\n* **TestAndSet:** This is an atomic instruction primarily used for synchronization in multi-threading scenarios, not directly for loop condition checking.\n* **TestCondn:** This is not a standard assembly instruction.  \n* **None of the mentioned:**  This is incorrect as \"Branch\" is the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 476,
        "doc": {
            "question_id": -1,
            "question": "When using Branching, the usual sequencing of the PC is altered. A new instruction is loaded which is called as ____",
            "options": [
                "Branch target",
                "Loop target",
                "Forward target",
                "Jump instruction"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When using Branching, the usual sequencing of the PC is altered. A new instruction is loaded which is called as ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\nWhile A, B, and C relate to the *location* where the program counter (PC) might point to after a branch, the instruction that *causes* this change in the PC is specifically called a **Jump instruction**. \n\nBranch instructions are a subset of jump instructions, so \"Jump instruction\" is a more general and accurate term. \n"
            }
        ]
    },
    {
        "doc_id": 477,
        "doc": {
            "question_id": -1,
            "question": "The condition flag Z is set to 1 to indicate ____",
            "options": [
                "The operation has resulted in an error",
                "The operation requires an interrupt call",
                "The result is zero",
                "There is no empty register available"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The condition flag Z is set to 1 to indicate ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 478,
        "doc": {
            "question_id": -1,
            "question": "____ converts the programs written in assembly language into machine instructions.",
            "options": [
                "Machine compiler",
                "Interpreter",
                "Assembler",
                "Converter"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ converts the programs written in assembly language into machine instructions.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\n* **Assembler:** An assembler is specifically designed to translate assembly language code (which uses mnemonics) into machine code (binary instructions) that the computer's processor can understand. \n\nLet's look at why the other options are incorrect:\n\n* **Machine Compiler:** This term is not standard. Compilers typically translate high-level languages into assembly or machine code.\n* **Interpreter:** Interpreters execute code line by line, without directly translating it to machine code beforehand.\n* **Converter:** This is a generic term and doesn't specifically refer to the process of translating assembly language. \n"
            }
        ]
    },
    {
        "doc_id": 479,
        "doc": {
            "question_id": -1,
            "question": "The instructions like MOV or ADD are called as ____",
            "options": [
                "OP-Code",
                "Operators",
                "Commands",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The instructions like MOV or ADD are called as ____",
                "Expected Label": "A",
                "Predicted Label": "**B) Operators** \n\nLet me break down why:\n\n* **Operators** are symbols or words that represent specific operations to be performed on data.  MOV (move data) and ADD are classic examples of operators in assembly languages.\n\nLet's look at why the other options aren't the best fit:\n\n* **A) OP-Code (Operation Code):**  While closely related, an opcode is the *numerical representation* of an instruction that the processor understands. It's the machine code equivalent of the operator.\n* **C) Commands:** This term is more general and often used in the context of operating systems or high-level programming. \n* **D) None of the mentioned:**  This is incorrect since \"Operators\" is the right answer. \n"
            }
        ]
    },
    {
        "doc_id": 480,
        "doc": {
            "question_id": -1,
            "question": "The alternate way of writing the instruction, ADD #5,R1 is ____",
            "options": [
                "ADD [5],[R1];",
                "ADDI 5,R1;",
                "ADDIME 5,[R1];",
                "There is no other way"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The alternate way of writing the instruction, ADD #5,R1 is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **ADDI** is a common assembly language mnemonic specifically designed for adding an *immediate* value to a register.  Immediate values are constants directly encoded within the instruction.\n\nLet's break down why the other options are incorrect:\n\n* **A) ADD [5],[R1];**  -  This looks like it's trying to access memory addresses, not use an immediate value.\n* **C) ADDIME 5,[R1];** - This is not a standard assembly instruction.\n* **D) There is no other way** -  Assembly languages often provide flexibility in how instructions are written.\n\n**In summary, \"ADDI 5,R1\" is a clear and typical way to represent adding the immediate value 5 to register R1.** \n"
            }
        ]
    },
    {
        "doc_id": 481,
        "doc": {
            "question_id": -1,
            "question": "Instructions which won\u2019t appear in the object program are called as ____",
            "options": [
                "Redundant instructions",
                "Exceptions",
                "Comments",
                "Assembler Directives"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Instructions which won\u2019t appear in the object program are called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\nAssembler Directives are instructions specifically for the assembler and do not translate directly into machine code that appears in the object program. \n"
            }
        ]
    },
    {
        "doc_id": 482,
        "doc": {
            "question_id": -1,
            "question": "The assembler directive EQU, when used in the instruction: Sum EQU 200 does ____",
            "options": [
                "Finds the first occurrence of Sum and assigns value 200 to it",
                "Replaces every occurrence of Sum with 200",
                "Re-assigns the address of Sum by adding 200 to its original address",
                "Assigns 200 bytes of memory starting the location of Sum"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The assembler directive EQU, when used in the instruction: Sum EQU 200 does ____",
                "Expected Label": "B",
                "Predicted Label": "Answer: **B)** Replaces every occurrence of Sum with 200 \n\n**Explanation:**\n\nThe EQU directive is essentially a text substitution mechanism within the assembly code.  It doesn't directly manipulate memory addresses or search for occurrences. When you write `Sum EQU 200`, the assembler will replace every instance of the symbol \"Sum\" with the literal value \"200\" during the assembly process. \n"
            }
        ]
    },
    {
        "doc_id": 483,
        "doc": {
            "question_id": -1,
            "question": "The purpose of the ORIGIN directive is ____",
            "options": [
                "To indicate the starting position in memory, where the program block is to be stored",
                "To indicate the starting of the computation code",
                "To indicate the purpose of the code",
                "To list the locations of all the registers used"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The purpose of the ORIGIN directive is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 484,
        "doc": {
            "question_id": -1,
            "question": "The directive used to perform initialization before the execution of the code is ____",
            "options": [
                "Reserve",
                "Store",
                "Dataword",
                "EQU"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The directive used to perform initialization before the execution of the code is ____",
                "Expected Label": "C",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 485,
        "doc": {
            "question_id": -1,
            "question": "____ directive is used to specify and assign the memory required for the block of code.",
            "options": [
                "Allocate",
                "Assign",
                "Set",
                "Reserve"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ directive is used to specify and assign the memory required for the block of code.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 486,
        "doc": {
            "question_id": -1,
            "question": "____ directive specifies the end of execution of a program.",
            "options": [
                "End",
                "Return",
                "Stop",
                "Terminate"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ directive specifies the end of execution of a program.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 487,
        "doc": {
            "question_id": -1,
            "question": "The last statement of the source program should be ____",
            "options": [
                "Stop",
                "Return",
                "OP",
                "End"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The last statement of the source program should be ____",
                "Expected Label": "D",
                "Predicted Label": "**B) Return** \n\n**Explanation:**\n\nWhile options like \"Stop\" and \"End\" might seem intuitive, they often represent higher-level programming concepts. In the context of computer architecture and assembly language (which is highly relevant when discussing the \"last statement\"), the \"Return\" statement is most fitting:\n\n* **Return:** Signals the end of a subroutine or function and returns control to the point in the code where the subroutine was called. In many architectures, the last instruction of the main program is effectively a return to the operating system or a designated termination routine.\n\nLet me know if you'd like more details on why the other options are less suitable! \n"
            }
        ]
    },
    {
        "doc_id": 488,
        "doc": {
            "question_id": -1,
            "question": "When dealing with the branching code the assembler ____",
            "options": [
                "Replaces the target with its address",
                "Does not replace until the test condition is satisfied",
                "Finds the Branch offset and replaces the Branch target with it",
                "Replaces the target with the value specified by the DATAWORD directive"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "When dealing with the branching code the assembler ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Finds the Branch offset and replaces the Branch target with it**. Here's why:\n\n* **Branching and Offsets:** Branch instructions in assembly code don't usually contain the full memory address of their target. Instead, they use a *branch offset*. This offset is a relative value that tells the CPU how many instructions to jump forward or backward from the current instruction to reach the branch target.\n\n* **Assembler's Role:** During the assembly process, the assembler's job is to:\n    1. **Resolve Labels:**  Branch targets are often specified using labels (symbolic names) in assembly code. The assembler finds the memory addresses corresponding to these labels.\n    2. **Calculate Offsets:**  The assembler determines the difference between the address of the branch instruction and the address of the branch target. This difference is the branch offset.\n    3. **Encode the Offset:**  Finally, the assembler replaces the branch target label in the instruction with the calculated branch offset.\n\n**Let's illustrate with an example:**\n\n```assembly\n     LOOP:\n         ; some instructions\n         BRA LOOP   ; Branch to the instruction labeled \"LOOP\" \n```\n\nThe assembler will:\n\n1. Determine the memory address of the instruction labeled \"LOOP\".\n2. Calculate the difference between the address of the `BRA LOOP` instruction and the address of the \"LOOP\" instruction.\n3. Replace \"LOOP\" in the `BRA LOOP` instruction with the calculated offset. \n\nLet me know if you'd like more details on assembly language concepts! \n"
            }
        ]
    },
    {
        "doc_id": 489,
        "doc": {
            "question_id": -1,
            "question": "The assembler stores all the names and their corresponding values in ____",
            "options": [
                "Special purpose Register",
                "Symbol Table",
                "Value map Set",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The assembler stores all the names and their corresponding values in ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 490,
        "doc": {
            "question_id": -1,
            "question": "The assembler stores the object code in  ____",
            "options": [
                "Main memory",
                "Cache",
                "RAM",
                "Magnetic disk"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The assembler stores the object code in  ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 491,
        "doc": {
            "question_id": -1,
            "question": "The utility program used to bring the object code into memory for execution is ____",
            "options": [
                "Loader",
                "Fetcher",
                "Extractor",
                "Linker"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The utility program used to bring the object code into memory for execution is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 492,
        "doc": {
            "question_id": -1,
            "question": "To overcome the problems of the assembler in dealing with branching code we use ____",
            "options": [
                "Interpreter",
                "Debugger",
                "Op-Assembler",
                "Two-pass assembler"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "To overcome the problems of the assembler in dealing with branching code we use ____",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 493,
        "doc": {
            "question_id": -1,
            "question": "The return address of the Sub-routine is pointed to by ____",
            "options": [
                "IR",
                "PC",
                "MAR",
                "Special memory registers"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The return address of the Sub-routine is pointed to by ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) PC**. Here's why:\n\n* **PC (Program Counter):** The PC holds the address of the *next* instruction to be executed. When a subroutine call is made, the address of the instruction *after* the call instruction is stored in the PC. This ensures that upon subroutine completion, execution resumes at the correct point in the main program.\n\nLet's look at why the other options are incorrect:\n\n* **IR (Instruction Register):**  Holds the currently executing instruction.\n* **MAR (Memory Address Register):** Holds the address of the memory location being accessed.\n* **Special Memory Registers:** While some architectures might have specialized registers, the return address is fundamentally tied to the program flow and thus managed by the PC. \n\nLet me know if you have any other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 494,
        "doc": {
            "question_id": -1,
            "question": "The location to return to, from the subroutine is stored in ____",
            "options": [
                "TLB",
                "PC",
                "MAR",
                "Link registers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The location to return to, from the subroutine is stored in ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Link registers** are specifically designed to store the return address during a subroutine call. \n\nHere's why the other options are incorrect:\n\n* **TLB (Translation Lookaside Buffer):**  A cache that speeds up virtual address translation in memory management.\n* **PC (Program Counter):** Holds the address of the *next* instruction to be executed.\n* **MAR (Memory Address Register):** Holds the address of the memory location being accessed. \n"
            }
        ]
    },
    {
        "doc_id": 495,
        "doc": {
            "question_id": -1,
            "question": "What is subroutine nesting?",
            "options": [
                "Having multiple subroutines in a program",
                "Using a linking nest statement to put many subroutines under the same name",
                "Having one routine call the other",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "What is subroutine nesting?",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 496,
        "doc": {
            "question_id": -1,
            "question": "The order in which the return addresses are generated and used is ____",
            "options": [
                "LIFO",
                "FIFO",
                "Random",
                "Highest priority"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The order in which the return addresses are generated and used is ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) LIFO**. Here's why:\n\n* **LIFO (Last-In, First-Out)**: This accurately describes how return addresses work in function calls. \n    * When a function call happens, the return address (where the program should jump back to after the function) is pushed onto the call stack. \n    * When the function finishes, the return address is popped off the stack, and the program execution resumes from that point.  \n\nLet me know if you'd like more details on call stacks or function call mechanisms! \n"
            }
        ]
    },
    {
        "doc_id": 497,
        "doc": {
            "question_id": -1,
            "question": "In case of nested subroutines the return addresses are stored in ____",
            "options": [
                "System heap",
                "Special memory buffers",
                "Processor stack",
                "Registers"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In case of nested subroutines the return addresses are stored in ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nThe correct answer is **C) Processor stack**. Here's why:\n\n* **Processor Stack:** Stacks are designed for exactly this type of Last-In, First-Out (LIFO) operation. When a subroutine is called, the return address is pushed onto the stack. Upon return, the address is popped, ensuring the program resumes at the correct location. \n\nLet's look at why the other options are incorrect:\n\n* **A) System Heap:** The heap is used for dynamic memory allocation, not for storing return addresses.\n* **B) Special memory buffers:** While some systems might have specialized buffers, the general and widely used approach is the stack.\n* **D) Registers:** Registers are too few and have other primary purposes. Using them for nested subroutine return addresses would severely limit program complexity. \n"
            }
        ]
    },
    {
        "doc_id": 498,
        "doc": {
            "question_id": -1,
            "question": "The appropriate return addresses are obtained with the help of ____ in case of nested routines.",
            "options": [
                "MAR",
                "MDR",
                "Buffers",
                "Stack-pointers"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The appropriate return addresses are obtained with the help of ____ in case of nested routines.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 499,
        "doc": {
            "question_id": -1,
            "question": "When parameters are being passed on to the subroutines they are stored in ____",
            "options": [
                "Registers",
                "Memory locations",
                "Processor stacks",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When parameters are being passed on to the subroutines they are stored in ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 500,
        "doc": {
            "question_id": -1,
            "question": "The most efficient way of handling parameter passing is by using ____",
            "options": [
                "General purpose registers",
                "Stacks",
                "Memory locations",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The most efficient way of handling parameter passing is by using ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 501,
        "doc": {
            "question_id": -1,
            "question": "The most Flexible way of logging the return addresses of the subroutines is by using ____",
            "options": [
                "Registers",
                "Stacks",
                "Memory locations",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The most Flexible way of logging the return addresses of the subroutines is by using ____",
                "Expected Label": "B",
                "Predicted Label": "**B) Stacks**\n\nHere's why:\n\n* **Stacks and Subroutines:** Stacks are inherently designed for managing function calls and returns.  Here's how:\n    * **Push on Call:** When a subroutine is called, the return address (where execution should resume after the subroutine) is pushed onto the stack.\n    * **Pop on Return:** When the subroutine finishes, the return address is popped off the stack, and the processor jumps back to that location.\n\n* **Flexibility:** Stacks offer the most flexibility for these key reasons:\n    * **Nested Calls:**  Stacks allow for deeply nested subroutine calls. Each call pushes the return address, and they're returned in the correct order (Last-In, First-Out).\n    * **Recursion:** Stacks are essential for recursive functions, where a function calls itself. \n\nLet me know if you'd like more details on how stacks work in computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 502,
        "doc": {
            "question_id": -1,
            "question": "The wrong statement/s regarding interrupts and subroutines among the following is/are ____. \ni) The sub-routine and interrupts have a return statement. \nii) Both of them alter the content of the PC. \niii) Both are software oriented. \niv) Both can be initiated by the user.",
            "options": [
                "i, ii and iv",
                "ii and iii",
                "iv",
                "iii and iv"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The wrong statement/s regarding interrupts and subroutines among the following is/are ____. \ni) The sub-routine and interrupts have a return statement. \nii) Both of them alter the content of the PC. \niii) Both are software oriented. \niv) Both can be initiated by the user.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 503,
        "doc": {
            "question_id": -1,
            "question": "The private work space dedicated to a subroutine is called as ____",
            "options": [
                "System heap",
                "Reserve",
                "Stack frame",
                "Allocation"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The private work space dedicated to a subroutine is called as ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 504,
        "doc": {
            "question_id": -1,
            "question": "If the subroutine exceeds the private space allocated to it then the values are pushed onto ____",
            "options": [
                "Stack",
                "System heap",
                "Reserve Space",
                "Stack frame"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "If the subroutine exceeds the private space allocated to it then the values are pushed onto ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhen a subroutine (or function) is called, a stack frame is created on the stack to hold its local variables, parameters, and return address. If the subroutine requires more space than allocated in its stack frame (e.g., for dynamically allocated data), it typically pushes those values onto the stack. \n\nLet's break down why the other options are incorrect:\n\n* **B) System heap:** The heap is used for dynamic memory allocation that persists beyond the lifetime of a function call. While a subroutine might allocate memory on the heap, exceeding its private space doesn't automatically push values there.\n* **C) Reserve Space:** This term is too general. There isn't a specific \"Reserve Space\" designated for handling subroutine overflow.\n* **D) Stack frame:** The stack frame itself is a fixed size allocated at function call. While exceeding its space is the issue, the values aren't pushed \"onto\" the existing stack frame; they go beyond it on the stack. \n"
            }
        ]
    },
    {
        "doc_id": 505,
        "doc": {
            "question_id": -1,
            "question": "____ pointer is used to point to parameters passed or local parameters of the subroutine.",
            "options": [
                "Stack pointer",
                "Frame pointer",
                "Parameter register",
                "Log register"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ pointer is used to point to parameters passed or local parameters of the subroutine.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Frame pointer (or Base pointer)** is a register that holds a pointer to the current stack frame. The stack frame contains local variables, parameters passed to the subroutine, and return addresses.\n\nHere's why other options are incorrect:\n\n* **Stack pointer:** Points to the top of the stack, which constantly changes as data is pushed or popped.\n* **Parameter register:**  Some architectures use specific registers to pass a few parameters, but not for all parameters or local variables.\n* **Log register:**  Not related to subroutine parameter handling; typically used in specific hardware for logging events. \n"
            }
        ]
    },
    {
        "doc_id": 506,
        "doc": {
            "question_id": -1,
            "question": "The reserved memory or private space of the subroutine gets deallocated when ____",
            "options": [
                "The stop instruction is executed by the routine",
                "The pointer reaches the end of the space",
                "When the routine\u2019s return statement is executed",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The reserved memory or private space of the subroutine gets deallocated when ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C**.\n\n**Explanation:**\n\n* **A) The stop instruction is executed by the routine:**  This typically halts the entire program execution, not just deallocate a subroutine's memory.\n* **B) The pointer reaches the end of the space:**  While pointers manage memory access *within* a subroutine, they don't inherently deallocate the space when they reach the end. \n* **C) When the routine\u2019s return statement is executed:** This is the standard mechanism. When a subroutine finishes, the return statement signals the system to deallocate the stack frame associated with that subroutine, freeing up its reserved memory.\n* **D) None of the mentioned:**  This is incorrect, as option C accurately describes the process. \n\n**Important Note:**  Memory management can be more complex depending on the programming language, compiler, and whether the memory is allocated on the stack or heap. However, for the scope of this general question, option C is the most accurate. \n"
            }
        ]
    },
    {
        "doc_id": 507,
        "doc": {
            "question_id": -1,
            "question": "The private space gets allocated to each subroutine when ____",
            "options": [
                "The first statement of the routine is executed",
                "When the context switch takes place",
                "When the routine gets called",
                "When the Allocate instruction is executed"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The private space gets allocated to each subroutine when ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe private space, typically referring to the stack frame, is allocated **when the routine gets called**. Here's why:\n\n* **A) The first statement of the routine is executed:**  By this point, the routine needs its private space already set up to store local variables, parameters, and return addresses.\n* **B) When the context switch takes place:**  Context switching happens between processes, not individual subroutine calls within a process.\n* **D) When the Allocate instruction is executed:** While some architectures might have explicit allocation instructions, private space allocation for subroutines is a fundamental part of the calling convention and usually handled automatically upon call. \n\n**In summary:** The call instruction initiates the allocation of the stack frame (private space) to ensure the subroutine has its own isolated memory area for execution. \n"
            }
        ]
    },
    {
        "doc_id": 508,
        "doc": {
            "question_id": -1,
            "question": "____ the most suitable data structure used to store the return addresses in the case of nested subroutines.",
            "options": [
                "Heap",
                "Stack",
                "Queue",
                "List"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ the most suitable data structure used to store the return addresses in the case of nested subroutines.",
                "Expected Label": "B",
                "Predicted Label": "Answer: B \n\n**Explanation:**\n\n* **Stack** follows the Last-In-First-Out (LIFO) principle, which is ideal for nested subroutine calls. \n    * When a subroutine is called, its return address is pushed onto the stack. \n    * When the subroutine finishes, the return address is popped from the stack, ensuring the program resumes execution at the correct point. \n\nThe other options are not suitable:\n\n* **Heap:** Used for dynamic memory allocation and doesn't inherently support the LIFO order needed for return addresses.\n* **Queue:**  Uses First-In-First-Out (FIFO), not appropriate for the nested structure of subroutine calls.\n* **List:** A more general data structure and doesn't automatically enforce the specific order needed for return addresses. \n"
            }
        ]
    },
    {
        "doc_id": 509,
        "doc": {
            "question_id": -1,
            "question": "In the case of nested subroutines, the stack top is always ____",
            "options": [
                "The saved contents of the called sub routine",
                "The saved contents of the calling sub routine",
                "The return addresses of the called sub routine",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In the case of nested subroutines, the stack top is always ____",
                "Expected Label": "A",
                "Predicted Label": "**B) The saved contents of the calling subroutine**\n\n**Explanation:**\n\nWhen a subroutine (or function) is called, the current state of the calling subroutine needs to be preserved so execution can resume correctly after the called subroutine completes. This includes:\n\n* **Return Address:** Where execution should jump back to in the calling subroutine.\n* **Registers:** The values of registers used by the calling subroutine.\n* **Local Variables:**  Any data stored on the stack specific to the calling subroutine.\n\nThe stack is used to store this information, with the most recently pushed data residing on the \"top.\"  Therefore, at any point during the execution of nested subroutines, the stack top will contain the saved contents (return address, registers, local variables) of the **most recently called subroutine that hasn't yet returned**. This makes it possible to unwind the nested calls properly. \n"
            }
        ]
    },
    {
        "doc_id": 510,
        "doc": {
            "question_id": -1,
            "question": "The stack frame for each subroutine is present in ____",
            "options": [
                "Main memory",
                "System Heap",
                "Processor Stack",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The stack frame for each subroutine is present in ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe stack frame, which holds crucial data for a subroutine (like local variables, return addresses, and parameters), is specifically found on the **Processor Stack**. This stack is a region of memory managed in a Last-In, First-Out (LIFO) manner, perfectly suited for the nested function calls and returns common in programs. \n\nLet's eliminate the other options:\n\n* **A) Main memory:** While the stack frame resides within the broader main memory, this answer is too general.  \n* **B) System Heap:** The heap is used for dynamically allocated memory (like with `malloc` or `new`), not for the structured function call data of a stack frame.\n* **D) None of the mentioned:**  This is incorrect, as the Processor Stack is the dedicated location. \n"
            }
        ]
    },
    {
        "doc_id": 511,
        "doc": {
            "question_id": -1,
            "question": "The data structure suitable for scheduling processes is ____",
            "options": [
                "List",
                "Heap",
                "Queue",
                "Stack"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The data structure suitable for scheduling processes is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Queues** are the most suitable data structure for scheduling processes because they inherently operate on a First-In, First-Out (FIFO) principle. This aligns perfectly with common scheduling algorithms like First-Come, First-Served (FCFS) where the first process arriving in the ready queue is the first to get CPU time.\n\nLet's look at why the others aren't as suitable:\n\n* **Lists:** While lists offer flexibility, they don't inherently enforce the ordering needed for efficient process scheduling.\n* **Heaps:** Heaps are great for finding minimum or maximum values quickly (priority queues), but general process scheduling often doesn't solely rely on strict priority.\n* **Stacks:** Stacks use a Last-In, First-Out (LIFO) approach, which isn't suitable for fair process scheduling. \n"
            }
        ]
    },
    {
        "doc_id": 512,
        "doc": {
            "question_id": -1,
            "question": "The sub-routine service procedure is similar to that of the interrupt service routine in ____",
            "options": [
                "Method of context switch",
                "Returning",
                "Process execution",
                "Method of context switch & Process execution"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The sub-routine service procedure is similar to that of the interrupt service routine in ____",
                "Expected Label": "D",
                "Predicted Label": "**A) Method of context switch** \n\nBoth subroutine calls and interrupt service routines require saving the current processor state (context switching) before executing their respective code and restoring it afterward. \n"
            }
        ]
    },
    {
        "doc_id": 513,
        "doc": {
            "question_id": -1,
            "question": "In memory-mapped I/O ____",
            "options": [
                "The I/O devices and the memory share the same address space",
                "The I/O devices have a separate address space",
                "The memory and I/O devices have an associated address space",
                "A part of the memory is specifically set aside for the I/O operation"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In memory-mapped I/O ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 514,
        "doc": {
            "question_id": -1,
            "question": "The usual BUS structure used to connect the I/O devices is ____",
            "options": [
                "Star BUS structure",
                "Multiple BUS structure",
                "Single BUS structure",
                "Node to Node BUS structure"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The usual BUS structure used to connect the I/O devices is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nA **Single BUS structure** is the most common for connecting I/O devices. Here's why:\n\n* **Simplicity:** It's straightforward to design and implement.\n* **Cost-effective:**  Requires less wiring and fewer components compared to other structures.\n* **Flexibility:**  New devices can be added relatively easily. \n\n**While other options exist, they are less common for general I/O:**\n\n* **Star BUS:**  Common in networking but not typically for connecting I/O devices within a computer system.\n* **Multiple BUS:** Used in high-performance systems, but increases complexity and cost.\n* **Node to Node:**  Not a typical bus structure for I/O; more relevant to network topologies. \n"
            }
        ]
    },
    {
        "doc_id": 515,
        "doc": {
            "question_id": -1,
            "question": "The advantage of I/O mapped devices to memory mapped is ____",
            "options": [
                "The former offers faster transfer of data",
                "The devices connected using I/O mapping have a bigger buffer space",
                "The devices have to deal with fewer address lines",
                "No advantage as such"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The advantage of I/O mapped devices to memory mapped is ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 516,
        "doc": {
            "question_id": -1,
            "question": "The system is notified of a read or write operation by ____",
            "options": [
                "Appending an extra bit of the address",
                "Enabling the read or write bits of the devices",
                "Raising an appropriate interrupt signal",
                "Sending a special signal along the BUS"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The system is notified of a read or write operation by ____",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 517,
        "doc": {
            "question_id": -1,
            "question": "To overcome the lag in the operating speeds of the I/O device and the processor we use ____",
            "options": [
                "Buffer spaces",
                "Status flags",
                "Interrupt signals",
                "Exceptions"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To overcome the lag in the operating speeds of the I/O device and the processor we use ____",
                "Expected Label": "B",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Buffer spaces** act as temporary storage areas, allowing the processor and I/O devices to work at different speeds without holding each other back.  Data is transferred to and from the buffer, smoothing out the speed discrepancies.\n\nLet's look at why the other options are less suitable:\n\n* **Status flags** indicate the current state of a device or operation but don't directly address the speed difference.\n* **Interrupt signals** are used to get the processor's attention when an I/O operation is complete, but they don't solve the underlying speed issue.\n* **Exceptions** are triggered by errors or unusual conditions and are not a primary mechanism for handling speed differences. \n"
            }
        ]
    },
    {
        "doc_id": 518,
        "doc": {
            "question_id": -1,
            "question": "The method of accessing the I/O devices by repeatedly checking the status flags is ____",
            "options": [
                "Program-controlled I/O",
                "Memory-mapped I/O",
                "I/O mapped",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The method of accessing the I/O devices by repeatedly checking the status flags is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 519,
        "doc": {
            "question_id": -1,
            "question": "The method of synchronising the processor with the I/O device in which the device sends a signal when it is ready is?",
            "options": [
                "Exceptions",
                "Signal handling",
                "Interrupts",
                "DMA"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The method of synchronising the processor with the I/O device in which the device sends a signal when it is ready is?",
                "Expected Label": "C",
                "Predicted Label": "C"
            }
        ]
    },
    {
        "doc_id": 520,
        "doc": {
            "question_id": -1,
            "question": "The method which offers higher speeds of I/O transfers is ____",
            "options": [
                "Interrupts",
                "Memory mapping",
                "Program-controlled I/O",
                "DMA"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The method which offers higher speeds of I/O transfers is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 521,
        "doc": {
            "question_id": -1,
            "question": "The process wherein the processor constantly checks the status flags is called as ____",
            "options": [
                "Polling",
                "Inspection",
                "Reviewing",
                "Echoing"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The process wherein the processor constantly checks the status flags is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 522,
        "doc": {
            "question_id": -1,
            "question": "The interrupt-request line is a part of the ____",
            "options": [
                "Data line",
                "Control line",
                "Address line",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The interrupt-request line is a part of the ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Control lines** are responsible for managing how data flows and operations are executed between different components of a computer system. The interrupt-request line is a crucial control signal that allows devices to interrupt the processor's normal execution flow and request service.\n\nLet's look at why the other options are incorrect:\n\n* **Data lines** carry the actual data being processed.\n* **Address lines** specify the location in memory where data should be read from or written to. \n"
            }
        ]
    },
    {
        "doc_id": 523,
        "doc": {
            "question_id": -1,
            "question": "The return address from the interrupt-service routine is stored on the ____",
            "options": [
                "System heap",
                "Processor register",
                "Processor stack",
                "Memory"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The return address from the interrupt-service routine is stored on the ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe processor stack is the most common and efficient place to store the return address during an interrupt. Here's why:\n\n* **LIFO Structure:** Stacks operate on a Last-In, First-Out (LIFO) basis, which is ideal for interrupt handling. The last return address pushed onto the stack will be the first one popped off when the interrupt is complete.\n* **Speed:** Stack operations (push and pop) are very fast, crucial for minimizing interrupt latency.\n* **Context Preservation:** The stack is also used to save other processor registers during an interrupt, ensuring the interrupted process can resume correctly.\n\nLet me know if you'd like more detail on any aspect of interrupt handling! \n"
            }
        ]
    },
    {
        "doc_id": 524,
        "doc": {
            "question_id": -1,
            "question": "The signal sent to the device from the processor to the device after receiving an interrupt is ____",
            "options": [
                "Interrupt-acknowledge",
                "Return signal",
                "Service signal",
                "Permission signal"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The signal sent to the device from the processor to the device after receiving an interrupt is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Interrupt-acknowledge (INTA):** This is the signal sent by the processor to acknowledge an interrupt request from a device.  It essentially tells the device, \"I've received your request and will handle it soon.\"\n\nLet's break down why the other options are incorrect:\n\n* **Return signal:**  This refers to the signal at the end of an interrupt service routine (ISR), indicating the processor can return to its previous task.\n* **Service signal:** This is not a standard term in computer architecture.\n* **Permission signal:**  The processor doesn't request permission from the device after an interrupt. The interrupt itself is the device requesting the processor's attention. \n"
            }
        ]
    },
    {
        "doc_id": 525,
        "doc": {
            "question_id": -1,
            "question": "When the process is returned after an interrupt service ____ should be loaded again. \ni) Register contents. \nii) Condition codes. \niii) Stack contents. \niv) Return addresses.",
            "options": [
                "i, iv",
                "ii, iii and iv",
                "iii, iv",
                "i, ii"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When the process is returned after an interrupt service ____ should be loaded again. \ni) Register contents. \nii) Condition codes. \niii) Stack contents. \niv) Return addresses.",
                "Expected Label": "D",
                "Predicted Label": "**D) i, ii**\n\n**Explanation:**\n\nWhen an interrupt occurs, the current state of the process (including register contents and condition codes) needs to be saved so it can be resumed correctly. Upon returning from the interrupt service routine, this saved state, including register contents (i) and condition codes (ii), needs to be restored. \n\nWhile the stack (iii) and return address (iv) are crucial for interrupt handling, they are generally managed automatically by the hardware and don't need to be explicitly reloaded when returning from an interrupt. \n"
            }
        ]
    },
    {
        "doc_id": 526,
        "doc": {
            "question_id": -1,
            "question": "The time between the receiver of an interrupt and its service is ____",
            "options": [
                "Interrupt delay",
                "Interrupt latency",
                "Cycle time",
                "Switching time"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The time between the receiver of an interrupt and its service is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 527,
        "doc": {
            "question_id": -1,
            "question": "Interrupts form an important part of ____ systems.",
            "options": [
                "Batch processing",
                "Multitasking",
                "Real-time processing",
                "Multi-user"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Interrupts form an important part of ____ systems.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 528,
        "doc": {
            "question_id": -1,
            "question": "____ type circuits are generally used for interrupt service lines. \ni) open-collector \nii) open-drain \niii) XOR \niv) XNOR",
            "options": [
                "i, ii",
                "ii",
                "ii, iii",
                "ii, iv"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ type circuits are generally used for interrupt service lines. \ni) open-collector \nii) open-drain \niii) XOR \niv) XNOR",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 529,
        "doc": {
            "question_id": -1,
            "question": "The resistor which is attached to the service line is called ____",
            "options": [
                "Push-down resistor",
                "Pull-up resistor",
                "Break down resistor",
                "Line resistor"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The resistor which is attached to the service line is called ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Pull-up resistors** are commonly used on service lines (like communication buses) to ensure a known default state when the line is idle. \n"
            }
        ]
    },
    {
        "doc_id": 530,
        "doc": {
            "question_id": -1,
            "question": "An interrupt that can be temporarily ignored is ____",
            "options": [
                "Vectored interrupt",
                "Non-maskable interrupt",
                "Maskable interrupt",
                "High priority interrupt"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "An interrupt that can be temporarily ignored is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 531,
        "doc": {
            "question_id": -1,
            "question": "The 8085 microprocessor responds to the presence of an interrupt ____",
            "options": [
                "As soon as the trap pin becomes \u2018LOW\u2019",
                "By checking the trap pin for \u2018high\u2019 status at the end of each instruction fetch",
                "By checking the trap pin for \u2018high\u2019 status at the end of execution of each instruction",
                "By checking the trap pin for \u2018high\u2019 status at regular intervals"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The 8085 microprocessor responds to the presence of an interrupt ____",
                "Expected Label": "C",
                "Predicted Label": "C) By checking the trap pin for \u2018high\u2019 status at the end of execution of each instruction \n"
            }
        ]
    },
    {
        "doc_id": 532,
        "doc": {
            "question_id": -1,
            "question": "CPU as two modes privileged and non-privileged. In order to change the mode from privileged to non-privileged.",
            "options": [
                "A hardware interrupt is needed",
                "A software interrupt is needed",
                "Either hardware or software interrupt is needed",
                "A non-privileged instruction (which does not generate an interrupt)is needed"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "CPU as two modes privileged and non-privileged. In order to change the mode from privileged to non-privileged.",
                "Expected Label": "B",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 533,
        "doc": {
            "question_id": -1,
            "question": "Which interrupt is unmaskable?",
            "options": [
                "RST 5.5",
                "RST 7.5",
                "TRAP",
                "Both RST 5.5 and 7.5"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Which interrupt is unmaskable?",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 534,
        "doc": {
            "question_id": -1,
            "question": "From amongst the following given scenarios determine the right one to justify interrupt mode of data transfer. \ni) Bulk transfer of several kilo-byte. \nii) Moderately large data transfer of more than 1kb. \niii) Short events like mouse action. \niv) Keyboard inputs.",
            "options": [
                "i and ii",
                "ii",
                "i, ii and iv",
                "iv"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "From amongst the following given scenarios determine the right one to justify interrupt mode of data transfer. \ni) Bulk transfer of several kilo-byte. \nii) Moderately large data transfer of more than 1kb. \niii) Short events like mouse action. \niv) Keyboard inputs.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nInterrupts are best suited for handling short, asynchronous events that require immediate attention from the processor. \n\n* **Keyboard inputs (iv)** are a prime example of this. Each keypress is a discrete, unpredictable event.\n\nLet's look at why the other options aren't the best fit:\n\n* **Bulk transfers (i) and moderately large data transfers (ii)** are more efficiently handled by DMA (Direct Memory Access). DMA allows data to be transferred between devices and memory without constant CPU intervention. \n* **Mouse actions (iii),** while somewhat short, are often handled with a combination of interrupts and polling depending on the system's design.\n\n**Therefore, the most fitting scenario for interrupt-driven data transfer is keyboard inputs (iv).** \n"
            }
        ]
    },
    {
        "doc_id": 535,
        "doc": {
            "question_id": -1,
            "question": "How can the processor ignore other interrupts when it is servicing one ____",
            "options": [
                "By turning off the interrupt request line",
                "By disabling the devices from sending the interrupts",
                "BY using edge-triggered request lines",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "How can the processor ignore other interrupts when it is servicing one ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 536,
        "doc": {
            "question_id": -1,
            "question": "When dealing with multiple devices interrupts, which mechanism is easy to implement?",
            "options": [
                "Polling method",
                "Vectored interrupts",
                "Interrupt nesting",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When dealing with multiple devices interrupts, which mechanism is easy to implement?",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Polling method**. Here's why:\n\n* **Polling Method:**  The simplest method. The CPU periodically checks each device to see if it needs service. Easy to implement but inefficient, as the CPU wastes time checking devices even when no interrupts are pending. \n\n* **Vectored Interrupts:** More efficient than polling. Devices send a unique interrupt vector (address) to the CPU, telling it directly which interrupt service routine to execute. Requires hardware support and more complex setup than polling.\n\n* **Interrupt Nesting:**  Allows higher-priority interrupts to interrupt lower-priority ones. Adds complexity to the interrupt handling system.\n\n* **None of the mentioned:**  Incorrect, as polling is a valid (though not always ideal) method.\n\n**Therefore, while not the most efficient, polling is the easiest to implement.** \n"
            }
        ]
    },
    {
        "doc_id": 537,
        "doc": {
            "question_id": -1,
            "question": "The interrupt servicing mechanism in which the requesting device identifies itself to the processor to be serviced is ____",
            "options": [
                "Polling",
                "Vectored interrupts",
                "Interrupt nesting",
                "Simultaneous requesting"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The interrupt servicing mechanism in which the requesting device identifies itself to the processor to be serviced is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Vectored interrupts** allow the requesting device to send an interrupt vector (a unique identifier) to the processor. This vector points to a specific interrupt service routine (ISR) in memory, allowing for faster and more efficient interrupt handling.\n\nLet's analyze why the other options are incorrect:\n\n* **Polling:** The processor continuously checks the status of devices, which is inefficient.\n* **Interrupt nesting:** This refers to the ability to handle an interrupt while another is being serviced, not the identification mechanism.\n* **Simultaneous requesting:** While multiple devices can request interrupts, the identification mechanism is still typically vectored. \n"
            }
        ]
    },
    {
        "doc_id": 538,
        "doc": {
            "question_id": -1,
            "question": "In vectored interrupts, how does the device identify itself to the processor?",
            "options": [
                "By sending its device id",
                "By sending the machine code for the interrupt service routine",
                "By sending the starting address of the service routine",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In vectored interrupts, how does the device identify itself to the processor?",
                "Expected Label": "C",
                "Predicted Label": "C) By sending the starting address of the service routine \n"
            }
        ]
    },
    {
        "doc_id": 539,
        "doc": {
            "question_id": -1,
            "question": "The code sent by the device in vectored interrupt is ____ long.",
            "options": [
                "upto 16 bits",
                "upto 32 bits",
                "upto 24 bits",
                "4-8 bits"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The code sent by the device in vectored interrupt is ____ long.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) 4-8 bits**.\n\nHere's why:\n\n* **Vectored Interrupts:**  In a vectored interrupt system, the interrupting device doesn't just signal the CPU that it needs attention. It also sends a short code, the **interrupt vector**, that points to a specific memory location. This location holds the starting address of the interrupt service routine (ISR) for that particular device. \n\n* **Why 4-8 bits is optimal:**  A 4-8 bit code provides enough combinations to identify a reasonable number of different devices in a system. Using longer codes would be inefficient in terms of bandwidth and processing.\n\nLet me know if you would like a deeper explanation of interrupt systems! \n"
            }
        ]
    },
    {
        "doc_id": 540,
        "doc": {
            "question_id": -1,
            "question": "The starting address sent by the device in vectored interrupt is called as ____",
            "options": [
                "Location id",
                "Interrupt vector",
                "Service location",
                "Service id"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The starting address sent by the device in vectored interrupt is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Interrupt vector** is the correct term for the starting address sent by a device during a vectored interrupt. \n\nHere's why:\n\n* **Interrupt Vector:**  This is a memory location (or a table of locations) that holds the address of the interrupt service routine (ISR) for a specific device or type of interrupt. The device essentially tells the CPU, \"An interrupt happened, go to this address to find the code that handles it.\" \n\nLet's look at the other options:\n\n* **Location ID:** This term isn't commonly used in the context of interrupts.\n* **Service Location:** While it broadly describes where the ISR resides, it's not the specific term.\n* **Service ID:**  This might be used to identify the type of service requested within an interrupt, but not the starting address itself. \n"
            }
        ]
    },
    {
        "doc_id": 541,
        "doc": {
            "question_id": -1,
            "question": "The processor indicates to the devices that it is ready to receive interrupts ____",
            "options": [
                "By enabling the interrupt request line",
                "By enabling the IRQ bits",
                "By activating the interrupt acknowledge line",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The processor indicates to the devices that it is ready to receive interrupts ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Interrupt Acknowledge (INTA):** The processor asserts the INTA line to specifically acknowledge an interrupt request it has received. This is a key signal in the interrupt process, confirming to the interrupting device that the processor is aware of the request and is ready to handle it.\n\nLet's look at why the other options are incorrect:\n\n* **A) By enabling the interrupt request line:** The interrupt request line (IRQ) is used by devices to signal an interrupt *to* the processor, not the other way around.\n* **B) By enabling the IRQ bits:**  IRQ bits are typically within an interrupt controller. The processor enabling these bits allows the controller to forward interrupt requests, but it doesn't directly indicate the processor's readiness to receive them.\n* **D) None of the mentioned:** The INTA line is the standard mechanism for this communication. \n\nLet me know if you have any other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 542,
        "doc": {
            "question_id": -1,
            "question": "We describe a protocol of input device communication: \ni) Each device has a distinct address. \nii) The BUS controller scans each device in a sequence of increasing address value to determine if the entity wishes to communicate. \niii) The device ready to communicate leaves its data in the I/O register. \niv) The data is picked up and the controller moves to the step a. Identify the form of communication best describes the I/O mode amongst the following.",
            "options": [
                "Programmed mode of data transfer",
                "DMA",
                "Interrupt mode",
                "Polling"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "We describe a protocol of input device communication: \ni) Each device has a distinct address. \nii) The BUS controller scans each device in a sequence of increasing address value to determine if the entity wishes to communicate. \niii) The device ready to communicate leaves its data in the I/O register. \niv) The data is picked up and the controller moves to the step a. Identify the form of communication best describes the I/O mode amongst the following.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 543,
        "doc": {
            "question_id": -1,
            "question": "Which one of the following is true with regard to a CPU having a single interrupt request line and single interrupt grant line? \ni) Neither vectored nor multiple interrupting devices is possible. \nii) Vectored interrupts is not possible but multiple interrupting devices is possible. \niii) Vectored interrupts is possible and multiple interrupting devices is not possible. \niv) Both vectored and multiple interrupting devices are possible.",
            "options": [
                "iii",
                "i, iv",
                "ii, iii",
                "iii, iv"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which one of the following is true with regard to a CPU having a single interrupt request line and single interrupt grant line? \ni) Neither vectored nor multiple interrupting devices is possible. \nii) Vectored interrupts is not possible but multiple interrupting devices is possible. \niii) Vectored interrupts is possible and multiple interrupting devices is not possible. \niv) Both vectored and multiple interrupting devices are possible.",
                "Expected Label": "A",
                "Predicted Label": "C) ii, iii \n\n**Explanation:**\n\n* **Single Interrupt Request Line:** This means all devices share the same line to signal an interrupt to the CPU. This allows for **multiple interrupting devices** as they take turns using the line.\n\n* **Single Interrupt Grant Line:**  The CPU uses this single line to acknowledge an interrupt.  This makes **vectored interrupts difficult** because the CPU has no way to directly ask the interrupting device for its interrupt vector (a unique identifier for the device or its interrupt type).\n\n**Therefore:**\n\n* **ii) is possible:** Multiple devices can interrupt, but a polling or daisy-chaining mechanism would be needed to identify the interrupting device.\n* **iii) is not possible:** With a single grant line, the CPU cannot efficiently request a specific interrupt vector from the interrupting device.\n\nLet me know if you would like a deeper dive into any interrupt mechanism! \n"
            }
        ]
    },
    {
        "doc_id": 544,
        "doc": {
            "question_id": -1,
            "question": "Which table handle stores the addresses of the interrupt handling sub-routines?",
            "options": [
                "Interrupt-vector table",
                "Vector table",
                "Symbol link table",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which table handle stores the addresses of the interrupt handling sub-routines?",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Interrupt-vector table**. \n\nHere's why:\n\n* **Interrupt-vector table:** This table specifically holds the memory addresses of interrupt service routines (ISRs). When an interrupt occurs, the processor uses the interrupt number to index into this table and jump to the corresponding ISR.\n\n* **Vector table:** This is a more general term that often refers to the interrupt-vector table. In some architectures, the vector table might include other pointers besides interrupt handlers. \n\n* **Symbol link table:** This table is used during program linking and has no direct relationship to interrupt handling.\n\n* **None of the mentioned:**  This is incorrect, as the interrupt-vector table is the primary mechanism for storing addresses of interrupt handling subroutines. \n"
            }
        ]
    },
    {
        "doc_id": 545,
        "doc": {
            "question_id": -1,
            "question": "____ method is used to establish priority by serially connecting all devices that request an interrupt.",
            "options": [
                "Vectored-interrupting",
                "Daisy chain",
                "Priority",
                "Polling"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ method is used to establish priority by serially connecting all devices that request an interrupt.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 546,
        "doc": {
            "question_id": -1,
            "question": "In daisy chaining device 0 will pass the signal only if it has ____",
            "options": [
                "Interrupt request",
                "No interrupt request",
                "Both No interrupt and Interrupt request",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In daisy chaining device 0 will pass the signal only if it has ____",
                "Expected Label": "B",
                "Predicted Label": "A) Interrupt request \n\n**Explanation:**\n\nDaisy chaining prioritizes interrupts.  A device further down the chain only receives the interrupt acknowledge signal if no prior device in the chain has an interrupt request. \n"
            }
        ]
    },
    {
        "doc_id": 547,
        "doc": {
            "question_id": -1,
            "question": "____ interrupt method uses register whose bits are set separately by interrupt signal for each device.",
            "options": [
                "Parallel priority interrupt",
                "Serial priority interrupt",
                "Daisy chaining",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ interrupt method uses register whose bits are set separately by interrupt signal for each device.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 548,
        "doc": {
            "question_id": -1,
            "question": "____ register is used for the purpose of controlling the status of each interrupt request in parallel priority interrupt.",
            "options": [
                "Mass",
                "Mark",
                "Make",
                "Mask"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ register is used for the purpose of controlling the status of each interrupt request in parallel priority interrupt.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) Mask**. Here's why:\n\n* **Mask Register (also called Interrupt Mask Register):**  This register holds bits that correspond to different interrupt sources. \n    * A '1' in a specific bit position typically means the corresponding interrupt is *enabled* (can interrupt the processor).\n    * A '0' in a specific bit position typically means the corresponding interrupt is *masked* or *disabled* (cannot interrupt the processor).\n\n**Let's break down why the other options are incorrect:**\n\n* **A) Mass:**  This term has no specific meaning in the context of computer architecture and interrupts.\n* **B) Mark:**  While \"marking\" might sound related, it doesn't have a direct application in interrupt handling like a mask register does.\n* **C) Make:** Similar to \"mark,\" \"make\" isn't a term used for a register involved in interrupt control. \n\n**In summary:** The Mask Register is crucial in parallel priority interrupt systems because it allows the system to selectively enable or disable specific interrupts based on their priority and current system needs. \n"
            }
        ]
    },
    {
        "doc_id": 549,
        "doc": {
            "question_id": -1,
            "question": "The added output of the bits of the interrupt register and the mask register is set as an input of ____",
            "options": [
                "Priority decoder",
                "Priority encoder",
                "Process id encoder",
                "Multiplexer"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The added output of the bits of the interrupt register and the mask register is set as an input of ____",
                "Expected Label": "B",
                "Predicted Label": "**A) Priority decoder** \n\nHere's why:\n\n* **Interrupt Register:** Holds pending interrupt requests (each bit representing a different device).\n* **Mask Register:**  Controls which interrupts are allowed to interrupt the CPU (1 means allow, 0 means block).\n\nThe combination of these two registers (specifically a bitwise AND operation) determines which interrupts are both pending *and* allowed.  This resulting value is then fed into a **priority decoder**.  The priority decoder identifies the highest priority interrupt among the enabled ones and signals the CPU accordingly. \n\nLet me know if you'd like a deeper dive into any of these components! \n"
            }
        ]
    },
    {
        "doc_id": 550,
        "doc": {
            "question_id": -1,
            "question": "Interrupts initiated by an instruction is called as ____",
            "options": [
                "Internal",
                "External",
                "Hardware",
                "Software"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Interrupts initiated by an instruction is called as ____",
                "Expected Label": "B",
                "Predicted Label": "D \n\nSoftware interrupts are initiated by instructions, typically a system call, to request services from the operating system or other system software. \n"
            }
        ]
    },
    {
        "doc_id": 551,
        "doc": {
            "question_id": -1,
            "question": "If during the execution of an instruction an exception is raised then ____",
            "options": [
                "The instruction is executed and the exception is handled",
                "The instruction is halted and the exception is handled",
                "The processor completes the execution and saves the data and then handle the exception",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If during the execution of an instruction an exception is raised then ____",
                "Expected Label": "B",
                "Predicted Label": "**B) The instruction is halted and the exception is handled**\n\n**Explanation:**\n\nWhen an exception occurs, the current instruction's execution is typically **halted** (not completed). The processor then saves the current state (including program counter and registers) and jumps to a predefined exception handler routine to address the cause of the exception. \n"
            }
        ]
    },
    {
        "doc_id": 552,
        "doc": {
            "question_id": -1,
            "question": "____ is/are types of exceptions.",
            "options": [
                "Trap",
                "Interrupt",
                "System calls",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is/are types of exceptions.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 553,
        "doc": {
            "question_id": -1,
            "question": "The program used to find out errors is called ____",
            "options": [
                "Debugger",
                "Compiler",
                "Assembler",
                "Scanner"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The program used to find out errors is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 554,
        "doc": {
            "question_id": -1,
            "question": "The two facilities provided by the debugger is ____",
            "options": [
                "Trace points",
                "Break points",
                "Compile",
                "Both Trace and Break points"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The two facilities provided by the debugger is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 555,
        "doc": {
            "question_id": -1,
            "question": "In trace mode of operation is ____",
            "options": [
                "The program is interrupted after each detection",
                "The program will not be stopped and the errors are sorted out after the complete program is scanned",
                "There is no effect on the program, i.e the program is executed without rectification of errors",
                "The program is halted only at specific points"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In trace mode of operation is ____",
                "Expected Label": "A",
                "Predicted Label": "**B) The program will not be stopped and the errors are sorted out after the complete program is scanned** \n\nHere's why:\n\n* **Trace mode** is used for debugging. It allows you to monitor the execution of a program without constantly interrupting it.  \n* In trace mode, the program runs to completion (or a breakpoint), and information about its execution (like instructions executed, memory accesses, etc.) is logged. This log is then analyzed to find errors.\n\nLet me know if you'd like more clarification on computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 556,
        "doc": {
            "question_id": -1,
            "question": "What is the operation in Breakpoint mode?",
            "options": [
                "The program is interrupted after each detection",
                "The program will not be stopped and the errors are sorted out after the complete program is scanned",
                "There is no effect on the program, i.e the program is executed without rectification of errors",
                "The program is halted only at specific points"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "What is the operation in Breakpoint mode?",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 557,
        "doc": {
            "question_id": -1,
            "question": "What are the different modes of operation of a computer?",
            "options": [
                "User and System mode",
                "User and Supervisor mode",
                "Supervisor and Trace mode",
                "Supervisor, User and Trace mode"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "What are the different modes of operation of a computer?",
                "Expected Label": "B",
                "Predicted Label": "D) Supervisor, User and Trace mode \n"
            }
        ]
    },
    {
        "doc_id": 558,
        "doc": {
            "question_id": -1,
            "question": "The instructions which can be run only supervisor mode are?",
            "options": [
                "Non-privileged instructions",
                "System instructions",
                "Privileged instructions",
                "Exception instructions"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instructions which can be run only supervisor mode are?",
                "Expected Label": "C",
                "Predicted Label": "C. \n\n**Privileged instructions** are specifically designed to be executed only when the processor is in supervisor mode (also known as kernel mode or privileged mode). These instructions provide access to sensitive hardware resources and system-level operations that must be protected from user-level programs. \n"
            }
        ]
    },
    {
        "doc_id": 559,
        "doc": {
            "question_id": -1,
            "question": "A privilege exception is raised ____",
            "options": [
                "When a process tries to change the mode of the system",
                "When a process tries to change the priority level of the other processes",
                "When a process tries to access the memory allocated to other users",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A privilege exception is raised ____",
                "Expected Label": "D",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Privilege exceptions** are triggered when a process attempts an operation requiring higher privileges than it currently possesses.  \n\nLet's break down why the options are right or wrong:\n\n* **A) When a process tries to change the mode of the system:**  Changing the system mode (e.g., from user mode to kernel mode) absolutely requires high privileges and would trigger a privilege exception.\n* **B) When a process tries to change the priority level of other processes:** This is also a privileged operation. Altering the scheduling priority of other processes could destabilize the system.\n* **C) When a process tries to access the memory allocated to other users:** This is the most common scenario for a privilege exception. Memory protection is a core security feature, and processes should only access their own allocated memory space.\n* **D) All of the mentioned:**  Since A, B, and C all represent privileged operations, the answer is D. \n"
            }
        ]
    },
    {
        "doc_id": 560,
        "doc": {
            "question_id": -1,
            "question": "How is a privilege exception dealt with?",
            "options": [
                "The program is halted and the system switches into supervisor mode and restarts the program execution",
                "The Program is stopped and removed from the queue",
                "The system switches the mode and starts the execution of a new process",
                "The system switches mode and runs the debugger"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "How is a privilege exception dealt with?",
                "Expected Label": "A",
                "Predicted Label": "**D) The system switches mode and runs the debugger**\n\n**Explanation:**\n\nWhen a privilege exception occurs (e.g., a user program tries to execute a privileged instruction), the operating system:\n\n1. **Switches mode:** From user mode to kernel/supervisor mode.\n2. **Runs an exception handler:** This is often associated with a debugger in development environments. The handler allows examination of the exception's cause (illegal instruction, memory access violation, etc.) and potentially modify the program's state. \n\n**Why other options are incorrect:**\n\n* **A)** While the system switches to supervisor mode, it doesn't typically restart the program automatically. The outcome depends on the exception handler's logic.\n* **B)**  The program isn't necessarily removed from the queue. It might be paused or terminated depending on the severity and handling of the exception.\n* **C)**  A new process isn't started. The focus remains on handling the exception within the context of the existing process that caused it. \n"
            }
        ]
    },
    {
        "doc_id": 561,
        "doc": {
            "question_id": -1,
            "question": "The DMA differs from the interrupt mode by ____",
            "options": [
                "The involvement of the processor for the operation",
                "The method of accessing the I/O devices",
                "The amount of data transfer possible",
                "None of the mentioned"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The DMA differs from the interrupt mode by ____",
                "Expected Label": "D",
                "Predicted Label": "**A) The involvement of the processor for the operation**\n\n**Explanation:**\n\n* **DMA (Direct Memory Access)** allows peripherals to transfer data directly to/from memory without involving the CPU. This significantly speeds up data transfer for large amounts of data. \n\n* **Interrupt mode** requires the CPU to handle each data transfer. The peripheral sends an interrupt signal, the CPU stops its current task, handles the data transfer, and then resumes its previous task. \n\nTherefore, the key difference lies in the CPU's involvement in the data transfer operation. \n"
            }
        ]
    },
    {
        "doc_id": 562,
        "doc": {
            "question_id": -1,
            "question": "The DMA transfers are performed by a control circuit called as ____",
            "options": [
                "Device interface",
                "DMA controller",
                "Data controller",
                "Overlooker"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The DMA transfers are performed by a control circuit called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 563,
        "doc": {
            "question_id": -1,
            "question": "In DMA transfers, the required signals and addresses are given by the ____",
            "options": [
                "Processor",
                "Device drivers",
                "DMA controllers",
                "The program itself"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In DMA transfers, the required signals and addresses are given by the ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nDMA controllers are specifically designed to manage data transfers between memory and peripherals *without* direct intervention from the processor. They handle the necessary signaling and address calculations, freeing up the processor for other tasks. \n"
            }
        ]
    },
    {
        "doc_id": 564,
        "doc": {
            "question_id": -1,
            "question": "After the completion of the DMA transfer, the processor is notified by ____",
            "options": [
                "Acknowledge signal",
                "Interrupt signal",
                "WMFC signal",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "After the completion of the DMA transfer, the processor is notified by ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 565,
        "doc": {
            "question_id": -1,
            "question": "The DMA controller has ____ registers.",
            "options": [
                "4",
                "2",
                "3",
                "1"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The DMA controller has ____ registers.",
                "Expected Label": "C",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 566,
        "doc": {
            "question_id": -1,
            "question": "When the R/W bit of the status register of the DMA controller is set to 1.",
            "options": [
                "Read operation is performed",
                "Write operation is performed",
                "Read & Write operation is performed",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When the R/W bit of the status register of the DMA controller is set to 1.",
                "Expected Label": "A",
                "Predicted Label": "**A) Read operation is performed** \n\n**Explanation:**\n\nThe R/W (Read/Write) bit in a DMA controller's status register specifically dictates the direction of data transfer:\n\n* **R/W = 1:** Indicates a **Read** operation. The DMA controller will read data from the memory and transfer it to the specified device.\n* **R/W = 0:** Indicates a **Write** operation. The DMA controller will write data from the device to the memory. \n"
            }
        ]
    },
    {
        "doc_id": 567,
        "doc": {
            "question_id": -1,
            "question": "The controller is connected to the ____",
            "options": [
                "Processor BUS",
                "System BUS",
                "External BUS",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The controller is connected to the ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 568,
        "doc": {
            "question_id": -1,
            "question": "The technique whereby the DMA controller steals the access cycles of the processor to operate is called ____",
            "options": [
                "Fast conning",
                "Memory Con",
                "Cycle stealing",
                "Memory stealing"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The technique whereby the DMA controller steals the access cycles of the processor to operate is called ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 569,
        "doc": {
            "question_id": -1,
            "question": "The technique where the controller is given complete access to main memory is ____",
            "options": [
                "Cycle stealing",
                "Memory stealing",
                "Memory Con",
                "Burst mode"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The technique where the controller is given complete access to main memory is ____",
                "Expected Label": "D",
                "Predicted Label": "**D) Burst mode** \n\n**Explanation:**\n\n* **Burst mode** allows the controller (like a DMA controller) to access main memory for an extended period, transferring a block of data without interruption. \n\nLet's look at why the other options are incorrect:\n\n* **Cycle stealing:** This is when a controller temporarily borrows bus cycles from the CPU to access memory. It doesn't have \"complete\" access.\n* **Memory stealing:** This is not a standard term in computer architecture.\n* **Memory Con:** This is not a relevant term in this context. \n"
            }
        ]
    },
    {
        "doc_id": 570,
        "doc": {
            "question_id": -1,
            "question": "The controller uses ____ to help with the transfers when handling network interfaces.",
            "options": [
                "Input Buffer storage",
                "Signal enhancers",
                "Bridge circuits",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The controller uses ____ to help with the transfers when handling network interfaces.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Input Buffer storage**. Here's why:\n\n* **Input Buffer Storage:** Network interfaces deal with bursts of data. Input buffers are crucial for temporarily storing incoming data from the network before the controller can process it. This prevents data loss and allows the controller to manage data flow efficiently.\n\nLet's look at why the other options are incorrect:\n\n* **Signal Enhancers:** While signal strength is important in networking, it's handled by components like repeaters and amplifiers, not directly by the controller.\n* **Bridge Circuits:** Bridges are networking devices used to connect different network segments. They are not directly involved in the controller's data transfer mechanisms. \n\n**Therefore, input buffer storage is the most accurate answer to how a controller manages network interface transfers.** \n"
            }
        ]
    },
    {
        "doc_id": 571,
        "doc": {
            "question_id": -1,
            "question": "To overcome the conflict over the possession of the BUS we use ____",
            "options": [
                "Optimizers",
                "BUS arbitrators",
                "Multiple BUS structure",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To overcome the conflict over the possession of the BUS we use ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 572,
        "doc": {
            "question_id": -1,
            "question": "The registers of the controller are ____",
            "options": [
                "64 bits",
                "24 bits",
                "32 bits",
                "16 bits"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The registers of the controller are ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **not determinable from the information given**. Here's why:\n\n* **Controller register size is not fixed:** The size of registers within a controller is highly dependent on the specific architecture of the controller itself. There's no universal standard size. \n* **Controllers vary widely:**  Controllers exist in CPUs, hard drives, network cards, and countless other devices. Each will have unique design requirements.\n\n**To illustrate:**\n\n* A simple microcontroller might use 8-bit or 16-bit registers for cost and efficiency.\n* A complex CPU might use 32-bit or 64-bit registers (or even wider) for performance reasons.\n\n**Therefore, none of the provided options (A, B, C, or D) can be definitively claimed as the correct answer.** \n"
            }
        ]
    },
    {
        "doc_id": 573,
        "doc": {
            "question_id": -1,
            "question": "When the process requests for a DMA transfer?",
            "options": [
                "Then the process is temporarily suspended",
                "The process continues execution",
                "Another process gets executed",
                "process is temporarily suspended & Another process gets executed"
            ],
            "answer": "D",
            "difficulty": "MEDIUM",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When the process requests for a DMA transfer?",
                "Expected Label": "D",
                "Predicted Label": "B) The process continues execution \n\n**Explanation:**\n\nDMA (Direct Memory Access) is designed to offload data transfer tasks from the CPU.  Here's why:\n\n* **DMA's Purpose:** The whole point of DMA is to let the CPU continue with other work while data transfer happens in the background. \n* **How it Works:** The CPU initiates the DMA transfer, providing the DMA controller with the source and destination addresses and the amount of data. The DMA controller then handles the transfer independently, directly accessing memory.\n* **Efficiency:** If the CPU had to pause for each data transfer, it would create a significant bottleneck. DMA allows for much more efficient system utilization. \n\nLet me know if you'd like more details about DMA! \n"
            }
        ]
    },
    {
        "doc_id": 574,
        "doc": {
            "question_id": -1,
            "question": "The DMA transfer is initiated by ____",
            "options": [
                "Processor",
                "The process being executed",
                "I/O devices",
                "OS"
            ],
            "answer": "C",
            "difficulty": "MEDIUM",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The DMA transfer is initiated by ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nDirect Memory Access (DMA) is designed to offload data transfer tasks from the CPU.  It's the **I/O devices** themselves that initiate the DMA transfer. They signal the DMA controller when they need to send or receive data from memory. \n"
            }
        ]
    },
    {
        "doc_id": 575,
        "doc": {
            "question_id": -1,
            "question": "To resolve the clash over the access of the system BUS we use ____",
            "options": [
                "Multiple BUS",
                "BUS arbitrator",
                "Priority access",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To resolve the clash over the access of the system BUS we use ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 576,
        "doc": {
            "question_id": -1,
            "question": "The device which is allowed to initiate data transfers on the BUS at any time is called ____",
            "options": [
                "BUS master",
                "Processor",
                "BUS arbitrator",
                "Controller"
            ],
            "answer": "A",
            "difficulty": "MEDIUM",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The device which is allowed to initiate data transfers on the BUS at any time is called ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 577,
        "doc": {
            "question_id": -1,
            "question": "____ BUS arbitration approach uses the involvement of the processor.",
            "options": [
                "Centralised arbitration",
                "Distributed arbitration",
                "Random arbitration",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ BUS arbitration approach uses the involvement of the processor.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Centralized Arbitration:** A dedicated hardware unit (usually part of the processor or memory controller) acts as the bus arbiter, determining which device gets access to the bus.  \n\n* **Distributed Arbitration:**  Each device on the bus has its own logic to determine if it can access the bus. This doesn't involve the processor directly.\n\n* **Random Arbitration:**  A less structured method (like a lottery system) is used, and the processor isn't inherently involved in the arbitration process. \n\nLet me know if you'd like more detail on bus arbitration! \n"
            }
        ]
    },
    {
        "doc_id": 578,
        "doc": {
            "question_id": -1,
            "question": "The circuit used for the request line is a ____",
            "options": [
                "Open-collector",
                "EX-OR circuit",
                "Open-drain",
                "Nand circuit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The circuit used for the request line is a ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Open-drain** circuits are commonly used for request lines in computer architecture.  They allow multiple devices to share the same line. When a device wants to signal a request, it pulls the line low. If no device is requesting, the line remains high due to a pull-up resistor.\n\nHere's why the other options are less suitable:\n\n* **Open-collector** is similar to open-drain but uses a transistor with a collector output. While conceptually similar, open-drain is more prevalent in modern systems, especially those using MOSFET technology.\n* **EX-OR circuits** are used for comparison and parity checking, not typically for request lines.\n* **Nand circuits** are fundamental logic gates but not ideal for shared request lines due to their output behavior when multiple inputs are active. \n"
            }
        ]
    },
    {
        "doc_id": 579,
        "doc": {
            "question_id": -1,
            "question": "The Centralised BUS arbitration is similar to ____ interrupt circuit.",
            "options": [
                "Priority",
                "Parallel",
                "Single",
                "Daisy chain"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The Centralised BUS arbitration is similar to ____ interrupt circuit.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **A) Priority**. Here's why:\n\n* **Centralized Bus Arbitration** involves a single arbiter (like a bus controller) that determines which device gets access to the bus based on a priority scheme.  \n* **Priority Interrupts** operate similarly. When multiple devices interrupt the processor, the interrupt controller prioritizes which interrupt gets serviced first.\n\n**Let's look at why the other options are incorrect:**\n\n* **Parallel Interrupts** imply multiple interrupt lines, which isn't directly related to centralized bus arbitration.\n* **Single Interrupts** don't reflect the priority-based decision-making in centralized arbitration.\n* **Daisy Chain Interrupts** use a chain of devices to propagate an interrupt signal. While this involves prioritization, it's a distributed approach, unlike the centralized nature of bus arbitration. \n"
            }
        ]
    },
    {
        "doc_id": 580,
        "doc": {
            "question_id": -1,
            "question": "When the processor receives the request from a device, it responds by sending ____",
            "options": [
                "Acknowledge signal",
                "BUS grant signal",
                "Response signal",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When the processor receives the request from a device, it responds by sending ____",
                "Expected Label": "B",
                "Predicted Label": "A \n\n**Explanation:**\n\nAn **Acknowledge (ACK)** signal is a standard signal used in digital communication to confirm the successful receipt of data or a request. When a device makes a request to the processor (like an interrupt request), the processor, upon receiving and recognizing this request, sends back an ACK signal to acknowledge the device. \n\nLet's look at why the other options are incorrect:\n\n* **BUS grant signal:** This signal is sent by a bus arbiter (often the processor itself) to grant a specific device permission to use the bus for data transfer. It's not a direct response to a device's request.\n* **Response signal:** This is too general. While an ACK is a type of response, it's specifically for acknowledging requests.\n* **None of the mentioned:**  An ACK signal is a fundamental concept in computer architecture and is used for device communication. \n\n**Therefore, the most accurate answer is A.** \n"
            }
        ]
    },
    {
        "doc_id": 581,
        "doc": {
            "question_id": -1,
            "question": "In Centralised Arbitration ____ is/are is the BUS master.",
            "options": [
                "Processor",
                "DMA controller",
                "Device",
                "Both Processor and DMA controller"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In Centralised Arbitration ____ is/are is the BUS master.",
                "Expected Label": "D",
                "Predicted Label": "B \n\n**Explanation:**\n\nIn centralized arbitration, a dedicated **DMA controller** typically acts as the bus master. It manages bus access requests from different devices and grants access based on a predefined priority scheme. \n\nLet me know if you'd like more details on bus arbitration! \n"
            }
        ]
    },
    {
        "doc_id": 582,
        "doc": {
            "question_id": -1,
            "question": "Once the BUS is granted to a device ____",
            "options": [
                "It activates the BUS busy line",
                "Performs the required operation",
                "Raises an interrupt",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Once the BUS is granted to a device ____",
                "Expected Label": "A",
                "Predicted Label": "B) Performs the required operation \n\n**Explanation:**\n\nHere's why:\n\n* **A) It activates the BUS busy line:** While some systems might use a busy line, it's not the universal action taken once a device is granted bus access.\n* **B) Performs the required operation:** This is the core purpose. When a device gets the bus, it's granted permission to read/write data or send/receive signals.\n* **C) Raises an interrupt:** Interrupts are for signaling the CPU, usually *after* an operation is complete or an event needs attention. They aren't the direct consequence of getting bus access.\n* **D) All of the mentioned:** This is incorrect as not all options are accurate. \n\n**In summary:** The primary function of bus granting is to allow a device to perform its intended operation on the bus. \n"
            }
        ]
    },
    {
        "doc_id": 583,
        "doc": {
            "question_id": -1,
            "question": "The BUS busy line is made of ____",
            "options": [
                "Open-drain circuit",
                "Open-collector circuit",
                "EX-Or circuit",
                "Nor circuit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The BUS busy line is made of ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 584,
        "doc": {
            "question_id": -1,
            "question": "After the device completes its operation ____ assumes the control of the BUS.",
            "options": [
                "Another device",
                "Processor",
                "Controller",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "After the device completes its operation ____ assumes the control of the BUS.",
                "Expected Label": "B",
                "Predicted Label": "**B) Processor**\n\n**Explanation:**\n\nThe processor is the primary component that controls the bus. When a device completes its operation, the bus control is typically released back to the processor, allowing it to initiate the next bus transaction or transfer control to another device as needed. \n"
            }
        ]
    },
    {
        "doc_id": 585,
        "doc": {
            "question_id": -1,
            "question": "The BUS busy line is used ____",
            "options": [
                "To indicate the processor is busy",
                "To indicate that the BUS master is busy",
                "To indicate the BUS is already allocated",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The BUS busy line is used ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) To indicate the BUS is already allocated**. Here's why:\n\n* **BUS Busy Line:** This line acts as a signal in a computer system's bus architecture. When a device wants to use the bus (to communicate with memory or other devices), it asserts the busy line.  If the line is already asserted, it means another device is currently using the bus.\n\nLet's look at why the other options are incorrect:\n\n* **A) To indicate the processor is busy:** While a busy processor might be using the bus, the busy line itself doesn't directly reflect the processor's state. Other devices use the bus too.\n* **B) To indicate that the BUS master is busy:** Similar to option A, a busy bus master might be using the bus, but the busy line is a more general indication of bus usage.\n* **D) None of the mentioned:**  The BUS busy line has a specific purpose, which is to signal when the bus is in use. \n\n**In summary, the BUS busy line is a crucial part of bus arbitration, ensuring that only one device can transmit data on the bus at any given time.** \n"
            }
        ]
    },
    {
        "doc_id": 586,
        "doc": {
            "question_id": -1,
            "question": "Distributed arbitration makes use of ____",
            "options": [
                "BUS master",
                "Processor",
                "Arbitrator",
                "4-bit ID"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Distributed arbitration makes use of ____",
                "Expected Label": "D",
                "Predicted Label": "**D) 4-bit ID**\n\nDistributed arbitration schemes often use unique identifiers (like a 4-bit ID) for each device on the bus to determine priority and grant access. \n\nLet me know if you'd like more explanation on distributed arbitration! \n"
            }
        ]
    },
    {
        "doc_id": 587,
        "doc": {
            "question_id": -1,
            "question": "In Distributed arbitration, the device requesting the BUS ____",
            "options": [
                "Asserts the Start arbitration signal",
                "Sends an interrupt signal",
                "Sends an acknowledge signal",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In Distributed arbitration, the device requesting the BUS ____",
                "Expected Label": "A",
                "Predicted Label": "**D) None of the mentioned**\n\nIn distributed arbitration, there's no central arbiter. Instead,  devices on the bus use a shared protocol to determine which device gets access. This often involves mechanisms like:\n\n* **Self-identification:** Each device has a unique ID.\n* **Priority encoding:** Devices might have pre-assigned priorities.\n* **Token passing:** A token grants bus access, and it's passed between devices. \n\nLet me know if you'd like to know more about distributed arbitration! \n"
            }
        ]
    },
    {
        "doc_id": 588,
        "doc": {
            "question_id": -1,
            "question": "How is a device selected in Distributed arbitration?",
            "options": [
                "By NANDing the signals passed on all the 4 lines",
                "By ANDing the signals passed on all the 4 lines",
                "By ORing the signals passed on all the 4 lines",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "How is a device selected in Distributed arbitration?",
                "Expected Label": "C",
                "Predicted Label": "**D) None of the mentioned**\n\n**Explanation:**\n\nDistributed arbitration doesn't use simple logic gates like AND, OR, or NAND for device selection.  Here's why:\n\n* **Decentralized Control:** In distributed arbitration, the control logic for granting device access is spread across multiple bus participants, not centralized in a single unit.\n* **Request and Acknowledgement:** Devices typically assert a request signal when they need the bus. An arbitration scheme (often involving priority levels or a fairness mechanism) determines which device gets access. The granted device receives an acknowledgment signal. \n\nLet me know if you'd like more details on how specific distributed arbitration methods work! \n"
            }
        ]
    },
    {
        "doc_id": 589,
        "doc": {
            "question_id": -1,
            "question": "If two devices A and B contesting for the BUS have ID\u2019s 5 and 6 respectively, which device gets the BUS based on the Distributed arbitration.",
            "options": [
                "Device A",
                "Device B",
                "Insufficient information",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If two devices A and B contesting for the BUS have ID\u2019s 5 and 6 respectively, which device gets the BUS based on the Distributed arbitration.",
                "Expected Label": "B",
                "Predicted Label": "**B) Device B**\n\n**Explanation:**\n\nIn distributed arbitration, each device on the bus has a unique priority. Typically, a lower ID number corresponds to a higher priority.  Since device B has an ID of 6, which is lower than device A's ID of 5, device B would have higher priority and win the bus contention. \n"
            }
        ]
    },
    {
        "doc_id": 590,
        "doc": {
            "question_id": -1,
            "question": "The master indicates that the address is loaded onto the BUS, by activating ____ signal.",
            "options": [
                "MSYN",
                "SSYN",
                "WMFC",
                "INTR"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The master indicates that the address is loaded onto the BUS, by activating ____ signal.",
                "Expected Label": "A",
                "Predicted Label": "**B) SSYN**\n\n**Explanation:**\n\n* **SSYN (Slave Sync):** This signal is used in asynchronous bus communication. When the master places the address on the bus, it asserts the SSYN signal. This signals to the slave devices that the address is valid and they should check if it matches their own address.\n\nLet's look at why the other options are incorrect:\n\n* **MSYN (Master Sync):**  While used in asynchronous communication, MSYN is asserted by the *slave* to indicate it has responded to the master's request. \n* **WMFC (Wait for Memory Function Complete):** This is related to memory operations and indicates that the processor needs to wait for a memory operation to finish.\n* **INTR (Interrupt Request):** This signal is used by devices to interrupt the processor and request service.\n\n**Therefore, the correct answer is B.** \n"
            }
        ]
    },
    {
        "doc_id": 591,
        "doc": {
            "question_id": -1,
            "question": "The MSYN signal is initiated ____",
            "options": [
                "Soon after the address and commands are loaded",
                "Soon after the decoding of the address",
                "After the slave gets the commands",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The MSYN signal is initiated ____",
                "Expected Label": "B",
                "Predicted Label": "**C) After the slave gets the commands**\n\nHere's why:\n\n* **MSYN (Master SYNchronization)** signals are used in asynchronous communication protocols, particularly in memory systems. \n* The master initiates a transaction with address and command signals.\n* The slave, upon receiving and interpreting these commands, needs to indicate it's ready for the next stage of the transaction (e.g., data transfer). This \"ready\" signal from the slave is the MSYN.\n\n**Let's eliminate the other options:**\n\n* **A) Soon after the address and commands are loaded:** This is too early. The slave needs time to process the commands.\n* **B) Soon after the decoding of the address:** Similar to option A, the slave still needs time to act upon the decoded address and commands.\n* **D) None of the mentioned:**  Option C accurately describes the timing of the MSYN signal. \n\n**Therefore, C is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 592,
        "doc": {
            "question_id": -1,
            "question": "In IBM\u2019s S360/370 systems ____ lines are used to select the I/O devices.",
            "options": [
                "SCAN in and out",
                "Connect",
                "Search",
                "Peripheral"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In IBM\u2019s S360/370 systems ____ lines are used to select the I/O devices.",
                "Expected Label": "A",
                "Predicted Label": "D) Peripheral \n"
            }
        ]
    },
    {
        "doc_id": 593,
        "doc": {
            "question_id": -1,
            "question": "The meter in and out lines are used for ____",
            "options": [
                "Monitoring the usage of devices",
                "Monitoring the amount of data transferred",
                "Measure the CPU usage",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The meter in and out lines are used for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:** Meter in and out lines are typically used in hardware design, particularly in networking equipment, to monitor the usage of devices or resources. While they might indirectly relate to data transfer, their primary function is to track device utilization. \n"
            }
        ]
    },
    {
        "doc_id": 594,
        "doc": {
            "question_id": -1,
            "question": "MRDC stands for ____",
            "options": [
                "Memory Read Enable",
                "Memory Ready Command",
                "Memory Re-direct Command",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "MRDC stands for ____",
                "Expected Label": "B",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 595,
        "doc": {
            "question_id": -1,
            "question": "The BUS that allows I/O, memory and Processor to coexist is ____",
            "options": [
                "Attributed BUS",
                "Processor BUS",
                "Backplane BUS",
                "External BUS"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The BUS that allows I/O, memory and Processor to coexist is ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Backplane BUS**. Here's why:\n\n* **Backplane BUS** serves as the central communication highway within a computer system. It connects the processor, memory, and various I/O devices, allowing them to communicate and exchange data.\n\nLet's look at why the other options aren't the best fit:\n\n* **Attributed BUS:**  This term isn't commonly used in computer architecture.\n* **Processor BUS:** This typically refers to the bus connecting the CPU to its immediate components like the Northbridge and Southbridge chipsets (in older architectures) or directly to RAM. \n* **External BUS:** While this term can refer to buses connecting external devices, it's too general. The backplane bus is specifically designed for the core internal components. \n\n**In summary:** The backplane bus acts as the backbone for communication between the processor, memory, and I/O, making it the most accurate answer. \n"
            }
        ]
    },
    {
        "doc_id": 596,
        "doc": {
            "question_id": -1,
            "question": "The transmission on the asynchronous BUS is also called ____",
            "options": [
                "Switch mode transmission",
                "Variable transfer",
                "Bulk transfer",
                "Hand-Shake transmission"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The transmission on the asynchronous BUS is also called ____",
                "Expected Label": "D",
                "Predicted Label": "D) Hand-Shake transmission \n\n**Explanation:**\n\nAsynchronous buses rely on a \"handshaking\" protocol for data transfer. This means the sender and receiver coordinate the transfer with control signals (like request and acknowledge) to ensure data integrity despite variations in timing. \n"
            }
        ]
    },
    {
        "doc_id": 597,
        "doc": {
            "question_id": -1,
            "question": "____ serves as an intermediary between the device and the BUSes.",
            "options": [
                "Interface circuits",
                "Device drivers",
                "Buffers",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ serves as an intermediary between the device and the BUSes.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Interface circuits:** These circuits handle the communication protocols and signal conversions necessary for a device to interact with the system bus. They act as the bridge between the device's specific requirements and the standardized bus signals.\n\nLet's look at why the other options are less suitable:\n\n* **Device drivers:** These are software components that allow the operating system to interact with a device. They operate at a higher level than interface circuits.\n* **Buffers:**  Buffers are temporary storage areas that help manage data flow differences between components. While used in conjunction with interface circuits, they don't serve as the primary intermediary.\n* **None of the mentioned:**  Interface circuits are essential for device-bus communication.\n\n**Therefore, the best answer is (A).** \n"
            }
        ]
    },
    {
        "doc_id": 598,
        "doc": {
            "question_id": -1,
            "question": "The side of the interface circuits, that has the data path and the control signals to transfer data between interface and device is ____",
            "options": [
                "BUS side",
                "Port side",
                "Hardwell side",
                "Software side"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The side of the interface circuits, that has the data path and the control signals to transfer data between interface and device is ____",
                "Expected Label": "B",
                "Predicted Label": "B) Port side \n"
            }
        ]
    },
    {
        "doc_id": 599,
        "doc": {
            "question_id": -1,
            "question": "What is the interface circuit?",
            "options": [
                "Helps in installing of the software driver for the device",
                "Houses the buffer that helps in data transfer",
                "Helps in the decoding of the address on the address BUs",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "What is the interface circuit?",
                "Expected Label": "C",
                "Predicted Label": "B) Houses the buffer that helps in data transfer \n"
            }
        ]
    },
    {
        "doc_id": 600,
        "doc": {
            "question_id": -1,
            "question": "The parallel mode of communication is not suitable for long devices because of ____",
            "options": [
                "Timing skew",
                "Memory access delay",
                "Latency",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The parallel mode of communication is not suitable for long devices because of ____",
                "Expected Label": "A",
                "Predicted Label": "A. \n\n**Timing skew** is the main reason parallel communication is unsuitable for long distances. Here's why:\n\n* **Parallel Communication:** Data is sent simultaneously over multiple wires.\n* **Timing Skew:**  Over long distances, slight variations in signal propagation time on different wires can cause bits to arrive at slightly different times.  This misalignment makes it difficult to reconstruct the intended data.\n\nLet me know if you'd like more detail on any of the other options! \n"
            }
        ]
    },
    {
        "doc_id": 601,
        "doc": {
            "question_id": -1,
            "question": "The status flags required for data transfer is present in ____",
            "options": [
                "Device",
                "Device driver",
                "Interface circuit",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The status flags required for data transfer is present in ____",
                "Expected Label": "C",
                "Predicted Label": "**C) Interface circuit**\n\n**Explanation:**\n\nInterface circuits, also known as I/O controllers or peripheral controllers, manage data transfer between the CPU and peripherals. They contain status flags that indicate the status of data transfer operations, such as:\n\n* **Data ready:** Indicates that the device has data available to be read.\n* **Data received:** Indicates that the interface circuit has received data from the device.\n* **Busy:** Indicates that the device is currently busy with another operation.\n* **Error:** Indicates that an error occurred during data transfer.\n\nThese status flags are essential for the CPU to coordinate data transfer with the device. \n"
            }
        ]
    },
    {
        "doc_id": 602,
        "doc": {
            "question_id": -1,
            "question": "User programmable terminals that combine VDT hardware with built-in microprocessor is ____",
            "options": [
                "KIPs",
                "Pc",
                "Mainframe",
                "Intelligent terminals"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "User programmable terminals that combine VDT hardware with built-in microprocessor is ____",
                "Expected Label": "D",
                "Predicted Label": "D. Intelligent terminals \n"
            }
        ]
    },
    {
        "doc_id": 603,
        "doc": {
            "question_id": -1,
            "question": "Which most popular input device is used today for interactive processing and for the one line entry of data for batch processing?",
            "options": [
                "Mouse",
                "Magnetic disk",
                "Visual display terminal",
                "Card punch"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which most popular input device is used today for interactive processing and for the one line entry of data for batch processing?",
                "Expected Label": "A",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile all the options listed have been used for input, the **Visual Display Terminal (VDT)**, encompassing keyboards and monitors, is the most prevalent input device for both interactive and batch processing. \n\n* **Interactive processing** relies heavily on real-time user input through keyboards, while the monitor provides immediate feedback.\n* **Batch processing**, though not real-time, often utilizes keyboards for initial command entries or data adjustments before running a batch job.\n\nLet me know if you'd like further clarification on any of the options! \n"
            }
        ]
    },
    {
        "doc_id": 604,
        "doc": {
            "question_id": -1,
            "question": "The use of spooler programs or ____ Hardware allows PC operators to do the processing work at the same time a printing operation is in progress.",
            "options": [
                "Registers",
                "Memory",
                "Buffer",
                "CPU"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The use of spooler programs or ____ Hardware allows PC operators to do the processing work at the same time a printing operation is in progress.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 605,
        "doc": {
            "question_id": -1,
            "question": "____ is used as an intermediate to extend the processor BUS.",
            "options": [
                "Bridge",
                "Router",
                "Connector",
                "Gateway"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is used as an intermediate to extend the processor BUS.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 606,
        "doc": {
            "question_id": -1,
            "question": "____ is an extension of the processor BUS.",
            "options": [
                "SCSI BUS",
                "USB",
                "PCI BUS",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is an extension of the processor BUS.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **B) USB**. Here's why:\n\n* **USB (Universal Serial Bus)** is designed as an extension to the processor bus. It allows for hot-swapping, data transfer, and even, in some cases, power delivery to peripheral devices. \n\nLet's look at why the other options are incorrect:\n\n* **SCSI (Small Computer System Interface)** is a bus standard, but it's not an extension of the processor bus. It's typically used for connecting storage devices like hard drives.\n* **PCI (Peripheral Component Interconnect)**  is another bus standard, more powerful than USB, but it's also not a direct extension of the processor bus. It's found inside the computer for connecting expansion cards.\n\nLet me know if you'd like more explanation on any of these bus types! \n"
            }
        ]
    },
    {
        "doc_id": 607,
        "doc": {
            "question_id": -1,
            "question": "What is the full form of ISA?",
            "options": [
                "International American Standard",
                "Industry Standard Architecture",
                "International Standard Architecture",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "What is the full form of ISA?",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 608,
        "doc": {
            "question_id": -1,
            "question": "What is the full form of ANSI?",
            "options": [
                "American National Standards Institute",
                "Architectural National Standards Institute",
                "Asian National Standards Institute",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "What is the full form of ANSI?",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 609,
        "doc": {
            "question_id": -1,
            "question": "The video devices are connected to ____ BUS.",
            "options": [
                "PCI",
                "USB",
                "HDMI",
                "SCSI"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The video devices are connected to ____ BUS.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **A) PCI**. Here's why:\n\n* **PCI (Peripheral Component Interconnect)** is a common bus standard used for connecting a wide range of internal peripherals, including video cards, to the motherboard. \n\nLet's look at the other options:\n\n* **USB (Universal Serial Bus)** is primarily designed for external peripherals and is not typically used for internal video cards.\n* **HDMI (High-Definition Multimedia Interface)** is a video and audio interface, not a bus standard. It's used to connect devices like monitors and TVs.\n* **SCSI (Small Computer System Interface)** is a bus standard used for high-speed data transfer, typically for hard drives and tape drives. It's less common for video cards. \n\n**Therefore, PCI is the most suitable bus for connecting video devices.** \n"
            }
        ]
    },
    {
        "doc_id": 610,
        "doc": {
            "question_id": -1,
            "question": "SCSI stands for ____",
            "options": [
                "Signal Computer System Interface",
                "Small Computer System Interface",
                "Small Coding System Interface",
                "Signal Coding System Interface"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "SCSI stands for ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 611,
        "doc": {
            "question_id": -1,
            "question": "ISO stands for ____",
            "options": [
                "International Standards Organisation",
                "International Software Organisation",
                "Industrial Standards Organisation",
                "Industrial Software Organisation"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "ISO stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 612,
        "doc": {
            "question_id": -1,
            "question": "The system developed by IBM with ISA architecture is ____",
            "options": [
                "SPARC",
                "SUN-SPARC",
                "PC-AT",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The system developed by IBM with ISA architecture is ____",
                "Expected Label": "C",
                "Predicted Label": "C) PC-AT \n\nLet me know if you'd like more details on ISA architecture or IBM's role! \n"
            }
        ]
    },
    {
        "doc_id": 613,
        "doc": {
            "question_id": -1,
            "question": "IDE disk is connected to the PCI BUS using ____ interface.",
            "options": [
                "ISA",
                "ISO",
                "ANSI",
                "IEEE"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "IDE disk is connected to the PCI BUS using ____ interface.",
                "Expected Label": "A",
                "Predicted Label": "D) IEEE \n\nIEEE standards are commonly used for various computer components and interfaces, including storage devices. \n"
            }
        ]
    },
    {
        "doc_id": 614,
        "doc": {
            "question_id": -1,
            "question": "IDE stands for ____",
            "options": [
                "Integrated Device Electronics",
                "International Device Encoding",
                "Industrial Decoder Electronics",
                "International Decoder Encoder"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "IP Design and Manufacturing"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "IDE stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 615,
        "doc": {
            "question_id": -1,
            "question": "The ____ circuit enables the generation of the ASCII code when the key is pressed.",
            "options": [
                "Generator",
                "Debouncing",
                "Encoder",
                "Logger"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ circuit enables the generation of the ASCII code when the key is pressed.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Encoder circuits** are specifically designed to translate key presses on a keyboard (or other input devices with multiple signals) into their corresponding ASCII codes. \n\nThe other options are incorrect:\n\n* **Generator (A)** is a broad term and doesn't specifically refer to ASCII code generation.\n* **Debouncing (B)**  is a technique used to eliminate spurious signals from bouncing mechanical contacts in switches, ensuring a single, clean signal for each key press.\n* **Logger (D)** is used for recording data, not generating codes. \n"
            }
        ]
    },
    {
        "doc_id": 616,
        "doc": {
            "question_id": -1,
            "question": "To overcome multiple signals being generated upon a single press of the button, we make use of ____",
            "options": [
                "Generator circuit",
                "Debouncing circuit",
                "Multiplexer",
                "XOR circuit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To overcome multiple signals being generated upon a single press of the button, we make use of ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 617,
        "doc": {
            "question_id": -1,
            "question": "The best mode of connection between devices which need to send or receive large amounts of data over a short distance is ____",
            "options": [
                "BUS",
                "Serial port",
                "Parallel port",
                "Isochronous port"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The best mode of connection between devices which need to send or receive large amounts of data over a short distance is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Parallel ports** are designed for high-speed, short-distance data transfer. They send multiple bits simultaneously, making them ideal for devices like printers which need to handle a lot of data quickly.\n\nLet's look at why the other options are less suitable:\n\n* **BUS (A):** While a bus system also allows multiple connections, it's a shared medium. This means the bandwidth is divided, making it less efficient for large data transfers between two specific devices.\n* **Serial port (B):** Serial ports send data one bit at a time, making them too slow for bulk data transfer.\n* **Isochronous port (D):**  Isochronous connections prioritize real-time data transfer with guaranteed timing (like audio/video streaming) over raw throughput. They are not ideal for transferring large files. \n"
            }
        ]
    },
    {
        "doc_id": 618,
        "doc": {
            "question_id": -1,
            "question": "The output of the encoder circuit is/are ____",
            "options": [
                "ASCII code",
                "ASCII code and the valid signal",
                "Encoded signal",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The output of the encoder circuit is/are ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) Encoded signal**. Here's why:\n\n* **Encoder circuits**, by definition, take an input (often a set of signals) and produce a coded output. This output doesn't have to be ASCII.\n\nLet's break down why the other options aren't the best fit:\n\n* **A) ASCII code:** While some encoders might produce ASCII, it's not a universal output for all encoder circuits. Encoders can be designed for various coding schemes.\n* **B) ASCII code and the valid signal:**  A \"valid signal\" might be part of an encoder's output to indicate a successful encoding, but again, the core output is the encoded data itself.\n* **D) None of the mentioned:** This is incorrect, as the primary function of an encoder is to produce an encoded output.\n\n**Therefore, \"C) Encoded signal\" is the most accurate and general answer.** \n"
            }
        ]
    },
    {
        "doc_id": 619,
        "doc": {
            "question_id": -1,
            "question": "The disadvantage of using a parallel mode of communication is ____",
            "options": [
                "It is costly",
                "Leads to erroneous data transfer",
                "Security of data",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The disadvantage of using a parallel mode of communication is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 620,
        "doc": {
            "question_id": -1,
            "question": "In a 32 bit processor, the A0 bit of the address line is connected to ____ of the parallel port interface.",
            "options": [
                "Valid bit",
                "Idle bit",
                "Interrupt enable bit",
                "Status or data register"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a 32 bit processor, the A0 bit of the address line is connected to ____ of the parallel port interface.",
                "Expected Label": "D",
                "Predicted Label": "**D) Status or data register**\n\n**Explanation:**\n\nIn a typical memory-mapped I/O system, the lower bits of the address bus are used to select specific registers within a peripheral device. A0, being the least significant bit, would most likely be used to select between the status register and the data register of a parallel port interface.\n\n* **Status Register:** Contains information about the status of the port (e.g., whether data is ready to be read or if the port is busy).\n* **Data Register:** Used to send and receive data from the connected device. \n\nLet me know if you'd like more details on memory-mapped I/O or parallel port communication! \n"
            }
        ]
    },
    {
        "doc_id": 621,
        "doc": {
            "question_id": -1,
            "question": "The Status flag circuit is implemented using ____",
            "options": [
                "RS flip flop",
                "D flip flop",
                "JK flip flop",
                "Xor circuit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The Status flag circuit is implemented using ____",
                "Expected Label": "B",
                "Predicted Label": "D \n\nThe Status flag is typically determined by the result of an ALU operation. The XOR circuit is excellent for comparing bits and identifying differences, making it suitable for setting flags like Zero (all bits zero) or Overflow. \n"
            }
        ]
    },
    {
        "doc_id": 622,
        "doc": {
            "question_id": -1,
            "question": "In the output interface of the parallel port, along with the valid signal ____ is also sent.",
            "options": [
                "Data",
                "Idle signal",
                "Interrupt",
                "Acknowledge signal"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the output interface of the parallel port, along with the valid signal ____ is also sent.",
                "Expected Label": "B",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 623,
        "doc": {
            "question_id": -1,
            "question": "DDR stands for ____",
            "options": [
                "Data Direction Register",
                "Data Decoding Register",
                "Data Decoding Rate",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "DDR stands for ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\nDDR stands for **Double Data Rate**. \n"
            }
        ]
    },
    {
        "doc_id": 624,
        "doc": {
            "question_id": -1,
            "question": "In a general 8-bit parallel interface, the INTR line is connected to ____",
            "options": [
                "Status and Control unit",
                "DDR",
                "Register select",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a general 8-bit parallel interface, the INTR line is connected to ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Status and Control unit**. Here's why:\n\n* **INTR (Interrupt Request)** is a signal sent from a peripheral device to the CPU. It indicates that the device needs attention, like having data to transfer or requiring a status change. \n* The **Status and Control unit** within a parallel interface is responsible for managing communication between the CPU and the peripheral. This includes handling interrupt requests.\n\nLet me know if you'd like more details on parallel interfaces or interrupt handling! \n"
            }
        ]
    },
    {
        "doc_id": 625,
        "doc": {
            "question_id": -1,
            "question": "The mode of transmission of data, where one bit is sent for each clock cycle is ____",
            "options": [
                "Asynchronous",
                "Parallel",
                "Serial",
                "Isochronous"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The mode of transmission of data, where one bit is sent for each clock cycle is ____",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 626,
        "doc": {
            "question_id": -1,
            "question": "The transformation between the Parallel and serial ports is done with the help of ____",
            "options": [
                "Flip flops",
                "Logic circuits",
                "Shift registers",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The transformation between the Parallel and serial ports is done with the help of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Shift registers** are specifically designed for parallel-to-serial and serial-to-parallel data conversion. \n\nHere's why:\n\n* **Shift registers** store data and can shift it bit-by-bit. This makes them ideal for taking parallel data (multiple bits at once) and outputting it serially (one bit at a time), or vice versa.\n\nLet's look at the other options:\n\n* **Flip-flops** are fundamental memory elements but don't inherently handle the shifting needed for parallel/serial conversion.\n* **Logic circuits** encompass a broad category. While conversion involves logic, shift registers are the specific circuit element designed for this purpose. \n* **None of the mentioned** is incorrect as shift registers are the core component for this transformation. \n"
            }
        ]
    },
    {
        "doc_id": 627,
        "doc": {
            "question_id": -1,
            "question": "The serial port is used to connect basically ____ and processor.",
            "options": [
                "I/O devices",
                "Speakers",
                "Printer",
                "Monitor"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The serial port is used to connect basically ____ and processor.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 628,
        "doc": {
            "question_id": -1,
            "question": "The double buffer is used for ____",
            "options": [
                "Enabling retrieval of multiple bits of input",
                "Combining the input and output operations",
                "Extending the buffer capacity",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The double buffer is used for ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **(D) None of the mentioned**. Here's why:\n\n* **Double buffering** is a technique used to speed up input/output operations, particularly in situations where data is being transferred between a fast processor and a slower device (like a hard drive or display). \n\nLet's break down why the other options are incorrect:\n\n* **A) Enabling retrieval of multiple bits of input:**  While double buffering can involve multiple bits, its core purpose isn't about retrieving multiple bits simultaneously.\n* **B) Combining the input and output operations:** Double buffering keeps input and output operations separate but allows them to happen more efficiently in parallel.\n* **C) Extending the buffer capacity:** While a double buffer system has more total capacity than a single buffer, its primary goal is not simply about increasing capacity.\n\n**How Double Buffering Works**\n\nImagine two buffers:\n\n1. **Buffer 1:**  Receives data from the input device.\n2. **Buffer 2:** Holds data to be sent to the output device.\n\nThe processor can quickly fill Buffer 1.  While Buffer 1 is being filled, the output device can simultaneously empty data from Buffer 2. Once Buffer 2 is empty, the roles switch \u2013 Buffer 1 (now full) is used for output, and Buffer 2 is ready to receive new input. This constant switching minimizes waiting time for the processor and output device. \n"
            }
        ]
    },
    {
        "doc_id": 629,
        "doc": {
            "question_id": -1,
            "question": "____ to increase the flexibility of the serial ports.",
            "options": [
                "The wires used for ports is changed",
                "The ports are made to allow different clock signals for input and output",
                "The drivers are modified",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ to increase the flexibility of the serial ports.",
                "Expected Label": "B",
                "Predicted Label": "**B) The ports are made to allow different clock signals for input and output** \n\n**Explanation:**\n\nFlexibility in serial communication often relates to asynchronous communication. A key feature of asynchronous communication is the ability to use different clock speeds for the sender and receiver. This allows devices with varying clock rates to communicate effectively. \n\nLet's look at why the other options are not the best fit:\n\n* **A) The wires used for ports is changed:**  While the physical medium (like wires) plays a role in signal transmission, changing them doesn't inherently increase the *flexibility* of how the serial port operates.\n* **C) The drivers are modified:** Drivers are software components. While they are crucial for communication, modifying them alone doesn't change the fundamental flexibility of the port's hardware capabilities.\n* **D) All of the mentioned:**  Options A and C are not directly related to increasing the flexibility of serial ports in terms of accommodating different data rates.\n\n**Therefore, B is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 630,
        "doc": {
            "question_id": -1,
            "question": "UART stands for ____",
            "options": [
                "Universal Asynchronous Relay Transmission",
                "Universal Accumulator Register Transfer",
                "Universal Asynchronous Receiver Transmitter",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "UART stands for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 631,
        "doc": {
            "question_id": -1,
            "question": "The key feature of UART is ____",
            "options": [
                "Its architectural design",
                "Its simple implementation",
                "Its general purpose usage",
                "Its enhancement of connecting low speed devices"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The key feature of UART is ____",
                "Expected Label": "D",
                "Predicted Label": "D) Its enhancement of connecting low speed devices \n\n**Explanation:**\n\nWhile UART has a simple design and is generally applicable, its **key** feature is the ability to easily interface with and enhance the connection of low-speed devices to a system. This is achieved by handling the serial-to-parallel and parallel-to-serial conversions necessary for communication. \n"
            }
        ]
    },
    {
        "doc_id": 632,
        "doc": {
            "question_id": -1,
            "question": "The data transfer in UART is done in ____",
            "options": [
                "Asynchronous start stop format",
                "Synchronous start stop format",
                "Isochronous format",
                "EBDIC format"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The data transfer in UART is done in ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 633,
        "doc": {
            "question_id": -1,
            "question": "The standard used in serial ports to facilitate communication is ____",
            "options": [
                "RS-246",
                "RS-LNK",
                "RS-232-C",
                "Both RS-246 and RS-LNK"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The standard used in serial ports to facilitate communication is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 634,
        "doc": {
            "question_id": -1,
            "question": "In a serial port interface, the INTR line is connected to ____",
            "options": [
                "Status register",
                "Shift register",
                "Chip select",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a serial port interface, the INTR line is connected to ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **D) None of the mentioned**. Here's why:\n\n* **INTR (Interrupt Request)** is an output signal from the serial port interface to the CPU.  It signals that the serial port needs attention, such as data being received or ready to transmit.\n\n* **The options provided are incorrect because:**\n    * **Status Register:** Holds information about the serial port's current state (e.g., errors, data ready). The INTR line reflects changes *within* the status register but isn't directly connected to it.\n    * **Shift Register:**  Handles the actual shifting of data bits for serial transmission/reception. \n    * **Chip Select:**  Used to activate a specific chip on a bus, not relevant to interrupt handling. \n\n**In summary:** The INTR line is a crucial part of interrupt-driven communication, but it's not directly wired to the components listed. It signals the CPU based on events reflected in registers like the status register. \n"
            }
        ]
    },
    {
        "doc_id": 635,
        "doc": {
            "question_id": -1,
            "question": "The PCI follows a set of standards primarily used in ____ PC\u2019s.",
            "options": [
                "Intel",
                "Motorola",
                "IBM",
                "SUN"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The PCI follows a set of standards primarily used in ____ PC\u2019s.",
                "Expected Label": "C",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 636,
        "doc": {
            "question_id": -1,
            "question": "The ____ is the BUS used in Macintosh PC\u2019s.",
            "options": [
                "NuBUS",
                "EISA",
                "PCI",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ is the BUS used in Macintosh PC\u2019s.",
                "Expected Label": "A",
                "Predicted Label": "A) NuBUS \n\nLet me know if you'd like to know more about NuBus or other computer bus architectures. \n"
            }
        ]
    },
    {
        "doc_id": 637,
        "doc": {
            "question_id": -1,
            "question": "The key feature of the PCI BUS is ____",
            "options": [
                "Low cost connectivity",
                "Plug and Play capability",
                "Expansion of Bandwidth",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The key feature of the PCI BUS is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nWhile PCI does offer advantages like increased bandwidth over older standards, its defining characteristic was **Plug and Play (PnP)**. This allowed users to connect devices without intricate configuration, significantly improving user-friendliness. \n"
            }
        ]
    },
    {
        "doc_id": 638,
        "doc": {
            "question_id": -1,
            "question": "PCI stands for ____",
            "options": [
                "Peripheral Component Interconnect",
                "Peripheral Computer Internet",
                "Processor Computer Interconnect",
                "Processor Cable Interconnect"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "IP Design and Manufacturing"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "PCI stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 639,
        "doc": {
            "question_id": -1,
            "question": "The PCI BUS supports ____ address space/s.",
            "options": [
                "I/O",
                "Memory",
                "Configuration",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The PCI BUS supports ____ address space/s.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 640,
        "doc": {
            "question_id": -1,
            "question": "____ address space gives the PCI its plug and plays capability.",
            "options": [
                "Configuration",
                "I/O",
                "Memory",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ address space gives the PCI its plug and plays capability.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 641,
        "doc": {
            "question_id": -1,
            "question": "____ provides a separate physical connection to the memory.",
            "options": [
                "PCI BUS",
                "PCI interface",
                "PCI bridge",
                "Switch circuit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ provides a separate physical connection to the memory.",
                "Expected Label": "C",
                "Predicted Label": "**A) PCI BUS** \n\nThe PCI (Peripheral Component Interconnect) bus is a parallel bus architecture that provides a direct connection between devices and the system memory. \n\n**Explanation:**\n\n* **PCI BUS:**  A shared pathway allowing multiple devices (including network cards, sound cards, etc.) to communicate directly with the memory.\n* **PCI interface:**  A general term referring to the connection point using the PCI standard, not a separate physical connection itself.\n* **PCI bridge:** A chip that connects different PCI buses, often used to connect slower peripherals.\n* **Switch circuit:** Used for routing electrical signals, not specifically for memory connections. \n\nLet me know if you'd like more details on PCI or computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 642,
        "doc": {
            "question_id": -1,
            "question": "The master is also called as ____ in PCI terminology.",
            "options": [
                "Initiator",
                "Commander",
                "Chief",
                "Starter"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The master is also called as ____ in PCI terminology.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 643,
        "doc": {
            "question_id": -1,
            "question": "Signals whose names end in ____ are asserted in the low voltage state.",
            "options": [
                "$",
                "#",
                "*",
                "!"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Signals whose names end in ____ are asserted in the low voltage state.",
                "Expected Label": "B",
                "Predicted Label": "B) # \n"
            }
        ]
    },
    {
        "doc_id": 644,
        "doc": {
            "question_id": -1,
            "question": "A complete transfer operation over the BUS, involving the address and a burst of data is called ____",
            "options": [
                "Transaction",
                "Transfer",
                "Move",
                "Procedure"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A complete transfer operation over the BUS, involving the address and a burst of data is called ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 645,
        "doc": {
            "question_id": -1,
            "question": "The device connected to the BUS are given addresses of ____ bit.",
            "options": [
                "24",
                "64",
                "32",
                "16"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The device connected to the BUS are given addresses of ____ bit.",
                "Expected Label": "B",
                "Predicted Label": "The answer depends on the context of the bus, as different buses have different address widths. However, some common address widths are 16, 32, and 64 bits. 24 bits is less common. \n\n**Without more context, the most likely answer is either C) 32 or B) 64.** \n\nHere's why:\n\n* **32-bit addressing** was very common for a long time and is still used in many systems.\n* **64-bit addressing** is becoming increasingly prevalent, especially in modern computers and servers.\n\n**To give a definitive answer, I need more information about the specific BUS in question.** \n"
            }
        ]
    },
    {
        "doc_id": 646,
        "doc": {
            "question_id": -1,
            "question": "The PCI BUS has ____ interrupt request lines.",
            "options": [
                "6",
                "1",
                "4",
                "3"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The PCI BUS has ____ interrupt request lines.",
                "Expected Label": "C",
                "Predicted Label": "C) 4 \n"
            }
        ]
    },
    {
        "doc_id": 647,
        "doc": {
            "question_id": -1,
            "question": "____ signal is sent by the initiator to indicate the duration of the transaction.",
            "options": [
                "FRAME#",
                "IRDY#",
                "TMY#",
                "SELD#"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ signal is sent by the initiator to indicate the duration of the transaction.",
                "Expected Label": "A",
                "Predicted Label": "C) TMY# \n\n**Explanation:**\n\n* **TMY# (Target Module Time)**: This signal is used in some bus protocols (like Futurebus) by the initiator (the device initiating the data transfer) to indicate to the target (the responding device) how long it needs to hold the bus. This essentially signals the duration of the transaction.\n\n**Why the other options are incorrect:**\n\n* **FRAME#**:  This signal typically indicates the start and end of a data frame on a bus. It doesn't specify duration.\n* **IRDY# (Initiator Ready)**: This signal indicates the initiator's readiness for data transfer, not the duration.\n* **SELD# (Select)**: This signal is used to select a particular device on the bus for communication. \n"
            }
        ]
    },
    {
        "doc_id": 648,
        "doc": {
            "question_id": -1,
            "question": "____ signal is used to enable commands.",
            "options": [
                "FRAME#",
                "IRDY#",
                "TMY#",
                "c/BE#"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ signal is used to enable commands.",
                "Expected Label": "D",
                "Predicted Label": "D) c/BE# \n\n**Explanation:**\n\n* **c/BE# (Command/Bus Enable)** is the signal used to enable commands in many computer architectures. It indicates that the data on the bus is a command to be interpreted by the receiving device.\n\nLet's look at why the other options are incorrect:\n\n* **FRAME#** is a signal used to indicate the start and end of a frame in synchronous communication.\n* **IRDY# (Interrupt Request)** is a signal used by a device to request service from the processor.\n* **TMY# (Target Ready)** is not a standard signal in computer architecture. \n\n**Therefore, the correct answer is D.** \n"
            }
        ]
    },
    {
        "doc_id": 649,
        "doc": {
            "question_id": -1,
            "question": "IRDY# signal is used for ____",
            "options": [
                "Selecting the interrupt line",
                "Sending an interrupt",
                "Saying that the initiator is ready",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "IRDY# signal is used for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nThe IRDY# (Initiator Ready) signal is used to indicate that the device initiating a data transfer (the \"initiator\") is ready to send or receive data. \n"
            }
        ]
    },
    {
        "doc_id": 650,
        "doc": {
            "question_id": -1,
            "question": "The signal used to indicate that the slave is ready is ____",
            "options": [
                "SLRY#",
                "TRDY#",
                "DSDY#",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The signal used to indicate that the slave is ready is ____",
                "Expected Label": "B",
                "Predicted Label": "C \n\n**DSDY# (Data Strobe/Data Ready)** is the signal used in asynchronous data transfer protocols to indicate that the slave device (the one receiving data) is ready to receive data. \n\nLet's break down why the other options are incorrect:\n\n* **SLRY#:** This acronym isn't commonly used in standard bus protocols.\n* **TRDY# (Transfer Ready):** While similar, TRDY# is usually sent by the master device to indicate it's ready for a data transfer.\n* **None of the mentioned:**  DSDY# is a standard signal for this purpose. \n\nLet me know if you'd like a deeper dive into asynchronous data transfer! \n"
            }
        ]
    },
    {
        "doc_id": 651,
        "doc": {
            "question_id": -1,
            "question": "DEVSEL# signal is used ____",
            "options": [
                "To select the device",
                "To list all the devices connected",
                "By the device to indicate that it is ready for a transaction",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "DEVSEL# signal is used ____",
                "Expected Label": "C",
                "Predicted Label": "A) To select the device \n\n**Explanation:**\n\nDEVSEL# (Device Select) is an active low signal used in computer architecture, typically within a bus-based system.  When low, it indicates that the bus master (e.g., CPU) has selected a specific device for a data transfer operation. \n\nLet me know if you'd like more details on how device selection works! \n"
            }
        ]
    },
    {
        "doc_id": 652,
        "doc": {
            "question_id": -1,
            "question": "The signal used to initiate device select ____",
            "options": [
                "IRDY#",
                "S/BE",
                "DEVSEL#",
                "IDSEL#"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The signal used to initiate device select ____",
                "Expected Label": "D",
                "Predicted Label": "C) DEVSEL# \n\n**Explanation:**\n\n* **DEVSEL#** (Device Select) is a common signal used to indicate that a particular device is being addressed or selected for communication.  \n\nLet's break down why the other options are incorrect:\n\n* **IRDY# (Interrupt Request)**: This signal indicates that a device is requesting an interrupt, not being selected.\n* **S/BE (Size/Byte Enable)**: This signal specifies the size of the data transfer and which bytes within a word are being accessed. It's not directly related to device selection.\n* **IDSEL# (ID Select)**: While this term isn't standard, it might be used in specific contexts. However, \"DEVSEL#\" is a more widely recognized term for device selection. \n"
            }
        ]
    },
    {
        "doc_id": 653,
        "doc": {
            "question_id": -1,
            "question": "The PCi BUS allows us to connect ____ I/O devices.",
            "options": [
                "21",
                "13",
                "9",
                "11"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The PCi BUS allows us to connect ____ I/O devices.",
                "Expected Label": "A",
                "Predicted Label": "D) 11 \n\nThe PCI bus typically allows for up to 11 devices (including bridges). \n"
            }
        ]
    },
    {
        "doc_id": 654,
        "doc": {
            "question_id": -1,
            "question": "The key features of the SCSI BUS are ____",
            "options": [
                "The cost effective connective media",
                "The ability overlap data transfer requests",
                "The highly efficient data transmission",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The key features of the SCSI BUS are ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 655,
        "doc": {
            "question_id": -1,
            "question": "In a data transfer operation involving SCSI BUS, the control is with ____",
            "options": [
                "Initiator",
                "Target",
                "SCSI controller",
                "Target Controller"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a data transfer operation involving SCSI BUS, the control is with ____",
                "Expected Label": "D",
                "Predicted Label": "A \n\n**Explanation:**\n\nIn SCSI (Small Computer System Interface) bus architecture, the **Initiator** is the device that initiates the data transfer operation. It has the overall control and issues commands to the Target device. \n"
            }
        ]
    },
    {
        "doc_id": 656,
        "doc": {
            "question_id": -1,
            "question": "What is DB(P) line?",
            "options": [
                "That the data line is carrying the device information",
                "That the data line is carrying the parity information",
                "That the data line is partly closed",
                "That the data line is temporarily occupied"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "What is DB(P) line?",
                "Expected Label": "B",
                "Predicted Label": "**B) That the data line is carrying the parity information** \n\nDB(P) typically stands for \"Data Bus (Parity)\". The \"(P)\" indicates that one or more lines within the data bus are dedicated to carrying parity bits, which are used for error detection. \n"
            }
        ]
    },
    {
        "doc_id": 657,
        "doc": {
            "question_id": -1,
            "question": "The BSY signal signifies ____",
            "options": [
                "The BUs is busy",
                "The controller is busy",
                "The Initiator is busy",
                "The Target is Busy"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The BSY signal signifies ____",
                "Expected Label": "A",
                "Predicted Label": "D) The Target is Busy \n"
            }
        ]
    },
    {
        "doc_id": 658,
        "doc": {
            "question_id": -1,
            "question": "The SEL signal signifies ____",
            "options": [
                "The initiator is selected",
                "The device for BUS control is selected",
                "That the target is being selected",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The SEL signal signifies ____",
                "Expected Label": "B",
                "Predicted Label": "C) That the target is being selected \n\n**Explanation:**\n\nThe SEL (Select) signal is a crucial part of bus arbitration in computer architecture. When a device wants to initiate a data transfer on the bus, it asserts the SEL signal to indicate that it's selecting a particular target device for the communication. \n"
            }
        ]
    },
    {
        "doc_id": 659,
        "doc": {
            "question_id": -1,
            "question": "____ signal is asserted when the initiator wishes to send a message to the target.",
            "options": [
                "MSG",
                "APP",
                "SMS",
                "ATN"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ signal is asserted when the initiator wishes to send a message to the target.",
                "Expected Label": "D",
                "Predicted Label": "D) ATN \n\n**Explanation:**\n\n* **ATN (Attention)** is a control signal commonly used in computer architecture to indicate that one device (the initiator) wants to initiate communication with another device (the target). \n\nLet me know if you'd like to explore other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 660,
        "doc": {
            "question_id": -1,
            "question": "The MSG signal is used ____",
            "options": [
                "To send a message to the target",
                "To receive a message from the mailbox",
                "To tell that the information being sent is a message",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The MSG signal is used ____",
                "Expected Label": "C",
                "Predicted Label": "C) To tell that the information being sent is a message \n"
            }
        ]
    },
    {
        "doc_id": 661,
        "doc": {
            "question_id": -1,
            "question": "____ is used to reset all the device controls to their startup state.",
            "options": [
                "SRT",
                "RST",
                "ATN",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is used to reset all the device controls to their startup state.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 662,
        "doc": {
            "question_id": -1,
            "question": "The SCSI BUS uses ____ arbitration.",
            "options": [
                "Distributed",
                "Centralised",
                "Daisy chain",
                "Hybrid"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The SCSI BUS uses ____ arbitration.",
                "Expected Label": "A",
                "Predicted Label": "A) Distributed \n"
            }
        ]
    },
    {
        "doc_id": 663,
        "doc": {
            "question_id": -1,
            "question": "SCSI stands for ____",
            "options": [
                "Small Computer System Interface",
                "Switch Computer system Interface",
                "Small Component System Interface",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "SCSI stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 664,
        "doc": {
            "question_id": -1,
            "question": "ANSI stands for ____",
            "options": [
                "American National System Interface",
                "ASCII National Standard Interface",
                "American Network System Interface",
                "American National Standard Institute"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "ANSI stands for ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 665,
        "doc": {
            "question_id": -1,
            "question": "A narrow SCSI BUS has ____ data lines.",
            "options": [
                "6",
                "8",
                "16",
                "4"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A narrow SCSI BUS has ____ data lines.",
                "Expected Label": "B",
                "Predicted Label": "B) 8 \n"
            }
        ]
    },
    {
        "doc_id": 666,
        "doc": {
            "question_id": -1,
            "question": "Single ended transmission means ____",
            "options": [
                "That all the signals have a similar bit pattern",
                "That the signals have a common source",
                "That the signals have a common ground return",
                "That the signals have a similar voltage signature"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Single ended transmission means ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nSingle-ended transmission relies on a **common ground reference** to measure the voltage of a signal. \n\nLet's break down why the other options are incorrect:\n\n* **A) That all the signals have a similar bit pattern:** This describes data similarity, not the transmission method.\n* **B) That the signals have a common source:** While signals might originate from a single source, it's not the defining characteristic of single-ended transmission.\n* **D) That the signals have a similar voltage signature:**  Voltage signatures can be similar, but the key is the reference point provided by the common ground. \n\n**Therefore, the correct answer is C.** \n"
            }
        ]
    },
    {
        "doc_id": 667,
        "doc": {
            "question_id": -1,
            "question": "HVD stands for ____",
            "options": [
                "High Voltage Differential",
                "High Voltage Density",
                "High Video Definition",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "HVD stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 668,
        "doc": {
            "question_id": -1,
            "question": "For better transfer rates on the SCSI BUS the length of the cable is limited to ____",
            "options": [
                "2m",
                "4m",
                "1.3m",
                "1.6m"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "For better transfer rates on the SCSI BUS the length of the cable is limited to ____",
                "Expected Label": "D",
                "Predicted Label": "D) 1.6m \n"
            }
        ]
    },
    {
        "doc_id": 669,
        "doc": {
            "question_id": -1,
            "question": "The maximum number of devices that can be connected to SCSI BUS is ____",
            "options": [
                "12",
                "10",
                "16",
                "8"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The maximum number of devices that can be connected to SCSI BUS is ____",
                "Expected Label": "C",
                "Predicted Label": "C) 16 \n"
            }
        ]
    },
    {
        "doc_id": 670,
        "doc": {
            "question_id": -1,
            "question": "The SCSI BUS is connected to the processor through ____",
            "options": [
                "SCSI Controller",
                "Bridge",
                "Switch",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The SCSI BUS is connected to the processor through ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe SCSI Controller acts as the interface between the processor/memory subsystem and the SCSI bus. Here's why:\n\n* **SCSI Controller:** This is a dedicated chip or circuit that manages communication between the processor and devices on the SCSI bus. It handles tasks like:\n    * Sending commands to and receiving data from SCSI devices.\n    * Managing data transfers and bus arbitration.\n    * Handling error detection and correction.\n\nLet's look at why the other options aren't the best fit:\n\n* **Bridge:** Bridges connect different types of buses (e.g., PCI to ISA), not directly to a processor.\n* **Switch:**  Switches are networking devices used to connect devices on a network, not within a single computer system.\n* **None of the mentioned:** This is incorrect as the SCSI Controller is the primary means of connection. \n"
            }
        ]
    },
    {
        "doc_id": 671,
        "doc": {
            "question_id": -1,
            "question": "The mode of data transfer used by the controller is ____",
            "options": [
                "Interrupt",
                "DMA",
                "Asynchronous",
                "Synchronous"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The mode of data transfer used by the controller is ____",
                "Expected Label": "B",
                "Predicted Label": "**D) Synchronous**\n\n**Explanation:**\n\nControllers typically communicate with peripherals using **synchronous** data transfer. Here's why:\n\n* **Synchronous Data Transfer:**  Involves a clock signal shared between the controller and the peripheral. Data is transferred in a predictable, timed manner with each clock pulse. This ensures reliable data transfer as both sides operate in lockstep.\n\nLet's look at why the other options are less suitable:\n\n* **A) Interrupt:**  Interrupts are signals from peripherals to the CPU, indicating an event needing attention. They don't define the data transfer mode itself.\n* **B) DMA:** DMA (Direct Memory Access) is a mechanism where peripherals can transfer data directly to/from memory without heavy CPU involvement. While DMA controllers exist, the core communication between a controller and a peripheral is still typically synchronous.\n* **C) Asynchronous Data Transfer:** Relies on handshaking signals instead of a shared clock, making it less predictable in timing and more complex for simple peripherals. \n\n**In summary:** Synchronous data transfer, with its reliance on a shared clock, is the most common and efficient method for communication between a controller and a peripheral. \n"
            }
        ]
    },
    {
        "doc_id": 672,
        "doc": {
            "question_id": -1,
            "question": "The data is stored on the disk in the form of blocks called ____",
            "options": [
                "Pages",
                "Frames",
                "Sectors",
                "Tables"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The data is stored on the disk in the form of blocks called ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 673,
        "doc": {
            "question_id": -1,
            "question": "The transfer rate, when the USB is operating in low-speed of operation is ____",
            "options": [
                "5 Mb/s",
                "12 Mb/s",
                "2.5 Mb/s",
                "1.5 Mb/s"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The transfer rate, when the USB is operating in low-speed of operation is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 674,
        "doc": {
            "question_id": -1,
            "question": "The high speed mode of operation of the USB was introduced by ____",
            "options": [
                "ISA",
                "USB 3.0",
                "USB 2.0",
                "ANSI"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "IP Design and Manufacturing"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The high speed mode of operation of the USB was introduced by ____",
                "Expected Label": "C",
                "Predicted Label": "B) USB 3.0 \n"
            }
        ]
    },
    {
        "doc_id": 675,
        "doc": {
            "question_id": -1,
            "question": "The sampling process in speaker output is a ____ process.",
            "options": [
                "Asynchronous",
                "Synchronous",
                "Isochronous",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The sampling process in speaker output is a ____ process.",
                "Expected Label": "C",
                "Predicted Label": "C) Isochronous \n\n**Explanation:**\n\n* **Isochronous** data transfer ensures that data is delivered at a regular and predictable rate. This is essential for audio output, as inconsistencies in timing would create noticeable distortions and clicks in the sound.\n\nLet's break down why the other options are incorrect:\n\n* **Asynchronous:** Data transfer happens irregularly, with no fixed timing relationship. This would be disastrous for audio.\n* **Synchronous:** Data transfer relies on a shared clock signal between sender and receiver. While this ensures timing consistency, it's not the most precise method for audio.\n* **None of the mentioned:** Isochronous is a specific term used for real-time data transfer with strict timing requirements, which is exactly what speaker output needs. \n"
            }
        ]
    },
    {
        "doc_id": 676,
        "doc": {
            "question_id": -1,
            "question": "The USB device follows ____ structure.",
            "options": [
                "List",
                "Huffman",
                "Hash",
                "Tree"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "IP Design and Manufacturing"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The USB device follows ____ structure.",
                "Expected Label": "D",
                "Predicted Label": "D) Tree \n"
            }
        ]
    },
    {
        "doc_id": 677,
        "doc": {
            "question_id": -1,
            "question": "The I/O devices form the ____ of the tree structure.",
            "options": [
                "Leaves",
                "Subordinate roots",
                "Left subtrees",
                "Right subtrees"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The I/O devices form the ____ of the tree structure.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nIn a typical computer architecture tree structure:\n\n* **Root:** Represents the CPU, the central processing unit.\n* **Branches:**  Represent the major system buses (e.g., front-side bus, PCI bus)\n* **Leaves:** Represent the I/O devices connected to those buses.\n\nI/O devices are at the \"end\" of the data paths, just like leaves are at the ends of branches. \n"
            }
        ]
    },
    {
        "doc_id": 678,
        "doc": {
            "question_id": -1,
            "question": "The device can send a message to the host by taking part in ____ for the communication path.",
            "options": [
                "Arbitration",
                "Polling",
                "Prioritizing",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The device can send a message to the host by taking part in ____ for the communication path.",
                "Expected Label": "B",
                "Predicted Label": "A) Arbitration \n\n**Explanation:**\n\n* **Arbitration** is the process where multiple devices, vying for the same resource (like a communication bus), establish which gets to transmit data. A device initiates communication by winning the arbitration, ensuring it can send its message without conflicts.\n\nLet's look at why the other options are incorrect:\n\n* **Polling:**  This is a method where the host continuously checks devices to see if they have data to send. It's the host initiating, not the device.\n* **Prioritizing:** While prioritizing can be part of the system, it doesn't directly enable a device to send a message. It determines the order of access if multiple devices are requesting. \n* **None of the mentioned:**  Arbitration is a fundamental concept in device communication. \n"
            }
        ]
    },
    {
        "doc_id": 679,
        "doc": {
            "question_id": -1,
            "question": "When the USB is connected to a system, its root hub is connected to the ____",
            "options": [
                "PCI BUS",
                "SCSI BUS",
                "Processor BUS",
                "IDE"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "When the USB is connected to a system, its root hub is connected to the ____",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe PCI (Peripheral Component Interconnect) bus is the most common bus that USB root hubs connect to. While other options might have been used in very old systems, they are essentially obsolete for this purpose:\n\n* **SCSI BUS:**  SCSI (Small Computer System Interface) was primarily used for hard drives and tape drives, not USB.\n* **Processor BUS:** The processor bus is a high-speed bus for components like the CPU, memory, and cache, not peripherals like USB.\n* **IDE:** IDE (Integrated Drive Electronics) was an interface standard for hard drives and optical drives and is unrelated to USB. \n"
            }
        ]
    },
    {
        "doc_id": 680,
        "doc": {
            "question_id": -1,
            "question": "The devices connected to USB is assigned a ____ address.",
            "options": [
                "9 bit",
                "16 bit",
                "4 bit",
                "7 bit"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The devices connected to USB is assigned a ____ address.",
                "Expected Label": "D",
                "Predicted Label": "D) 7 bit \n"
            }
        ]
    },
    {
        "doc_id": 681,
        "doc": {
            "question_id": -1,
            "question": "The initial address of a device just connected to the HUB is ____",
            "options": [
                "AHFG890",
                "0000000",
                "FFFFFFF",
                "0101010"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The initial address of a device just connected to the HUB is ____",
                "Expected Label": "B",
                "Predicted Label": "**B) 0000000** \n\n**Explanation:**\n\nWhile specific initialization processes can vary, it's most common for devices to start with a default address of all zeros (0000000) when connected to a hub.  Here's why:\n\n* **Clean Slate:** Starting at zero provides a clean slate for the hub to assign a unique address to the device.\n* **Plug-and-Play:**  This default address facilitates plug-and-play functionality. The hub can easily recognize a new device needing an address assignment.\n\nLet me know if you'd like more details on device addressing in computer networks! \n"
            }
        ]
    },
    {
        "doc_id": 682,
        "doc": {
            "question_id": -1,
            "question": "Locations in the device to or from which data transfers can take place is called ____",
            "options": [
                "End points",
                "Hosts",
                "Source",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Locations in the device to or from which data transfers can take place is called ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) End points**. Here's why:\n\n* **End points** in computer architecture refer to the specific locations (like registers, memory addresses, I/O ports) where data is sent from or received during a transfer. \n\nLet's look at why the other options are less accurate:\n\n* **Hosts:** Hosts are typically computers or devices connected to a network, not individual data transfer locations.\n* **Source:** While \"source\" indicates where data originates, it doesn't encompass the receiving location, which \"end point\" does.\n* **None of the mentioned:**  \"End points\" is the most accurate term in this context. \n\n**Therefore, A is the most suitable answer.** \n"
            }
        ]
    },
    {
        "doc_id": 683,
        "doc": {
            "question_id": -1,
            "question": "A USB pipe is a ____ channel.",
            "options": [
                "Simplex",
                "Half-Duplex",
                "Full-Duplex",
                "Both Simplex and Full-Duplex"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A USB pipe is a ____ channel.",
                "Expected Label": "C",
                "Predicted Label": "B) Half-Duplex \n\n**Explanation:**\n\nWhile USB supports different transfer types (control, bulk, interrupt, isochronous), the underlying pipe mechanism itself operates in a half-duplex manner. This means data can flow in both directions, but not simultaneously. One direction must be completed before switching to the other. \n"
            }
        ]
    },
    {
        "doc_id": 684,
        "doc": {
            "question_id": -1,
            "question": "The type/s of packets sent by the USB is/are ____",
            "options": [
                "Data",
                "Address",
                "Control",
                "Both Data and Control"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The type/s of packets sent by the USB is/are ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 685,
        "doc": {
            "question_id": -1,
            "question": "The first field of any packet is ____",
            "options": [
                "PID",
                "ADDR",
                "ENDP",
                "CRC16"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The first field of any packet is ____",
                "Expected Label": "A",
                "Predicted Label": "None of the options provided are universally true for the first field of *any* packet. Packet structures vary widely depending on the protocol and layer of the networking model. \n\nTherefore, the most accurate answer would be **none of the above**. However, since you require a single character selection, there is no correct answer among the choices. \n"
            }
        ]
    },
    {
        "doc_id": 686,
        "doc": {
            "question_id": -1,
            "question": "The last field in the packet is ____",
            "options": [
                "PID",
                "ADDR",
                "ENDP",
                "CRC"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The last field in the packet is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**CRC (Cyclic Redundancy Check)** is commonly used as the last field in a packet for error detection. \n\nLet me know if you'd like more explanation on any of these packet fields! \n"
            }
        ]
    },
    {
        "doc_id": 687,
        "doc": {
            "question_id": -1,
            "question": "The CRC bits are computed based on the values of the ____",
            "options": [
                "PID",
                "ADDR",
                "ENDP",
                "Both ADDR and ENDP"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The CRC bits are computed based on the values of the ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nCRC (Cyclic Redundancy Check) bits are calculated over the data portion of a frame to detect errors. In most protocols, the \"data portion\" includes both the address information (ADDR) and the actual payload data (ENDP). \n\nLet me know if you'd like a deeper dive into how CRC works! \n"
            }
        ]
    },
    {
        "doc_id": 688,
        "doc": {
            "question_id": -1,
            "question": "The data packets can contain data upto ____",
            "options": [
                "512 bytes",
                "256 bytes",
                "1024 bytes",
                "2 KB"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The data packets can contain data upto ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 689,
        "doc": {
            "question_id": -1,
            "question": "The most important objective of the USB is to provide ____",
            "options": [
                "Isochronous transmission",
                "Plug and play",
                "Easy device connection",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The most important objective of the USB is to provide ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 690,
        "doc": {
            "question_id": -1,
            "question": "The transmission over the USB is divided into ____",
            "options": [
                "Frames",
                "Pages",
                "Packets",
                "Tokens"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The transmission over the USB is divided into ____",
                "Expected Label": "A",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 691,
        "doc": {
            "question_id": -1,
            "question": "The ____ signal is used to indicate the beginning of a new frame.",
            "options": [
                "Start",
                "SOF",
                "BEG",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ signal is used to indicate the beginning of a new frame.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**SOF** stands for **Start of Frame**. This signal is commonly used in data transmission protocols to delimit frames, especially in synchronous communication. \n"
            }
        ]
    },
    {
        "doc_id": 692,
        "doc": {
            "question_id": -1,
            "question": "The SOF is transmitted every ____",
            "options": [
                "1s",
                "5s",
                "1ms",
                "1Us"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The SOF is transmitted every ____",
                "Expected Label": "C",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 693,
        "doc": {
            "question_id": -1,
            "question": "The power specification of usb is ____",
            "options": [
                "5v",
                "10v",
                "24v",
                "10v"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The power specification of usb is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 694,
        "doc": {
            "question_id": -1,
            "question": "The duration between the read and the mfc signal is ____",
            "options": [
                "Access time",
                "Latency",
                "Delay",
                "Cycle time"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The duration between the read and the mfc signal is ____",
                "Expected Label": "A",
                "Predicted Label": "B) Latency \n\n**Explanation:**\n\n* **Latency** specifically refers to the time it takes for data to become available *after* a read request is made. In this context, the duration between the read signal and the data being ready (signaled by mfc) is precisely the latency of that memory access.\n\nLet's look at why the other options aren't the best fit:\n\n* **Access Time:**  Often used synonymously with latency, but can sometimes refer to the total time taken from request to the data being *usable* by the processor.\n* **Delay:**  Too general a term.  While latency is a type of delay, it doesn't specifically pinpoint the read-to-available data timeframe.\n* **Cycle Time:**  Refers to the minimum time between initiating consecutive operations in a memory system. \n"
            }
        ]
    },
    {
        "doc_id": 695,
        "doc": {
            "question_id": -1,
            "question": "The minimum time delay between two successive memory read operations is ____",
            "options": [
                "Cycle time",
                "Latency",
                "Delay",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The minimum time delay between two successive memory read operations is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Cycle Time:** This represents the minimum duration required between the start of two consecutive memory operations. It dictates how frequently the memory can be accessed. \n\nLet me know if you'd like a breakdown of the other options and why they are incorrect! \n"
            }
        ]
    },
    {
        "doc_id": 696,
        "doc": {
            "question_id": -1,
            "question": "MFC is used to ____",
            "options": [
                "Issue a read signal",
                "Signal to the device that the memory read operation is complete",
                "Signal the processor the memory operation is complete",
                "Assign a device to perform the read operation"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "MFC is used to ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **MFC (Memory Function Complete)** is a signal used in computer architecture to indicate that a memory operation (read or write) initiated by the processor has been successfully completed by the memory controller. \n\nLet's break down why the other options are incorrect:\n\n* **A) Issue a read signal:**  Other control signals, like \"Read Enable,\" are used for this purpose.\n* **B) Signal to the device...**: MFC is specifically related to communication between the processor and the *memory controller*, not general I/O devices.\n* **D) Assign a device...**: Device assignment and management are handled through different mechanisms, not directly related to MFC. \n"
            }
        ]
    },
    {
        "doc_id": 697,
        "doc": {
            "question_id": -1,
            "question": "____ is the bottleneck, when it comes computer performance.",
            "options": [
                "Memory access time",
                "Memory cycle time",
                "Delay",
                "Latency"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is the bottleneck, when it comes computer performance.",
                "Expected Label": "B",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 698,
        "doc": {
            "question_id": -1,
            "question": "The logical addresses generated by the cpu are mapped onto physical memory by ____",
            "options": [
                "Relocation register",
                "TLB",
                "MMU",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The logical addresses generated by the cpu are mapped onto physical memory by ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **MMU (Memory Management Unit)** is the hardware component responsible for translating logical addresses to physical addresses. \n\nLet's break down why the other options are incorrect:\n\n* **Relocation Register:** This holds a base address offset used in simple memory management schemes, not the full translation mechanism.\n* **TLB (Translation Lookaside Buffer):** This is a cache within the MMU to speed up address translations. It doesn't perform the mapping itself, but rather stores recent translations. \n* **None of the mentioned:** This is incorrect, as the MMU is specifically designed for this purpose. \n"
            }
        ]
    },
    {
        "doc_id": 699,
        "doc": {
            "question_id": -1,
            "question": "VLSI stands for ____",
            "options": [
                "Very Large Scale Integration",
                "Very Large Stand-alone Integration",
                "Volatile Layer System Interface",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "VLSI stands for ____",
                "Expected Label": "A",
                "Predicted Label": "Answer: A \n"
            }
        ]
    },
    {
        "doc_id": 700,
        "doc": {
            "question_id": -1,
            "question": "The cells in a row are connected to a common line called ____",
            "options": [
                "Work line",
                "Word line",
                "Length line",
                "Principle diagonal"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The cells in a row are connected to a common line called ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 701,
        "doc": {
            "question_id": -1,
            "question": "The cells in each column are connected to ____",
            "options": [
                "Word line",
                "Data line",
                "Read line",
                "Sense/ Write line"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The cells in each column are connected to ____",
                "Expected Label": "D",
                "Predicted Label": "The answer is **A) Word line**. Here's why:\n\n* **Word line:**  In a memory structure like DRAM, each row of cells is connected to a single \"word line.\" When a specific word line is activated, it effectively \"reads\" the entire row of data.\n* **Data line:** Data lines are responsible for carrying the actual data being read from or written to the memory cells. \n* **Read line:** While a concept of a \"read\" signal exists, it's not typically how memory cells are directly connected.\n* **Sense/Write line:** Similar to the \"read line,\" a sense/write line is a functional signal but not how cells are directly connected within a column.\n\n**In summary:**  Word lines are the primary means of selecting which row (and therefore which set of cells) are being accessed within a memory structure. \n"
            }
        ]
    },
    {
        "doc_id": 702,
        "doc": {
            "question_id": -1,
            "question": "The word line is driven by the ____",
            "options": [
                "Chip select",
                "Address decoder",
                "Data line",
                "Control line"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The word line is driven by the ____",
                "Expected Label": "B",
                "Predicted Label": "B) Address decoder \n\n**Explanation:**\n\nThe address decoder is responsible for selecting the correct word line to activate based on the provided memory address. Here's why:\n\n* **Word Line:**  A word line is a single line that runs horizontally across a memory array, activating all the cells in a particular row simultaneously.\n* **Address Decoder:** This circuit takes the provided memory address and translates it into the appropriate word line selection signal.  \n\nLet me know if you'd like more details on how memory addressing works! \n"
            }
        ]
    },
    {
        "doc_id": 703,
        "doc": {
            "question_id": -1,
            "question": "A 16 X 8 Organisation of memory cells, can store upto ____",
            "options": [
                "256 bits",
                "1024 bits",
                "512 bits",
                "128 bits"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A 16 X 8 Organisation of memory cells, can store upto ____",
                "Expected Label": "D",
                "Predicted Label": "D) 128 bits \n\n**Explanation:**\n\n* **16 x 8 organization** means there are 16 memory locations (rows), and each location stores 8 bits (columns). \n* **Total storage capacity** is calculated by multiplying the number of locations by the bits per location: 16 x 8 = 128 bits. \n"
            }
        ]
    },
    {
        "doc_id": 704,
        "doc": {
            "question_id": -1,
            "question": "A memory organisation that can hold upto 1024 bits and has a minimum of 10 address lines can be organized into ____",
            "options": [
                "128 X 8",
                "256 X 4",
                "512 X 2",
                "1024 X 1"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A memory organisation that can hold upto 1024 bits and has a minimum of 10 address lines can be organized into ____",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 705,
        "doc": {
            "question_id": -1,
            "question": "Circuits that can hold their state as long as power is applied is ____",
            "options": [
                "Dynamic memory",
                "Static memory",
                "Register",
                "Cache"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Circuits that can hold their state as long as power is applied is ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Static memory**. Here's why:\n\n* **Static memory (SRAM)** uses transistors in a latching configuration to store data. As long as power is supplied, these latches retain their state.\n\nLet's look at the other options:\n\n* **Dynamic memory (DRAM)** stores data as electrical charges in capacitors. These charges leak over time, requiring periodic refreshing to maintain data integrity.  This means they don't hold their state indefinitely even with power applied.\n* **Registers** are small, fast storage locations within the CPU itself. While they hold data while powered, they are primarily used for immediate operations and are not considered bulk memory like the other options.\n* **Cache** is a type of memory designed for speed, typically implemented using SRAM. While it holds data as long as powered, the term \"cache\" refers to its role in the memory hierarchy, not the fundamental storage mechanism.\n\n**Therefore, the most accurate answer to the question is B) Static memory.** \n"
            }
        ]
    },
    {
        "doc_id": 706,
        "doc": {
            "question_id": -1,
            "question": "The number of external connections required in 16 X 8 memory organisation is ____",
            "options": [
                "14",
                "19",
                "15",
                "12"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The number of external connections required in 16 X 8 memory organisation is ____",
                "Expected Label": "A",
                "Predicted Label": "**C) 15**\n\n**Explanation:**\n\nIn a 16x8 memory organization:\n\n* **16** represents the number of memory locations.\n* **8** represents the number of data lines (bits) per location.\n\nTo address 16 locations, you need **4 address lines** (2^4 = 16). Additionally, you need **8 data lines** for input/output and **3 control lines** (Chip Select, Read/Write, Output Enable). \n\nTherefore, the total number of external connections required is 4 + 8 + 3 = **15**. \n"
            }
        ]
    },
    {
        "doc_id": 707,
        "doc": {
            "question_id": -1,
            "question": "The advantage of CMOS SRAM over the transistor one\u2019s is ____",
            "options": [
                "Low cost",
                "High efficiency",
                "High durability",
                "Low power consumption"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The advantage of CMOS SRAM over the transistor one\u2019s is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n\nCMOS SRAM is known for its **low power consumption**, especially in standby mode, compared to older transistor-based SRAM technologies. \n"
            }
        ]
    },
    {
        "doc_id": 708,
        "doc": {
            "question_id": -1,
            "question": "In a 4M-bit chip organisation has a total of 19 external connections.then it has ____ address if 8 data lines are there.",
            "options": [
                "10",
                "8",
                "9",
                "12"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a 4M-bit chip organisation has a total of 19 external connections.then it has ____ address if 8 data lines are there.",
                "Expected Label": "C",
                "Predicted Label": "Here's how to solve this:\n\n* **Total addressable memory:** 4 Mbits = 2^22 bits\n* **Data lines:** 8 (This means 2^8 = 256 data bits can be accessed at once)\n* **Address lines needed:** To access 2^22 bits, you need 22 address lines.\n* **Remaining external connections:** 19 total connections - 8 data lines = 11 connections\n* **Address lines from remaining connections:** Since we still need more than 11 address lines, some address lines must be multiplexed.  \n\n**However, the question is ambiguous:** It's unclear if the 19 external connections *include* the 8 data lines. \n\n**Let's consider both scenarios:**\n\n* **Scenario 1 (Connections INCLUDE data lines):** We already used up 8 for data, leaving only 11 for addressing. This isn't enough to address 4Mbits. This scenario is unlikely.\n* **Scenario 2 (Connections EXCLUDE data lines):** We have 19 lines solely for addressing. This is still not enough for 22 address lines, meaning multiplexing is used.\n\n**The problem is, we don't know the multiplexing scheme.**  We can't definitively say how many address lines are directly exposed.\n\n**Therefore, none of the provided answers are definitively correct.** The question lacks the information needed to give a precise answer. \n"
            }
        ]
    },
    {
        "doc_id": 709,
        "doc": {
            "question_id": -1,
            "question": "The Reason for the disregarding of the SRAM\u2019s is ____",
            "options": [
                "Low Efficiency",
                "High power consumption",
                "High Cost",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Reason for the disregarding of the SRAM\u2019s is ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) High Cost**. Here's why:\n\n* **SRAMs (Static Random Access Memory)** are known for their speed and use in applications like CPU caches where performance is critical. \n\n* While SRAMs might have slightly higher power consumption than some other memory types (like DRAM), it's not their primary drawback.\n\n* **The main reason SRAMs are not used everywhere is their high cost per bit compared to DRAM.** This limits their use to situations where speed is paramount and the required capacity is relatively small. \n\nLet me know if you have any other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 710,
        "doc": {
            "question_id": -1,
            "question": "The disadvantage of DRAM over SRAM is/are ____",
            "options": [
                "Lower data storage capacities",
                "Higher heat dissipation",
                "The cells are not static",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The disadvantage of DRAM over SRAM is/are ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 711,
        "doc": {
            "question_id": -1,
            "question": "The reason for the cells to lose their state over time is ____",
            "options": [
                "The lower voltage levels",
                "Usage of capacitors to store the charge",
                "Use of Shift registers",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The reason for the cells to lose their state over time is ____",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 712,
        "doc": {
            "question_id": -1,
            "question": "The capacitors lose the charge over time due to ____",
            "options": [
                "The leakage resistance of the capacitor",
                "The small current in the transistor after being turned on",
                "The defect of the capacitor",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The capacitors lose the charge over time due to ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 713,
        "doc": {
            "question_id": -1,
            "question": "____ circuit is used to restore the capacitor value.",
            "options": [
                "Sense amplify",
                "Signal amplifier",
                "Delta modulator",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ circuit is used to restore the capacitor value.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nA **sense amplifier** is used in Dynamic RAM (DRAM) to read the small charge stored in a capacitor, amplify it, and restore the capacitor to its original state (either fully charged or discharged). \n\nHere's why the other options are incorrect:\n\n* **Signal Amplifier:**  While used in various parts of a computer system, they are not specifically designed for capacitor restoration in DRAM.\n* **Delta Modulator:** This is a type of analog-to-digital converter and not related to DRAM capacitor restoration. \n* **None of the mentioned:** This is incorrect as the sense amplifier is the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 714,
        "doc": {
            "question_id": -1,
            "question": "To reduce the number of external connections required, we make use of ____",
            "options": [
                "De-multiplexer",
                "Multiplexer",
                "Encoder",
                "Decoder"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To reduce the number of external connections required, we make use of ____",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 715,
        "doc": {
            "question_id": -1,
            "question": "The processor must take into account the delay in accessing the memory location, such memories are called ____",
            "options": [
                "Delay integrated",
                "Asynchronous memories",
                "Synchronous memories",
                "Isochronous memories"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The processor must take into account the delay in accessing the memory location, such memories are called ____",
                "Expected Label": "B",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Isochronous memories** are designed with predictable and consistent access times. The processor knows exactly how long it takes to fetch data, eliminating the need for complex timing circuitry and allowing for efficient memory access. \n\nLet's break down why the other options are incorrect:\n\n* **Delay integrated:** This term isn't commonly used in computer architecture.\n* **Asynchronous memories:** These memories don't have a fixed clock signal, making access times variable.\n* **Synchronous memories:** These use a clock signal but may still have varying access times due to factors like cache misses. \n"
            }
        ]
    },
    {
        "doc_id": 716,
        "doc": {
            "question_id": -1,
            "question": "To get the row address of the required data ____ is enabled.",
            "options": [
                "CAS",
                "RAS",
                "CS",
                "Sense/write"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To get the row address of the required data ____ is enabled.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **RAS (Row Address Strobe)** is the signal used to latch the row address into the DRAM chip's internal row address register. This selects a specific row of memory cells. \n\nLet's break down the other options:\n\n* **CAS (Column Address Strobe)** is used *after* RAS to latch the column address, selecting the specific column within the already chosen row.\n* **CS (Chip Select)** enables or disables the entire memory chip, not a specific row.\n* **Sense/Write** controls the direction of data flow (reading or writing) to/from the memory chip.\n\n**Therefore, the correct answer is B.** \n"
            }
        ]
    },
    {
        "doc_id": 717,
        "doc": {
            "question_id": -1,
            "question": "In order to read multiple bytes of a row at the same time, we make use of ____",
            "options": [
                "Latch",
                "Shift register",
                "Cache",
                "Memory extension"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In order to read multiple bytes of a row at the same time, we make use of ____",
                "Expected Label": "A",
                "Predicted Label": "B) Shift register \n\nHere's why:\n\n* **Shift registers** are specifically designed to hold and shift data sequentially. By clocking data in, you can load multiple bytes and then output them individually or in a burst. This is how DRAM reads work internally.\n\nLet's look at why the other options are incorrect:\n\n* **Latch:** A latch holds a single value; it doesn't inherently support reading multiple bytes sequentially.\n* **Cache:** Cache stores frequently used data to speed up access, but it doesn't define the mechanism for reading multiple bytes from the underlying memory.\n* **Memory extension:** This is a general term for increasing memory capacity and doesn't relate to the specific operation of reading multiple bytes. \n"
            }
        ]
    },
    {
        "doc_id": 718,
        "doc": {
            "question_id": -1,
            "question": "The block transfer capability of the DRAM is called ____",
            "options": [
                "Burst mode",
                "Block mode",
                "Fast page mode",
                "Fast frame mode"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The block transfer capability of the DRAM is called ____",
                "Expected Label": "C",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 719,
        "doc": {
            "question_id": -1,
            "question": "The difference between DRAM\u2019s and SDRAM\u2019s is/are ____",
            "options": [
                "The DRAM\u2019s will not use the master slave relationship in data transfer",
                "The SDRAM\u2019s make use of clock",
                "The SDRAM\u2019s are more power efficient",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The difference between DRAM\u2019s and SDRAM\u2019s is/are ____",
                "Expected Label": "B",
                "Predicted Label": "**B) The SDRAM\u2019s make use of clock**\n\n**Explanation:**\n\n* **SDRAM (Synchronous DRAM)** is synchronized with the system clock, allowing it to operate faster and more efficiently than **DRAM (Dynamic RAM)**, which is asynchronous. \n* Option A is incorrect as both DRAM and SDRAM can use master-slave configurations.\n* Option C is debatable. While SDRAM can be *more* power efficient in some operations due to synchronous data transfer, DRAM technology itself has seen many power-saving advancements. It's not a defining difference.\n* Option D is incorrect because the clock synchronization is a key difference. \n"
            }
        ]
    },
    {
        "doc_id": 720,
        "doc": {
            "question_id": -1,
            "question": "The difference in the address and data connection between DRAM\u2019s and SDRAM\u2019s is ____",
            "options": [
                "The usage of more number of pins in SDRAM\u2019s",
                "The requirement of more address lines in SDRAM\u2019s",
                "The usage of a buffer in SDRAM\u2019s",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The difference in the address and data connection between DRAM\u2019s and SDRAM\u2019s is ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(D) None of the mentioned**. Here's why:\n\n* **Address and data lines are fundamentally the same.** Both DRAM and SDRAM use the same fundamental principles for addressing memory locations and transferring data. \n* **The key difference lies in timing and synchronization.** SDRAM (Synchronous DRAM) is synchronized with the system's clock, allowing it to operate at much faster speeds than asynchronous DRAM.  \n\nLet me know if you'd like more details on the differences between DRAM and SDRAM! \n"
            }
        ]
    },
    {
        "doc_id": 721,
        "doc": {
            "question_id": -1,
            "question": "A ____ is used to restore the contents of the cells.",
            "options": [
                "Sense amplifier",
                "Refresh counter",
                "Restorer",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ is used to restore the contents of the cells.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 722,
        "doc": {
            "question_id": -1,
            "question": "The mode register is used to ____",
            "options": [
                "Select the row or column data transfer mode",
                "Select the mode of operation",
                "Select mode of storing the data",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The mode register is used to ____",
                "Expected Label": "B",
                "Predicted Label": "B) Select the mode of operation \n"
            }
        ]
    },
    {
        "doc_id": 723,
        "doc": {
            "question_id": -1,
            "question": "The time taken to transfer a word of data to or from the memory is called as ____",
            "options": [
                "Access time",
                "Cycle time",
                "Memory latency",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The time taken to transfer a word of data to or from the memory is called as ____",
                "Expected Label": "C",
                "Predicted Label": "**C) Memory latency** \n\nWhile access time is related, memory latency is the more precise and encompassing term for the time it takes to transfer data to or from memory. \n\nHere's why:\n\n* **Memory Latency:** This refers to the total time delay from the moment a memory request is issued to the moment the data is available. It includes factors like queueing delays, refresh cycles (in DRAM), and the actual time to access and transfer the data.\n* **Access Time:** This is often used synonymously with latency, but it can specifically refer to the time it takes to access a particular memory location *after* any necessary setup or delays. \n\nLet me know if you'd like a deeper dive into any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 724,
        "doc": {
            "question_id": -1,
            "question": "The SDRAM performs operation on the ____",
            "options": [
                "Rising edge of the clock",
                "Falling edge of the clock",
                "Middle state of the clock",
                "Transition state of the clock"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The SDRAM performs operation on the ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nSDRAM (Synchronous Dynamic RAM) operates on the **rising edge of the clock signal**. This means that data is read or written to the memory cells when the clock signal transitions from a low voltage level to a high voltage level. \n"
            }
        ]
    },
    {
        "doc_id": 725,
        "doc": {
            "question_id": -1,
            "question": "DDR SDRAM\u2019s perform faster data transfer by ____",
            "options": [
                "Integrating the hardware",
                "Transferring on both edges",
                "Improving the clock speeds",
                "Increasing the bandwidth"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "DDR SDRAM\u2019s perform faster data transfer by ____",
                "Expected Label": "B",
                "Predicted Label": "B) Transferring on both edges \n\n**Explanation:**\n\nDDR SDRAM (Double Data Rate Synchronous Dynamic Random-Access Memory) doubles the data transfer rate compared to SDRAM by transferring data on both the rising and falling edges of the clock signal.  \n\nLet's look at why the other options are less accurate:\n\n* **A) Integrating the hardware:** While integration can improve performance in general, it's not the defining factor for DDR's speed advantage.\n* **C) Improving the clock speeds:** DDR can operate at higher clock speeds than SDRAM, but the primary reason for its speed increase is the double data rate transfer. \n* **D) Increasing the bandwidth:**  Increased bandwidth is a *result* of the faster data transfer (due to the double data rate), not the cause. \n"
            }
        ]
    },
    {
        "doc_id": 726,
        "doc": {
            "question_id": -1,
            "question": "To improve the data retrieval rate ____",
            "options": [
                "The memory is divided into two banks",
                "The hardware is changed",
                "The clock frequency is increased",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To improve the data retrieval rate ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nDividing memory into banks allows simultaneous access to different memory locations, effectively increasing the data retrieval rate. Here's why:\n\n* **Memory Interleaving:**  Memory banks are often used in a technique called interleaving. Data is distributed across the banks in a way that consecutive addresses are usually in different banks.  This is particularly beneficial for burst transfers, as you can fetch data from one bank while the other bank is still refreshing.\n\nLet's look at why the other options are not the best answers:\n\n* **B) The hardware is changed:** This is too general. While some hardware changes *could* improve data retrieval, it's not a guaranteed or specific method.\n* **C) The clock frequency is increased:** Increasing the clock frequency might speed up the entire system, but it doesn't directly address the speed of data retrieval from memory. Bottlenecks could still occur. \n* **D) None of the mentioned:** We've already established that option A is a valid technique. \n\n**Therefore, A is the most accurate and specific answer.** \n"
            }
        ]
    },
    {
        "doc_id": 727,
        "doc": {
            "question_id": -1,
            "question": "The chip can be disabled or cut off from an external connection using ____",
            "options": [
                "Chip select",
                "LOCK",
                "ACPT",
                "RESET"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The chip can be disabled or cut off from an external connection using ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 728,
        "doc": {
            "question_id": -1,
            "question": "To organise large memory chips we make use of ____",
            "options": [
                "Integrated chips",
                "Upgraded hardware",
                "Memory modules",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To organise large memory chips we make use of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 729,
        "doc": {
            "question_id": -1,
            "question": "The less space consideration as lead to the development of ____ (for large memories).",
            "options": [
                "SIMM\u2019s",
                "DIMS\u2019s",
                "SRAM\u2019s",
                "Both SIMM\u2019s and DIMS\u2019s"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The less space consideration as lead to the development of ____ (for large memories).",
                "Expected Label": "D",
                "Predicted Label": "D) Both SIMM\u2019s and DIMS\u2019s \n\n**Explanation:**\n\nWhile both SIMMs and DIMMs are designed to save space compared to earlier memory module designs, the question asks about \"less space consideration,\" implying a move away from those standards.  Here's why:\n\n* **SIMMs (Single In-line Memory Modules)** and **DIMMs (Dual In-line Memory Modules)** were indeed developed to address space constraints on motherboards.  \n* However, as the need for even larger memory capacities grew, these modules themselves became a limiting factor.\n* Modern systems often utilize technologies like **SO-DIMMs** (Small Outline DIMMs) for laptops and compact devices, or even **onboard memory** directly integrated onto the motherboard or CPU package for further space savings.\n\n**Therefore, \"less space consideration\" hints at a move beyond traditional SIMMs and DIMMs towards solutions emphasizing even greater integration and density.** \n"
            }
        ]
    },
    {
        "doc_id": 730,
        "doc": {
            "question_id": -1,
            "question": "The SRAM\u2019s are basically used as ____",
            "options": [
                "Registers",
                "Caches",
                "TLB",
                "Buffer"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The SRAM\u2019s are basically used as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 731,
        "doc": {
            "question_id": -1,
            "question": "The higher order bits of the address are used to ____",
            "options": [
                "Specify the row address",
                "Specify the column address",
                "Input the CS",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The higher order bits of the address are used to ____",
                "Expected Label": "A",
                "Predicted Label": "A) Specify the row address \n\n**Explanation:**\n\nIn memory organization, especially with DRAM (Dynamic RAM), the address space is divided into rows and columns.  The higher-order bits of an address typically select the **row**, while the lower-order bits select the **column** within that row. \n"
            }
        ]
    },
    {
        "doc_id": 732,
        "doc": {
            "question_id": -1,
            "question": "The address lines multiplexing is done using ____",
            "options": [
                "MMU",
                "Memory controller unit",
                "Page table",
                "Overlay generator"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The address lines multiplexing is done using ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe Memory Controller Unit (MCU) is responsible for address line multiplexing. This technique allows the CPU to address a larger memory space using a smaller number of physical address lines on the bus. \n\nHere's why the other options are incorrect:\n\n* **MMU (Memory Management Unit):** The MMU translates logical addresses to physical addresses, handling virtual memory. While related to memory addressing, it doesn't directly perform address line multiplexing.\n* **Page Table:** The page table is a data structure used by the MMU for virtual-to-physical address translation.\n* **Overlay Generator:** Overlay generation is a programming technique used in systems with limited memory, not directly related to hardware address multiplexing. \n"
            }
        ]
    },
    {
        "doc_id": 733,
        "doc": {
            "question_id": -1,
            "question": "The controller multiplexes the addresses after getting the ____ signal.",
            "options": [
                "INTR",
                "ACK",
                "RESET",
                "Request"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The controller multiplexes the addresses after getting the ____ signal.",
                "Expected Label": "D",
                "Predicted Label": "D) Request \n\n**Explanation:**\n\nThe controller uses the **Request** signal to determine when a device is ready to communicate. Once a request is received, the controller then multiplexes the appropriate addresses to handle the data transfer. \n\nLet's break down why the other options are incorrect:\n\n* **INTR (Interrupt):** This signal indicates that a device requires attention, but it doesn't directly trigger address multiplexing.\n* **ACK (Acknowledge):** This signal confirms that data has been successfully received. It's part of the data transfer process but not directly related to address multiplexing.\n* **RESET:**  This signal initializes the system and doesn't directly relate to the address multiplexing process. \n"
            }
        ]
    },
    {
        "doc_id": 734,
        "doc": {
            "question_id": -1,
            "question": "The RAS and CAS signals are provided by the ____",
            "options": [
                "Mode register",
                "CS",
                "Memory controller",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The RAS and CAS signals are provided by the ____",
                "Expected Label": "C",
                "Predicted Label": "**C) Memory controller** \n\n**Explanation:**\n\nRAS (Row Address Strobe) and CAS (Column Address Strobe) are signals used to access data in DRAM (Dynamic Random Access Memory). These signals are generated and managed by the **memory controller**, a dedicated circuit responsible for coordinating data transfer between the CPU and the memory. \n"
            }
        ]
    },
    {
        "doc_id": 735,
        "doc": {
            "question_id": -1,
            "question": "Consider a memory organised into 8K rows, and that it takes 4 cycles to complete a read operation. Then the refresh overhead of the chip is ____",
            "options": [
                "0.0021",
                "0.0038",
                "0.0064",
                "0.0128"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Consider a memory organised into 8K rows, and that it takes 4 cycles to complete a read operation. Then the refresh overhead of the chip is ____",
                "Expected Label": "B",
                "Predicted Label": "Here's how to break down the problem and arrive at the answer:\n\n**Understanding Memory Refresh**\n\n* **Dynamic RAM (DRAM)** requires periodic refreshing to retain data. This is because the electrical charges representing data leak over time.\n* **Refresh Overhead:**  The time spent refreshing the memory is overhead, as it takes away from the time available for normal read/write operations.\n\n**Calculations**\n\nWe need more information to calculate the refresh overhead accurately. Here's why:\n\n* **Refresh Rate:** We need to know how often each row needs to be refreshed (e.g., every few milliseconds). \n* **Refresh Cycles per Row:** How many cycles does it take to refresh a single row?\n\n**Without this information, we can't determine the exact refresh overhead.**\n\n**However, here's a conceptual explanation:**\n\n* **Refresh overhead is a percentage of total memory access time.** A higher refresh rate or longer refresh cycles per row will increase the overhead.\n* **The provided information (8K rows and 4 cycles per read) is insufficient** to calculate this percentage.\n\n**Therefore, none of the provided options (A, B, C, or D) can be definitively determined as correct.** \n\n**To get a definitive answer, you'll need the missing refresh rate and refresh cycle information.** \n"
            }
        ]
    },
    {
        "doc_id": 736,
        "doc": {
            "question_id": -1,
            "question": "RAMBUS is better than the other memory chips in terms of ____",
            "options": [
                "Efficiency",
                "Speed of operation",
                "Wider bandwidth",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "RAMBUS is better than the other memory chips in terms of ____",
                "Expected Label": "B",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 737,
        "doc": {
            "question_id": -1,
            "question": "The key feature of the RAMBUS tech is ____",
            "options": [
                "Greater memory utilisation",
                "Efficiency",
                "Speed of transfer",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The key feature of the RAMBUS tech is ____",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\nRAMBUS technology is primarily known for its high-speed data transfer capabilities. It achieves this through a narrow, high-frequency bus and differential signaling. \n\nLet's look at why the other options are less accurate:\n\n* **A) Greater memory utilization:** While RAMBUS might contribute indirectly to better utilization in some system designs, it's not its defining characteristic. \n* **B) Efficiency:**  RAMBUS can be power-hungry compared to some other memory technologies.\n* **D) None of the mentioned:** This is incorrect, as speed is the key feature. \n"
            }
        ]
    },
    {
        "doc_id": 738,
        "doc": {
            "question_id": -1,
            "question": "The increase in operation speed is done by ____",
            "options": [
                "Reducing the reference voltage",
                "Increasing the clk frequency",
                "Using enhanced hardware",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The increase in operation speed is done by ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **B) Increasing the clk frequency**. Here's why:\n\n* **Clock frequency** determines how fast a processor executes instructions. A higher clock frequency means more instructions can be processed per second, directly leading to faster operation speed.\n\nLet's look at why the other options are incorrect:\n\n* **A) Reducing the reference voltage:** While reducing voltage can lower power consumption and sometimes allow for slightly higher clock speeds, it's not a primary method for increasing operational speed and can lead to instability.\n* **C) Using enhanced hardware:**  Enhanced hardware can definitely improve performance, but it's a broad term. This might involve more cores, faster memory, etc., which contribute to overall performance rather than directly increasing operation speed in the same way a higher clock frequency does. \n* **D) None of the mentioned:** This is incorrect, as increasing clock frequency is a fundamental way to increase operation speed. \n\n**Therefore, B is the most accurate and direct answer to the question.** \n"
            }
        ]
    },
    {
        "doc_id": 739,
        "doc": {
            "question_id": -1,
            "question": "The data is transferred over the RAMBUS as ____",
            "options": [
                "Packets",
                "Blocks",
                "Swing voltages",
                "Bits"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The data is transferred over the RAMBUS as ____",
                "Expected Label": "C",
                "Predicted Label": "D) Bits \n\n**Explanation:**\n\nWhile RAMBUS architectures might use various packaging and signaling methods, the fundamental unit of data transfer remains the **bit**. \n\nLet me know if you'd like to explore the nuances of the other options! \n"
            }
        ]
    },
    {
        "doc_id": 740,
        "doc": {
            "question_id": -1,
            "question": "The type of signaling used in RAMBUS is ____",
            "options": [
                "CLK signaling",
                "Differential signaling",
                "Integral signaling",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The type of signaling used in RAMBUS is ____",
                "Expected Label": "B",
                "Predicted Label": "B) Differential signaling \n\n**Explanation:**\n\nRAMBUS memory technologies are known for using high-speed **differential signaling**. This method transmits data using two complementary signals, improving noise immunity and allowing for faster data rates compared to single-ended signaling (like CLK signaling). \n"
            }
        ]
    },
    {
        "doc_id": 741,
        "doc": {
            "question_id": -1,
            "question": "The special communication used in RAMBUS are ____",
            "options": [
                "RAMBUS channel",
                "D-link",
                "Dial-up",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The special communication used in RAMBUS are ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nRAMBUS (Rambus Dynamic Random Access Memory) technology utilizes a proprietary communication method called the **RAMBUS channel**. This high-speed channel facilitates data transfer between the RAMBUS memory modules and the memory controller. \n\nThe other options are incorrect:\n\n* **D-link:** A digital audio interface.\n* **Dial-up:** An outdated method for internet access. \n* **None of the mentioned:**  The question specifically asks about a communication method used by RAMBUS, making this option incorrect. \n"
            }
        ]
    },
    {
        "doc_id": 742,
        "doc": {
            "question_id": -1,
            "question": "The original design of the RAMBUS required for ____ data lines.",
            "options": [
                "4",
                "6",
                "8",
                "9"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The original design of the RAMBUS required for ____ data lines.",
                "Expected Label": "D",
                "Predicted Label": "D) 9 \n"
            }
        ]
    },
    {
        "doc_id": 743,
        "doc": {
            "question_id": -1,
            "question": "The RAMBUS requires specially designed memory chips similar to ____",
            "options": [
                "SRAM",
                "SDRAM",
                "DRAM",
                "DDRRAM"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The RAMBUS requires specially designed memory chips similar to ____",
                "Expected Label": "C",
                "Predicted Label": "B) SDRAM \n\nHere's why:\n\n* **RAMBUS (RDRAM)** was a memory technology designed for high bandwidth. It used a different signaling method than standard DRAM, requiring specialized memory chips.\n* **SDRAM (Synchronous DRAM)** is the closest analogy. While not identical to RDRAM, SDRAM also introduced specialized signaling and timing schemes compared to conventional DRAM, necessitating specific memory chips.\n\n**Let's look at the other options:**\n\n* **SRAM** is much faster but more expensive than DRAM and uses a completely different technology.\n* **DRAM** is the broad category, and RDRAM was a specific type, not directly comparable.\n* **DDRRAM** is an evolution of SDRAM and, while advanced, still falls under the SDRAM umbrella.\n\n**In summary, although RAMBUS had its unique implementation, it shared the commonality of requiring specialized memory chips similar to how SDRAM introduced a shift from conventional DRAM.** \n"
            }
        ]
    },
    {
        "doc_id": 744,
        "doc": {
            "question_id": -1,
            "question": "A RAMBUS which has 18 data lines is called as ____",
            "options": [
                "Extended RAMBUS",
                "Direct RAMBUS",
                "Multiple RAMBUS",
                "Indirect RAMBUS"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A RAMBUS which has 18 data lines is called as ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **(B) Direct RAMBUS**. Here's why:\n\n* **Direct RDRAM** configurations use the data lines directly for memory access.  The number of data lines (in this case 18) is a key characteristic of Direct RDRAM.\n\nLet's look at why the other options are incorrect:\n\n* **Extended RDRAM, Multiple RDRAM, and Indirect RDRAM** are not standard terms related to RAMBUS technology.  RAMBUS specifications focus on factors like data rate, bus width, and signaling methods, not these generalized terms. \n"
            }
        ]
    },
    {
        "doc_id": 745,
        "doc": {
            "question_id": -1,
            "question": "The RDRAM chips assembled into larger memory modules called ____",
            "options": [
                "RRIM",
                "DIMM",
                "SIMM",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The RDRAM chips assembled into larger memory modules called ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 746,
        "doc": {
            "question_id": -1,
            "question": "PROM stands for ____",
            "options": [
                "Programmable Read Only Memory",
                "Pre-fed Read Only Memory",
                "Pre-required Read Only Memory",
                "Programmed Read Only Memory"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "PROM stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 747,
        "doc": {
            "question_id": -1,
            "question": "The PROM is more effective than ROM chips in regard to ____",
            "options": [
                "Cost",
                "Memory management",
                "Speed of operation",
                "Both Cost and Speed of operation"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The PROM is more effective than ROM chips in regard to ____",
                "Expected Label": "D",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **PROM (Programmable Read-Only Memory)** is more expensive than **ROM (Read-Only Memory)** because it offers the flexibility of being programmed once by the user. \n\n* Both ROM and PROM offer similar performance in terms of speed and have no specific advantages in memory management.\n\nLet me know if you'd like more details on ROM or PROM! \n"
            }
        ]
    },
    {
        "doc_id": 748,
        "doc": {
            "question_id": -1,
            "question": "The difference between the EPROM and ROM circuitry is ____",
            "options": [
                "The usage of MOSFET\u2019s over transistors",
                "The usage of JFET\u2019s over transistors",
                "The usage of an extra transistor",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The difference between the EPROM and ROM circuitry is ____",
                "Expected Label": "C",
                "Predicted Label": "**D) None of the mentioned**\n\n**Explanation:**\n\nThe primary difference between EPROM (Erasable Programmable Read-Only Memory) and ROM (Read-Only Memory) lies in their **erasability and programmability**. \n\n* **ROM** is permanently programmed during manufacturing and cannot be altered later. \n* **EPROM**, on the other hand, can be erased and reprogrammed multiple times. This is achieved through a different internal structure involving a special type of transistor that can hold an electrical charge representing data.\n\nOptions A, B, and C are incorrect as they don't accurately describe the fundamental difference between EPROM and ROM. While the specific transistor types used might vary depending on the manufacturing process and technology, the core distinction lies in the erasability and re-programmability aspect. \n"
            }
        ]
    },
    {
        "doc_id": 749,
        "doc": {
            "question_id": -1,
            "question": "The ROM chips are mainly used to store ____",
            "options": [
                "System files",
                "Root directories",
                "Boot files",
                "Driver files"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ROM chips are mainly used to store ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 750,
        "doc": {
            "question_id": -1,
            "question": "The contents of the EPROM are erased by ____",
            "options": [
                "Overcharging the chip",
                "Exposing the chip to UV rays",
                "Exposing the chip to IR rays",
                "Discharging the Chip"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The contents of the EPROM are erased by ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 751,
        "doc": {
            "question_id": -1,
            "question": "The disadvantage of the EPROM chip is ____",
            "options": [
                "The high cost factor",
                "The low efficiency",
                "The low speed of operation",
                "The need to remove the chip physically to reprogram it"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The disadvantage of the EPROM chip is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 752,
        "doc": {
            "question_id": -1,
            "question": "The disadvantage of the EEPROM is/are ____",
            "options": [
                "The requirement of different voltages to read, write and store information",
                "The Latency read operation",
                "The inefficient memory mapping schemes used",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The disadvantage of the EEPROM is/are ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 753,
        "doc": {
            "question_id": -1,
            "question": "The memory devices which are similar to EEPROM but differ in the cost effectiveness is ____",
            "options": [
                "Memory sticks",
                "Blue-ray devices",
                "Flash memory",
                "CMOS"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The memory devices which are similar to EEPROM but differ in the cost effectiveness is ____",
                "Expected Label": "C",
                "Predicted Label": "C. Flash memory \n\n**Explanation:**\n\n* **Flash memory** is closely related to EEPROM in that they both offer non-volatile storage (data persists even when power is off) and allow for electrical erasing and reprogramming. However, flash memory is significantly more cost-effective, especially at larger storage capacities, which is why it's widely used in SSDs, USB drives, and memory cards.\n\nLet's look at why the other options aren't the best fit:\n\n* **Memory sticks (A):** This is a broad term, often referring to USB flash drives, which themselves utilize flash memory.\n* **Blue-ray devices (B):** These are optical storage media, fundamentally different in technology and cost structure compared to EEPROM.\n* **CMOS (D):**  CMOS (Complementary Metal-Oxide-Semiconductor) is a technology used for constructing integrated circuits, including various types of memory, but it's not a direct counterpart to EEPROM in terms of cost-effectiveness. \n"
            }
        ]
    },
    {
        "doc_id": 754,
        "doc": {
            "question_id": -1,
            "question": "The flash memories find application in ____",
            "options": [
                "Super computers",
                "Mainframe systems",
                "Distributed systems",
                "Portable devices"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The flash memories find application in ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 755,
        "doc": {
            "question_id": -1,
            "question": "The memory module obtained by placing a number of flash chips for higher memory storage called as ____",
            "options": [
                "FIMM",
                "SIMM",
                "Flash card",
                "RIMM"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The memory module obtained by placing a number of flash chips for higher memory storage called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Flash card** is the correct answer. \n\nHere's why:\n\n* **Flash cards** are storage devices that utilize flash memory chips to provide non-volatile storage. They come in various form factors and capacities, often used in cameras, phones, and as expansion slots.\n\nThe other options are incorrect:\n\n* **FIMM** is not a standard term in memory technology.\n* **SIMM** (Single In-line Memory Module) and **RIMM** (Rambus In-line Memory Module) are older memory module types primarily used for RAM, not flash storage. \n"
            }
        ]
    },
    {
        "doc_id": 756,
        "doc": {
            "question_id": -1,
            "question": "The flash memory modules designed to replace the functioning of a hard disk is ____",
            "options": [
                "RIMM",
                "Flash drives",
                "FIMM",
                "DIMM"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The flash memory modules designed to replace the functioning of a hard disk is ____",
                "Expected Label": "B",
                "Predicted Label": "B) Flash drives \n"
            }
        ]
    },
    {
        "doc_id": 757,
        "doc": {
            "question_id": -1,
            "question": "The reason for the fast operating speeds of the flash drives is ____",
            "options": [
                "The absence of any movable parts",
                "The integrated electronic hardware",
                "The improved bandwidth connection",
                "All of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The reason for the fast operating speeds of the flash drives is ____",
                "Expected Label": "A",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 758,
        "doc": {
            "question_id": -1,
            "question": "The standard SRAM chips are costly as ____",
            "options": [
                "They use highly advanced micro-electronic devices",
                "They house 6 transistor per chip",
                "They require specially designed PCB\u2019s",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The standard SRAM chips are costly as ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **D) None of the mentioned**. Here's why:\n\n* **SRAM chips are costly because they use a more complex circuitry (6 transistors) per bit of data storage compared to DRAM.**  While they use advanced micro-electronic fabrication processes, this is true for most modern chips and isn't the primary reason for SRAM's cost.\n\nLet's break down why the other options are incorrect:\n\n* **A) They use highly advanced micro-electronic devices:** While true, this is a general characteristic of many chip types, not just SRAM.\n* **B) They house 6 transistor per chip:** This is incorrect. SRAM uses 6 transistors *per bit*, not per chip. \n* **C) They require specially designed PCB\u2019s:** SRAM chips use standard packaging and PCB design techniques.\n\n**Therefore, none of the provided options accurately explain why SRAM chips are costly.** \n"
            }
        ]
    },
    {
        "doc_id": 759,
        "doc": {
            "question_id": -1,
            "question": "The drawback of building a large memory with DRAM is ____",
            "options": [
                "The large cost factor",
                "The inefficient memory organisation",
                "The Slow speed of operation",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The drawback of building a large memory with DRAM is ____",
                "Expected Label": "C",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 760,
        "doc": {
            "question_id": -1,
            "question": "The fastest data access is provided using ____",
            "options": [
                "Caches",
                "DRAM\u2019s",
                "SRAM\u2019s",
                "Registers"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The fastest data access is provided using ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 761,
        "doc": {
            "question_id": -1,
            "question": "The memory which is used to store the copy of data or instructions stored in larger memories, inside the CPU is called ____",
            "options": [
                "Level 1 cache",
                "Level 2 cache",
                "Registers",
                "TLB"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The memory which is used to store the copy of data or instructions stored in larger memories, inside the CPU is called ____",
                "Expected Label": "A",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 762,
        "doc": {
            "question_id": -1,
            "question": "The larger memory placed between the primary cache and the memory is called ____",
            "options": [
                "Level 1 cache",
                "Level 2 cache",
                "EEPROM",
                "TLB"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The larger memory placed between the primary cache and the memory is called ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 763,
        "doc": {
            "question_id": -1,
            "question": "The next level of memory hierarchy after the L2 cache is ____",
            "options": [
                "Secondary storage",
                "TLB",
                "Main memory",
                "Register"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The next level of memory hierarchy after the L2 cache is ____",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 764,
        "doc": {
            "question_id": -1,
            "question": "The last on the hierarchy scale of memory devices is ____",
            "options": [
                "Main memory",
                "Secondary memory",
                "TLB",
                "Flash drives"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The last on the hierarchy scale of memory devices is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 765,
        "doc": {
            "question_id": -1,
            "question": "The reason for the implementation of the cache memory is ____",
            "options": [
                "To increase the internal memory of the system",
                "The difference in speeds of operation of the processor and memory",
                "To reduce the memory access and cycle time",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The reason for the implementation of the cache memory is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 766,
        "doc": {
            "question_id": -1,
            "question": "The effectiveness of the cache memory is based on the property of ____",
            "options": [
                "Locality of reference",
                "Memory localisation",
                "Memory size",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The effectiveness of the cache memory is based on the property of ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 767,
        "doc": {
            "question_id": -1,
            "question": "The temporal aspect of the locality of reference means ____",
            "options": [
                "That the recently executed instruction won\u2019t be executed soon",
                "That the recently executed instruction is temporarily not referenced",
                "That the recently executed instruction will be executed soon again",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The temporal aspect of the locality of reference means ____",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\nTemporal locality refers to the tendency of a program to access the same data or instructions that have been recently accessed. So, the correct answer is **C) That the recently executed instruction will be executed soon again**.\n\nLet's break down why the other options are incorrect:\n\n* **A) That the recently executed instruction won\u2019t be executed soon:**  This contradicts the concept of temporal locality.\n* **B) That the recently executed instruction is temporarily not referenced:**  While there might be a brief period where an instruction isn't referenced, temporal locality focuses on the likelihood of it being accessed again soon.\n* **D) None of the mentioned:**  Option C accurately describes temporal locality.\n\n**In essence, temporal locality is leveraged by caches to store recently used data/instructions, making future accesses faster.** \n"
            }
        ]
    },
    {
        "doc_id": 768,
        "doc": {
            "question_id": -1,
            "question": "The spatial aspect of the locality of reference means ____",
            "options": [
                "That the recently executed instruction is executed again next",
                "That the recently executed won\u2019t be executed again",
                "That the instruction executed will be executed at a later time",
                "That the instruction in close proximity of the instruction executed will be executed in future"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The spatial aspect of the locality of reference means ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 769,
        "doc": {
            "question_id": -1,
            "question": "The correspondence between the main memory blocks and those in the cache is given by ____",
            "options": [
                "Hash function",
                "Mapping function",
                "Locale function",
                "Assign function"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The correspondence between the main memory blocks and those in the cache is given by ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Mapping function** determines how main memory blocks are mapped to cache blocks. \n\nHere's why the other options are incorrect:\n\n* **Hash function:** Used for indexing hash tables, not directly for cache mapping.\n* **Locale function:** Deals with regional settings in software.\n* **Assign function:**  A general term, not specific to memory mapping. \n"
            }
        ]
    },
    {
        "doc_id": 770,
        "doc": {
            "question_id": -1,
            "question": "The algorithm to remove and place new contents into the cache is called ____",
            "options": [
                "Replacement algorithm",
                "Renewal algorithm",
                "Updation",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The algorithm to remove and place new contents into the cache is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 771,
        "doc": {
            "question_id": -1,
            "question": "The write-through procedure is used ____",
            "options": [
                "To write onto the memory directly",
                "To write and read from memory simultaneously",
                "To write directly on the memory and the cache simultaneously",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The write-through procedure is used ____",
                "Expected Label": "C",
                "Predicted Label": "C) To write directly on the memory and the cache simultaneously \n\n**Explanation:**\n\nWrite-through is a cache coherence strategy where data is written to both the cache and the main memory **at the same time**.  This ensures that the main memory always holds the most up-to-date data. \n"
            }
        ]
    },
    {
        "doc_id": 772,
        "doc": {
            "question_id": -1,
            "question": "The bit used to signify that the cache location is updated is ____",
            "options": [
                "Dirty bit",
                "Update bit",
                "Reference bit",
                "Flag bit"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The bit used to signify that the cache location is updated is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Dirty bit (A):** This bit is set to 1 when the data in the cache block is modified (written to). This indicates that the cache block holds data different from the main memory, and the modified data needs to be written back to the main memory before the block is replaced.\n\nLet's look at why the other options are incorrect:\n\n* **Update bit (B):** There isn't a standard \"update bit\" specifically used for this purpose in cache memory terminology.\n* **Reference bit (C):** This bit is used in cache replacement algorithms. It's set to 1 when the cache block is accessed. It's not directly related to signifying an update.\n* **Flag bit (D):** This is a generic term for a bit used as an indicator. While the dirty bit is technically a flag bit, \"flag bit\" itself is not the specific term used for this purpose. \n\n**Therefore, \"Dirty bit\" is the most accurate and specific answer.** \n"
            }
        ]
    },
    {
        "doc_id": 773,
        "doc": {
            "question_id": -1,
            "question": "The copy-back protocol is used ____",
            "options": [
                "To copy the contents of the memory onto the cache",
                "To update the contents of the memory from the cache",
                "To remove the contents of the cache and push it on to the memory",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The copy-back protocol is used ____",
                "Expected Label": "B",
                "Predicted Label": "B) To update the contents of the memory from the cache \n\n**Explanation:**\n\nThe copy-back protocol (also known as write-back) focuses on updating main memory only when necessary. Here's how it works:\n\n* **Data is modified in the cache:** When data in the cache is modified, the change is marked (often with a \"dirty\" bit) but not immediately written back to the main memory. \n* **Cache block eviction:**  When the cache needs space for new data, a \"dirty\" block (one that has been modified) is written back to the main memory before being replaced.\n\n**Why other options are incorrect:**\n\n* **A) To copy the contents of the memory onto the cache:** This describes the process of *loading* data into the cache, which is a general aspect of caching, not specific to the copy-back protocol.\n* **C) To remove the contents of the cache and push it onto the memory:** This is partially correct for \"dirty\" blocks in the copy-back protocol but not the only action it governs.\n* **D) None of the mentioned:** Option B accurately describes the core function of the copy-back protocol. \n"
            }
        ]
    },
    {
        "doc_id": 774,
        "doc": {
            "question_id": -1,
            "question": "The approach where the memory contents are transferred directly to the processor from the memory is called ____",
            "options": [
                "Read-later",
                "Read-through",
                "Early-start",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The approach where the memory contents are transferred directly to the processor from the memory is called ____",
                "Expected Label": "C",
                "Predicted Label": "**B) Read-through** \n\n**Explanation:**\n\nRead-through is a cache mechanism where data is fetched from the main memory and simultaneously loaded into the cache. This ensures the processor directly receives the data from the main memory even on a cache miss.\n\nLet me know if you'd like explanations for the other options! \n"
            }
        ]
    },
    {
        "doc_id": 775,
        "doc": {
            "question_id": -1,
            "question": "The memory blocks are mapped on to the cache with the help of ____",
            "options": [
                "Hash functions",
                "Vectors",
                "Mapping functions",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The memory blocks are mapped on to the cache with the help of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nMapping functions determine how memory blocks are placed into cache lines.  Here's why the other options are incorrect:\n\n* **A) Hash functions:** While used in some cache structures (like victim caches), they are not the primary mechanism for general memory-to-cache mapping.\n* **B) Vectors:** Vectors are data structures, not memory mapping mechanisms.\n* **D) None of the mentioned:**  This is incorrect as mapping functions are essential for cache operation. \n\n**Therefore, the most accurate answer is C. ** \n"
            }
        ]
    },
    {
        "doc_id": 776,
        "doc": {
            "question_id": -1,
            "question": "During a write operation if the required block is not present in the cache then ____ occurs.",
            "options": [
                "Write latency",
                "Write hit",
                "Write delay",
                "Write miss"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "During a write operation if the required block is not present in the cache then ____ occurs.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 777,
        "doc": {
            "question_id": -1,
            "question": "In ____ protocol the information is directly written into the main memory.",
            "options": [
                "Write through",
                "Write back",
                "Write first",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In ____ protocol the information is directly written into the main memory.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Write through**. \n\nHere's why:\n\n* **Write through:** In this protocol, data is written to both the cache and the main memory simultaneously. This ensures data consistency but can be slower due to the extra write operation.\n* **Write back:** Data is written only to the cache. The modified cache block is written back to main memory only when it needs to be replaced. This is faster but requires a more complex mechanism to track modified data.\n* **Write first:** This is not a standard cache write protocol.\n\nLet me know if you'd like more details on cache write policies! \n"
            }
        ]
    },
    {
        "doc_id": 778,
        "doc": {
            "question_id": -1,
            "question": "The only draw back of using the early start protocol is ____",
            "options": [
                "Time delay",
                "Complexity of circuit",
                "Latency",
                "High miss rate"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The only draw back of using the early start protocol is ____",
                "Expected Label": "B",
                "Predicted Label": "**D) High miss rate** \n\n**Explanation:**\n\nThe early start protocol in computer architecture aims to improve performance by allowing data forwarding even before a cache miss is fully resolved. While this can hide some latency, it carries the risk of starting to process incorrect data if the requested data is ultimately not found in the cache (a cache miss). This can lead to a higher miss rate as the pipeline might proceed with incorrect assumptions. \n"
            }
        ]
    },
    {
        "doc_id": 779,
        "doc": {
            "question_id": -1,
            "question": "The method of mapping the consecutive memory blocks to consecutive cache blocks is called ____",
            "options": [
                "Set associative",
                "Associative",
                "Direct",
                "Indirect"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The method of mapping the consecutive memory blocks to consecutive cache blocks is called ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 780,
        "doc": {
            "question_id": -1,
            "question": "While using the direct mapping technique, in a 16 bit system the higher order 5 bits are used for ____",
            "options": [
                "Tag",
                "Block",
                "Word",
                "Id"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "While using the direct mapping technique, in a 16 bit system the higher order 5 bits are used for ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Tag**. Here's why:\n\n* **Direct Mapping:** In direct mapping, each block of main memory maps to one specific cache block. The address is divided into three parts:\n    * **Tag:**  Used to identify if the desired block from main memory is actually present in the cache block.\n    * **Index:** Used to select the specific cache block.\n    * **Offset:** Used to locate the desired word within the selected cache block.\n\n* **16-bit System and 5 Higher Order Bits:**  In a 16-bit system, the address has 16 bits.  If the higher 5 bits are used for the tag, the remaining bits are used for the index and offset.\n\n**Let's illustrate with an example:**\n\nImagine a cache with 8 blocks (2^3). You need 3 bits for the index to address all the blocks. For a 16-bit address, this leaves 13 bits (16 - 3) for the tag and offset combined.\n\nLet me know if you'd like a deeper dive into cache addressing techniques! \n"
            }
        ]
    },
    {
        "doc_id": 781,
        "doc": {
            "question_id": -1,
            "question": "In associative mapping, in a 16 bit system the tag field has ____ bits.",
            "options": [
                "12",
                "8",
                "9",
                "10"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In associative mapping, in a 16 bit system the tag field has ____ bits.",
                "Expected Label": "A",
                "Predicted Label": "The answer depends on the size of the cache and block size, which isn't given in the question.  **Therefore, there is no correct answer from the options provided.** \n\nHere's why:\n\n* **Associative mapping** uses the tag field to identify which block of main memory is stored in a particular cache location. \n* The size of the tag field is calculated as: \n   **Tag bits = Total address bits - Index bits - Offset bits**\n    * **Total address bits:** 16 in this case.\n    * **Index bits:** Determined by the number of sets in the cache (which we don't know).\n    * **Offset bits:** Determined by the block size (which we don't know).\n\n**Without knowing the cache size and block size, we cannot determine the tag field size.** \n"
            }
        ]
    },
    {
        "doc_id": 782,
        "doc": {
            "question_id": -1,
            "question": "The technique of searching for a block by going through all the tags is ____",
            "options": [
                "Linear search",
                "Binary search",
                "Associative search",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The technique of searching for a block by going through all the tags is ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(A) Linear search**. Here's why:\n\n* **Linear Search:** Involves checking each tag one by one until a match is found or the entire set is exhausted. This is the simplest but potentially slowest method for cache tag comparison.\n\n* **Binary Search:** Requires a sorted dataset.  Cache tags are not inherently sorted, making binary search inapplicable.\n\n* **Associative Search:** (Also called content-addressable memory) allows simultaneous comparison with all tags, making it the fastest but most complex and expensive to implement. \n\nLet me know if you'd like more details on cache organization and search techniques! \n"
            }
        ]
    },
    {
        "doc_id": 783,
        "doc": {
            "question_id": -1,
            "question": "In set-associative technique, the blocks are grouped into ____ sets.",
            "options": [
                "4",
                "8",
                "12",
                "6"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In set-associative technique, the blocks are grouped into ____ sets.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **not** A, B, C, or D. Here's why:\n\n* **Set-associative caching doesn't have a fixed number of sets.** The number of sets is a fundamental design parameter of a set-associative cache. \n\nHere's how it works:\n\n* **Direct Mapped:** Each memory block maps to one specific cache block.\n* **Fully Associative:** A memory block can be placed in any cache block.\n* **Set-Associative:**  A compromise between the two. Memory blocks are divided into sets. A block can go into any cache line within its assigned set.\n\n**Example:**  A 2-way set-associative cache with 8 cache blocks would have 4 sets.\n\n**Therefore, there's no single correct answer to the question as presented.** \n"
            }
        ]
    },
    {
        "doc_id": 784,
        "doc": {
            "question_id": -1,
            "question": "A control bit called ____ has to be provided to each block in set-associative.",
            "options": [
                "Idol bit",
                "Valid bit",
                "Reference bit",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A control bit called ____ has to be provided to each block in set-associative.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Valid bit:** In set-associative caches, each block within a set needs a valid bit to indicate whether the block holds meaningful data (valid bit = 1) or is empty (valid bit = 0). This is crucial for the cache to function correctly.\n\nLet's look at why the other options aren't the best fit:\n\n* **Idol bit:** This is not a standard term used in cache memory.\n* **Reference bit:**  Reference bits (often part of a Least Recently Used - LRU replacement policy) are used to track which blocks have been accessed recently, but they don't determine if a block holds valid data.\n* **All of the mentioned:** While reference bits might be present in a set-associative cache, they are not mandatory for its basic functionality like the valid bit. \n"
            }
        ]
    },
    {
        "doc_id": 785,
        "doc": {
            "question_id": -1,
            "question": "The bit used to indicate whether the block was recently used or not is ____",
            "options": [
                "Idol bit",
                "Control bit",
                "Reference bit",
                "Dirty bit"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The bit used to indicate whether the block was recently used or not is ____",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 786,
        "doc": {
            "question_id": -1,
            "question": "Data which is not up-to date is called as ____",
            "options": [
                "Spoilt data",
                "Stale data",
                "Dirty data",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Data which is not up-to date is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 787,
        "doc": {
            "question_id": -1,
            "question": "The main memory is structured into modules each with its own address register called ____",
            "options": [
                "ABR",
                "TLB",
                "PC",
                "IR"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The main memory is structured into modules each with its own address register called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**ABR (Address Buffer Register)** is the register within a memory module that holds the address being accessed. \n\nHere's why the other options are incorrect:\n\n* **TLB (Translation Lookaside Buffer)** is a cache used to speed up virtual address translation.\n* **PC (Program Counter)** holds the address of the next instruction to be fetched.\n* **IR (Instruction Register)** holds the currently executing instruction. \n"
            }
        ]
    },
    {
        "doc_id": 788,
        "doc": {
            "question_id": -1,
            "question": "In memory interleaving, the lower order bits of the address is used to ____",
            "options": [
                "Get the data",
                "Get the address of the module",
                "Get the address of the data within the module",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In memory interleaving, the lower order bits of the address is used to ____",
                "Expected Label": "B",
                "Predicted Label": "B) Get the address of the module \n\n**Explanation:**\n\nMemory interleaving distributes consecutive memory addresses across multiple memory modules. This allows for faster access to sequential data. \n\nHere's how it works:\n\n* **Lower-order bits:** These bits determine which module the data resides in. \n* **Higher-order bits:** These bits determine the location (address) of the data *within* the selected module.\n\nLet me know if you'd like a more detailed explanation of memory interleaving! \n"
            }
        ]
    },
    {
        "doc_id": 789,
        "doc": {
            "question_id": -1,
            "question": "The number successful accesses to memory stated as a fraction is called as ____",
            "options": [
                "Hit rate",
                "Miss rate",
                "Success rate",
                "Access rate"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The number successful accesses to memory stated as a fraction is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 790,
        "doc": {
            "question_id": -1,
            "question": "The number failed attempts to access memory, stated in the form of a fraction is called as ____",
            "options": [
                "Hit rate",
                "Miss rate",
                "Failure rate",
                "Delay rate"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The number failed attempts to access memory, stated in the form of a fraction is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 791,
        "doc": {
            "question_id": -1,
            "question": "In associative mapping during LRU, the counter of the new block is set to \u20180\u2019 and all the others are incremented by one, when ____ occurs.",
            "options": [
                "Delay",
                "Miss",
                "Hit",
                "Delayed hit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In associative mapping during LRU, the counter of the new block is set to \u20180\u2019 and all the others are incremented by one, when ____ occurs.",
                "Expected Label": "B",
                "Predicted Label": "**B) Miss** \n\n**Explanation:**\n\nIn associative mapping with Least Recently Used (LRU) replacement policy:\n\n* **Hit:**  The cache block is found. No counter updates are needed because the accessed block is already identified as the most recently used.\n* **Miss:** The cache block is not found. This means a new block needs to be brought in.  \n    * The counter of the new block is set to '0' to mark it as the most recently used.\n    * All other counters are incremented by one to reflect their relative usage becoming older compared to the new block. \n\n**Therefore, counter updates in associative mapping with LRU happen during a cache miss.** \n"
            }
        ]
    },
    {
        "doc_id": 792,
        "doc": {
            "question_id": -1,
            "question": "In LRU, the referenced blocks counter is set to\u20190\u2032 and that of the previous blocks are incremented by one and others remain same, in the case of ____",
            "options": [
                "Hit",
                "Miss",
                "Delay",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In LRU, the referenced blocks counter is set to\u20190\u2032 and that of the previous blocks are incremented by one and others remain same, in the case of ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 793,
        "doc": {
            "question_id": -1,
            "question": "The extra time needed to bring the data into memory in case of a miss is called as ____",
            "options": [
                "Delay",
                "Propagation time",
                "Miss penalty",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The extra time needed to bring the data into memory in case of a miss is called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 794,
        "doc": {
            "question_id": -1,
            "question": "The CPU is also called as ____",
            "options": [
                "Processor hub",
                "ISP",
                "Controller",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The CPU is also called as ____",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 795,
        "doc": {
            "question_id": -1,
            "question": "The PC gets incremented ____",
            "options": [
                "After the instruction decoding",
                "After the IR instruction gets executed",
                "After the fetch cycle",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The PC gets incremented ____",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) After the fetch cycle**. Here's why:\n\n* **Fetch, Decode, Execute:** The basic operation of a computer follows a fetch-decode-execute cycle.\n* **Fetch Cycle:** During the fetch cycle, the CPU retrieves the next instruction from memory. The address of this instruction is stored in the Program Counter (PC).\n* **Incrementing the PC:**  Immediately after fetching the instruction, the PC is incremented to point to the address of the *next* instruction in the sequence. \n\nLet me know if you'd like more details on the fetch-decode-execute cycle! \n"
            }
        ]
    },
    {
        "doc_id": 796,
        "doc": {
            "question_id": -1,
            "question": "Which register in the processor is single directional?",
            "options": [
                "MAR",
                "MDR",
                "PC",
                "Temp"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which register in the processor is single directional?",
                "Expected Label": "A",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **PC (Program Counter):** The PC holds the address of the *next* instruction to be fetched. It's inherently single-directional because the flow of instructions generally moves forward. While jumps and branches modify the PC, they still dictate the *next* instruction.\n\nLet's examine why the others are incorrect:\n\n* **MAR (Memory Address Register):** The MAR holds the address of the memory location to be accessed for reading *or* writing data. \n* **MDR (Memory Data Register):** The MDR can buffer data being written *to* memory or read *from* memory.\n* **Temp:** Temporary registers are general-purpose and can be written to and read from. \n"
            }
        ]
    },
    {
        "doc_id": 797,
        "doc": {
            "question_id": -1,
            "question": "The transparent register/s is/are ____",
            "options": [
                "Y",
                "Z",
                "Temp",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The transparent register/s is/are ____",
                "Expected Label": "D",
                "Predicted Label": "The answer is **C) Temp**. Here's why:\n\n* **Transparent registers** are internal to the CPU's architecture. They are used by the CPU during operations but are not directly accessible by the programmer.  \n* **Temp registers** are a common example of transparent registers. They are used temporarily by the CPU for intermediate calculations or data movements.\n\nThe other options are incorrect because:\n\n* **Y and Z** are not standard names for transparent registers. They might represent something else depending on the specific architecture.\n* While **all of the mentioned** could potentially be transparent registers in a specific CPU design, it's not a universally true statement. \n\n**Therefore, \"Temp\" is the most accurate and generally applicable answer.** \n"
            }
        ]
    },
    {
        "doc_id": 798,
        "doc": {
            "question_id": -1,
            "question": "Which register is connected to the MUX?",
            "options": [
                "Y",
                "Z",
                "R0",
                "Temp"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which register is connected to the MUX?",
                "Expected Label": "A",
                "Predicted Label": "The answer cannot be determined without a diagram or further context. \n\n**Explanation:**\n\nThe question asks about a connection to a MUX (multiplexer). Multiplexers select one input from multiple sources based on a select signal. To determine which register connects to the MUX, you need information about:\n\n* **The MUX itself:**  How many inputs does it have? What is its purpose in the circuit?\n* **The registers:** What are their roles in the system? Are there any hints about data flow?\n\n**Without a diagram or more context, it's impossible to give a definitive answer.** \n"
            }
        ]
    },
    {
        "doc_id": 799,
        "doc": {
            "question_id": -1,
            "question": "The registers, ALU and the interconnecting path together are called as ____",
            "options": [
                "Control path",
                "Flow path",
                "Data path",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The registers, ALU and the interconnecting path together are called as ____",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\nThe **data path** is the heart of a CPU where data manipulation occurs.  Here's why:\n\n* **Registers:** Hold the data being actively processed.\n* **ALU (Arithmetic Logic Unit):** Performs calculations and logical operations on the data.\n* **Interconnecting Path:**  The buses and connections that allow data to flow between registers, the ALU, and other components within the data path. \n\nLet me know if you'd like a deeper dive into any of these components! \n"
            }
        ]
    },
    {
        "doc_id": 800,
        "doc": {
            "question_id": -1,
            "question": "The input and output of the registers are governed by ____",
            "options": [
                "Transistors",
                "Diodes",
                "Gates",
                "Switches"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The input and output of the registers are governed by ____",
                "Expected Label": "D",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 801,
        "doc": {
            "question_id": -1,
            "question": "When two or more clock cycles are used to complete data transfer it is called as ____",
            "options": [
                "Single phase clocking",
                "Multi-phase clocking",
                "Edge triggered clocking",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When two or more clock cycles are used to complete data transfer it is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 802,
        "doc": {
            "question_id": -1,
            "question": "____ signal is used to show complete of memory operation.",
            "options": [
                "MFC",
                "WMFC",
                "CFC",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ signal is used to show complete of memory operation.",
                "Expected Label": "A",
                "Predicted Label": "**A) MFC**\n\n**MFC (Memory Function Complete)** signal is used to indicate the completion of a memory operation. \n"
            }
        ]
    },
    {
        "doc_id": 803,
        "doc": {
            "question_id": -1,
            "question": "The completion of the memory operation is indicated using ____ signal.",
            "options": [
                "MFC",
                "WMFC",
                "CFC",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The completion of the memory operation is indicated using ____ signal.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 804,
        "doc": {
            "question_id": -1,
            "question": "____ signal enables the processor to wait for the memory operation to complete.",
            "options": [
                "MFC",
                "TLB",
                "WMFC",
                "ALB"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ signal enables the processor to wait for the memory operation to complete.",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **MFC (Memory Function Complete)** is a signal sent from the memory controller to the processor indicating that a requested memory operation (read/write) has been completed. The processor waits for this signal before proceeding with instructions that depend on the data being fetched or written. \n\nLet's look at why the other options are incorrect:\n\n* **TLB (Translation Lookaside Buffer)** is a cache that speeds up address translation in virtual memory systems. It doesn't directly signal operation completion.\n* **WMFC and ALB** are not standard computer architecture terms related to memory operations. \n"
            }
        ]
    },
    {
        "doc_id": 805,
        "doc": {
            "question_id": -1,
            "question": "The small extremely fast RAM is called as ____",
            "options": [
                "Cache",
                "Heaps",
                "Accumulators",
                "Stacks"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The small extremely fast RAM is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 806,
        "doc": {
            "question_id": -1,
            "question": "The main virtue for using a single Bus structure is ____",
            "options": [
                "Fast data transfers",
                "Cost-effective connectivity and speed",
                "Cost-effective connectivity and ease of attaching peripheral devices",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The main virtue for using a single Bus structure is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 807,
        "doc": {
            "question_id": -1,
            "question": "To extend the connectivity of the processor bus we use ____",
            "options": [
                "PCI bus",
                "SCSI bus",
                "Controllers",
                "Multiple buses"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To extend the connectivity of the processor bus we use ____",
                "Expected Label": "A",
                "Predicted Label": "D) Multiple buses \n\n**Explanation:**\n\nWhile the other options have roles in computer architecture, using **multiple buses** is the fundamental way to extend processor bus connectivity. Here's why:\n\n* **Multiple buses** allow different parts of the system (CPU, memory, peripherals) to communicate efficiently without clogging a single pathway. \n* **PCI bus** (Peripheral Component Interconnect) is a type of bus used for connecting peripherals, but it's a specific implementation, not a general solution for extending processor bus connectivity.\n* **SCSI bus** (Small Computer System Interface) is another type of bus, primarily used for storage devices, and doesn't directly address extending processor bus connectivity.\n* **Controllers** manage devices connected to the bus, but they don't inherently extend the bus's connectivity.\n\n**In summary:**  Multiple buses provide the infrastructure for expanding how many devices and components can interact with the processor. \n"
            }
        ]
    },
    {
        "doc_id": 808,
        "doc": {
            "question_id": -1,
            "question": "The bus used to connect the monitor to the CPU is ____",
            "options": [
                "PCI bus",
                "SCSI bus",
                "Memory bus",
                "Rambus"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The bus used to connect the monitor to the CPU is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile monitors aren't directly connected to the CPU via a bus in the traditional sense, the closest analogy here is the **PCI bus**. Here's why:\n\n* **Modern Connections:** Monitors today primarily use interfaces like HDMI, DisplayPort, DVI, or older VGA connections. These interfaces are often integrated into graphics cards.\n* **Graphics Cards and PCI:** Graphics cards are typically connected to the motherboard (and thus the CPU) using the PCI Express (PCIe) bus, a modern evolution of the PCI standard.\n\nThe other options are incorrect:\n\n* **SCSI:**  Used for storage devices like hard drives.\n* **Memory Bus:** Connects the CPU directly to RAM.\n* **Rambus:** An older type of memory technology. \n"
            }
        ]
    },
    {
        "doc_id": 809,
        "doc": {
            "question_id": -1,
            "question": "The IDE bus is used to connect ____",
            "options": [
                "RAM and processor",
                "GPU and processor",
                "Both Harddisk and CD/DVD drives and Processor",
                "Only CD/DVD drives and Processor"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The IDE bus is used to connect ____",
                "Expected Label": "C",
                "Predicted Label": "C) Both Harddisk and CD/DVD drives and Processor \n\n**Explanation:** IDE (Integrated Drive Electronics) was a standard interface primarily used to connect storage devices like hard disk drives and CD/DVD drives to the motherboard. \n"
            }
        ]
    },
    {
        "doc_id": 810,
        "doc": {
            "question_id": -1,
            "question": "ANSI stands for ____",
            "options": [
                "American National Standards Institute",
                "American National Standard Interface",
                "American Network Standard Interfacing",
                "American Network Security Interrupt"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "ANSI stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 811,
        "doc": {
            "question_id": -1,
            "question": "IBM developed a bus standard for their line of computers \u2018PC-AT\u2019 called ____",
            "options": [
                "IB bus",
                "M-bus",
                "ISA",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "IBM developed a bus standard for their line of computers \u2018PC-AT\u2019 called ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 812,
        "doc": {
            "question_id": -1,
            "question": "The general purpose registers are combined into a block called as ____",
            "options": [
                "Register bank",
                "Register Case",
                "Register file",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The general purpose registers are combined into a block called as ____",
                "Expected Label": "C",
                "Predicted Label": "C"
            }
        ]
    },
    {
        "doc_id": 813,
        "doc": {
            "question_id": -1,
            "question": "In ____ technology, the implementation of the register file is by using an array of memory locations.",
            "options": [
                "VLSI",
                "ANSI",
                "ISA",
                "ASCI"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In ____ technology, the implementation of the register file is by using an array of memory locations.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 814,
        "doc": {
            "question_id": -1,
            "question": "In a three BUS architecture, how many input and output ports are there?",
            "options": [
                "2 output and 2 input",
                "1 output and 2 input",
                "2 output and 1 input",
                "1 output and 1 input"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a three BUS architecture, how many input and output ports are there?",
                "Expected Label": "C",
                "Predicted Label": "**B) 1 output and 2 input** \n\nHere's why:\n\n* **Three-Bus Architecture:** This typically refers to a system with separate buses for:\n    * **Data Bus:**  Bi-directional, carrying data between components.\n    * **Address Bus:** Uni-directional, specifying the memory location being accessed.\n    * **Control Bus:**  Carries control signals (read/write, etc.).\n\n* **Input/Output Perspective:** While the data bus is bi-directional, the control signals determine whether data is being written (output) or read (input) at a given time.  \n\nTherefore, a three-bus architecture conceptually has:\n\n* **One Output Port:**  Where data is placed onto the data bus for writing.\n* **Two Input Ports:** One for data coming from memory, and one for data coming from input/output devices. \n"
            }
        ]
    },
    {
        "doc_id": 815,
        "doc": {
            "question_id": -1,
            "question": "The main advantage of multiple bus organisation over a single bus is ____",
            "options": [
                "Reduction in the number of cycles for execution",
                "Increase in size of the registers",
                "Better Connectivity",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The main advantage of multiple bus organisation over a single bus is ____",
                "Expected Label": "A",
                "Predicted Label": "C) Better Connectivity \n\nMultiple bus architectures allow for simultaneous data transfer between different components, offering better connectivity and improved performance over a single, shared bus. \n"
            }
        ]
    },
    {
        "doc_id": 816,
        "doc": {
            "question_id": -1,
            "question": "CISC stands for ____",
            "options": [
                "Complete Instruction Sequential Compilation",
                "Computer Integrated Sequential Compiler",
                "Complex Instruction Set Computer",
                "Complex Instruction Sequential Compilation"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "CISC stands for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 817,
        "doc": {
            "question_id": -1,
            "question": "If the instruction Add R1, R2, R3 is executed in a system which is pipelined, then the value of S is (Where S is term of the Basic performance equation).",
            "options": [
                "3",
                "~2",
                "~1",
                "6"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "If the instruction Add R1, R2, R3 is executed in a system which is pipelined, then the value of S is (Where S is term of the Basic performance equation).",
                "Expected Label": "C",
                "Predicted Label": "C) ~1 \n\n**Explanation:**\n\nThe variable 'S' in the basic performance equation represents the number of clock cycles per instruction (CPI). In a pipelined architecture, the ideal CPI approaches 1. This means that, on average, one instruction is completed per clock cycle. \n\nWhile an individual instruction like \"Add R1, R2, R3\" might take multiple cycles to complete its stages within the pipeline, the overlapping nature of pipelining allows for a new instruction to be fetched and started in each subsequent cycle. Therefore, the effective CPI approaches 1 as the pipeline fills up.\n\n**Why other options are incorrect:**\n\n* **A) 3:** This would suggest each instruction takes 3 cycles, which is not ideal for a pipelined system.\n* **B) ~2:** Still higher than the ideal CPI for a pipelined system.\n* **D) 6:** This is far too high for a pipelined system and indicates a severe performance bottleneck. \n"
            }
        ]
    },
    {
        "doc_id": 818,
        "doc": {
            "question_id": -1,
            "question": "In multiple BUS organisation ____ is used to select any of the BUSes for input into ALU.",
            "options": [
                "MUX",
                "DE-MUX",
                "En-CDS",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In multiple BUS organisation ____ is used to select any of the BUSes for input into ALU.",
                "Expected Label": "A",
                "Predicted Label": "A) MUX \n\n**Explanation:**\n\nA multiplexer (MUX) is the ideal choice for selecting one input from several to feed into the ALU. Here's why:\n\n* **Function:** A MUX takes multiple input lines and, based on a select signal, outputs only one of those inputs. This perfectly aligns with the need to choose which bus's data will be used by the ALU.\n* **Flexibility:** MUXes come in various sizes, allowing a system to handle different numbers of buses. \n* **Common Use:** MUXes are fundamental components in computer architecture and are commonly employed for signal routing and selection.\n\nLet me know if you have any other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 819,
        "doc": {
            "question_id": -1,
            "question": "____ are the different type/s of generating control signals.",
            "options": [
                "Micro-programmed",
                "Hardwired",
                "Micro-instruction",
                "Both Micro-programmed and Hardwired"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ are the different type/s of generating control signals.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 820,
        "doc": {
            "question_id": -1,
            "question": "The type of control signal is generated based on ____",
            "options": [
                "contents of the step counter",
                "Contents of IR",
                "Contents of condition flags",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The type of control signal is generated based on ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 821,
        "doc": {
            "question_id": -1,
            "question": "What does the hardwired control generator consist of?",
            "options": [
                "Decoder/encoder",
                "Condition codes",
                "Control step counter",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "What does the hardwired control generator consist of?",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) All of the mentioned**. Here's why:\n\n* **Decoder/encoder:** These circuits translate instruction opcodes into specific control signals that activate different parts of the CPU.\n* **Condition codes:** These flags (like zero, carry, overflow) are set based on the result of previous operations and influence the control flow, especially for conditional branching.\n* **Control step counter:** This component keeps track of the current step in the execution of an instruction, ensuring each micro-operation within the instruction cycle happens in the correct sequence.\n\nA hardwired control unit is built using fixed logic circuits (often a complex arrangement of logic gates) to generate control signals. All the components listed are essential parts of this fixed logic implementation. \n"
            }
        ]
    },
    {
        "doc_id": 822,
        "doc": {
            "question_id": -1,
            "question": "What does the end instruction do?",
            "options": [
                "It ends the generation of a signal",
                "It ends the complete generation process",
                "It starts a new instruction fetch cycle and resets the counter",
                "It is used to shift the control to the processor"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "What does the end instruction do?",
                "Expected Label": "C",
                "Predicted Label": "**C) It starts a new instruction fetch cycle and resets the counter** \n\nThe `end` instruction (or a similar instruction depending on the specific assembly language) typically signifies the end of a program or a section of code. While it doesn't directly \"end\" hardware processes, it triggers the following:\n\n* **Instruction Pointer Reset:**  The program counter (or instruction pointer) is reset to the beginning of the next instruction cycle or a designated starting point.\n* **Fetch Cycle Initiation:** This reset prompts the processor to start fetching the next instruction from memory. \n\n**Let's look at why the other options are incorrect:**\n\n* **A) It ends the generation of a signal:**  While instructions involve signals, `end` doesn't directly target a single signal's generation.\n* **B) It ends the complete generation process:** `end` usually marks the end of the program's execution, not the processor's internal operations. \n* **D) It is used to shift the control to the processor:** The processor already has control; the `end` instruction provides a signal for orderly program termination or branching. \n"
            }
        ]
    },
    {
        "doc_id": 823,
        "doc": {
            "question_id": -1,
            "question": "What does the RUN signal do?",
            "options": [
                "It causes the termination of a signal",
                "It causes a particular signal to perform its operation",
                "It causes a particular signal to end",
                "It increments the step counter by one"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "What does the RUN signal do?",
                "Expected Label": "D",
                "Predicted Label": "B) It causes a particular signal to perform its operation \n"
            }
        ]
    },
    {
        "doc_id": 824,
        "doc": {
            "question_id": -1,
            "question": "The benefit of using this approach is ____",
            "options": [
                "It is cost effective",
                "It is highly efficient",
                "It is very reliable",
                "It increases the speed of operation"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The benefit of using this approach is ____",
                "Expected Label": "D",
                "Predicted Label": "Please provide me with the \"approach\" being referred to in the question. I need the context to determine the benefit and choose the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 825,
        "doc": {
            "question_id": -1,
            "question": "The disadvantage/s of the hardwired approach is ____",
            "options": [
                "It is less flexible",
                "It cannot be used for complex instructions",
                "It is costly",
                "less flexible & cannot be used for complex instructions"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The disadvantage/s of the hardwired approach is ____",
                "Expected Label": "D",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 826,
        "doc": {
            "question_id": -1,
            "question": "In micro-programmed approach, the signals are generated by ____",
            "options": [
                "Machine instructions",
                "System programs",
                "Utility tools",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In micro-programmed approach, the signals are generated by ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\nMicro-programmed control units don't rely on any of the options listed. Here's why:\n\n* **Micro-programmed control** uses a sequence of **microinstructions** (stored in a control memory) to generate control signals for the processor.  \n\nLet's break down why the other options are incorrect:\n\n* **A) Machine instructions:** These are high-level instructions executed by the processor, not the control unit itself.\n* **B) System programs:** These are software programs that manage computer hardware and resources. They don't directly generate hardware control signals.\n* **C) Utility tools:**  These are software tools for specific tasks and don't interact with the control unit at this level. \n"
            }
        ]
    },
    {
        "doc_id": 827,
        "doc": {
            "question_id": -1,
            "question": "A word whose individual bits represent a control signal is ____",
            "options": [
                "Command word",
                "Control word",
                "Co-ordination word",
                "Generation word"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A word whose individual bits represent a control signal is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 828,
        "doc": {
            "question_id": -1,
            "question": "A sequence of control words corresponding to a control sequence is called ____",
            "options": [
                "Micro routine",
                "Micro function",
                "Micro procedure",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A sequence of control words corresponding to a control sequence is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 829,
        "doc": {
            "question_id": -1,
            "question": "Individual control words of the micro routine are called as ____",
            "options": [
                "Micro task",
                "Micro operation",
                "Micro instruction",
                "Micro command"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Individual control words of the micro routine are called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 830,
        "doc": {
            "question_id": -1,
            "question": "The special memory used to store the micro routines of a computer is ____",
            "options": [
                "Control table",
                "Control store",
                "Control mart",
                "Control shop"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The special memory used to store the micro routines of a computer is ____",
                "Expected Label": "B",
                "Predicted Label": "B) Control store \n"
            }
        ]
    },
    {
        "doc_id": 831,
        "doc": {
            "question_id": -1,
            "question": "To read the control words sequentially ____ is used.",
            "options": [
                "PC",
                "IR",
                "UPC",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To read the control words sequentially ____ is used.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **A) PC**. Here's why:\n\n* **PC (Program Counter)**: The Program Counter holds the memory address of the *next* instruction to be fetched and executed.  It's how the CPU knows the sequence of instructions within a program.\n\nLet's break down why the other options are incorrect:\n\n* **IR (Instruction Register)**: The Instruction Register holds the *current* instruction being decoded and executed. \n* **UPC (Usually not a standard acronym)**:  There's no widely recognized architectural term \"UPC\" in the context of computer architecture.\n* **None of the mentioned**: The PC is essential for sequential instruction reading.\n\n**In summary, the PC is the primary mechanism for ensuring instructions are read and executed in the correct order.** \n"
            }
        ]
    },
    {
        "doc_id": 832,
        "doc": {
            "question_id": -1,
            "question": "Every time a new instruction is loaded into IR the output of ____ is loaded into UPC.",
            "options": [
                "Starting address generator",
                "Loader",
                "Linker",
                "Clock"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Every time a new instruction is loaded into IR the output of ____ is loaded into UPC.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Instruction Register (IR):** Holds the currently executing instruction.\n* **Program Counter (PC):** Holds the address of the *next* instruction to be fetched.\n\nWhen a new instruction needs to be loaded:\n\n1. The PC currently holds the address of that new instruction.\n2. The content of the PC is used to fetch the instruction from memory and load it into the IR.\n3.  The PC needs to be updated to point to the subsequent instruction. This is where a **Starting Address Generator** comes in. It calculates the address of the next instruction (which might be simply incrementing the PC, or could be modified by jumps or branches).\n\n**Why other options are incorrect:**\n\n* **Loader:** Loads the program into memory initially, but isn't involved in fetching individual instructions.\n* **Linker:**  Combines object files and resolves addresses, happening before execution.\n* **Clock:** Provides timing signals for the system but doesn't directly manage instruction addresses. \n"
            }
        ]
    },
    {
        "doc_id": 833,
        "doc": {
            "question_id": -1,
            "question": "The case/s where micro-programmed can perform well ____",
            "options": [
                "When it requires to check the condition codes",
                "When it has to choose between the two alternatives",
                "When it is triggered by an interrupt",
                "None of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The case/s where micro-programmed can perform well ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 834,
        "doc": {
            "question_id": -1,
            "question": "Highly encoded schemes that use compact codes to specify a small number of functions in each micro instruction is ____",
            "options": [
                "Horizontal organisation",
                "Vertical organisation",
                "Diagonal organisation",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Highly encoded schemes that use compact codes to specify a small number of functions in each micro instruction is ____",
                "Expected Label": "B",
                "Predicted Label": "**B) Vertical organisation** \n\n**Explanation:**\n\n* **Vertical microprogramming** uses highly encoded instructions, meaning each bit or a small group of bits has a specific meaning. This results in longer microprograms (more microinstructions) because each instruction carries out a smaller, more basic operation. \n\nLet me know if you'd like more details on microprogramming organization! \n"
            }
        ]
    },
    {
        "doc_id": 835,
        "doc": {
            "question_id": -1,
            "question": "The surroundings of the recently accessed block is called as ____",
            "options": [
                "Neighborhood",
                "Neighbour",
                "Locality of reference",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The surroundings of the recently accessed block is called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 836,
        "doc": {
            "question_id": -1,
            "question": "The algorithm which replaces the block which has not been referenced for a while is called ____",
            "options": [
                "LRU",
                "ORF",
                "Direct",
                "Both LRU and ORF"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The algorithm which replaces the block which has not been referenced for a while is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **LRU (Least Recently Used)** is a page replacement algorithm that replaces the page that has not been used for the longest period. This aligns perfectly with the question's description. \n\nLet's look at the other options:\n\n* **ORF (Optimal Replacement Frame)** is an ideal algorithm (not practically implementable) that replaces the page that won't be used for the longest time **in the future**.\n* **Direct** is not a page replacement algorithm.\n* **Both LRU and ORF** is incorrect because ORF is not a practical replacement algorithm. \n"
            }
        ]
    },
    {
        "doc_id": 837,
        "doc": {
            "question_id": -1,
            "question": "In associative mapping during LRU, the counter of the new block is set to \u20180\u2019 and all the others are incremented by one when ____ occurs.",
            "options": [
                "Delay",
                "Miss",
                "Hit",
                "Delayed hit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In associative mapping during LRU, the counter of the new block is set to \u20180\u2019 and all the others are incremented by one when ____ occurs.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **(C) Hit**. \n\nHere's why:\n\n* **Associative Mapping:**  A flexible cache mapping technique where a block can be placed in any available cache line. It uses tags to determine if a block is present.\n* **LRU (Least Recently Used):** A replacement algorithm that evicts the cache line that hasn't been accessed for the longest time.\n* **How LRU Works with Counters:**  Each cache line has a counter. When a hit occurs, the counter of the hit line is reset to 0. All other counters with values less than the hit line's counter are incremented. Counters with values greater than or equal to the hit line's counter remain unchanged.\n\n**In summary:** During a hit in an associative mapped cache using LRU, the counter of the accessed block is reset to indicate it's the most recently used.  This ensures that the least recently used block will have the highest counter value, making it the target for replacement when needed. \n"
            }
        ]
    },
    {
        "doc_id": 838,
        "doc": {
            "question_id": -1,
            "question": "The LRU provides very bad performance when it comes to ____",
            "options": [
                "Blocks being accessed is sequential",
                "When the blocks are randomised",
                "When the consecutive blocks accessed are in the extremes",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The LRU provides very bad performance when it comes to ____",
                "Expected Label": "A",
                "Predicted Label": "C) \n\n**Explanation:**\n\nLRU stands for Least Recently Used, a common page replacement algorithm.  Here's why the answer is C:\n\n* **LRU's Weakness:** LRU performs poorly when there's a pattern of accessing data blocks that are far apart in the address space, especially if this access pattern repeats. Think of it like constantly flipping between the first and last pages of a large book \u2013 LRU would keep replacing pages you need soon.\n\nLet's analyze the other options:\n\n* **A) Blocks being accessed sequentially:**  LRU performs very well in this scenario. It naturally keeps the most recently used blocks, which are highly likely to be needed again soon.\n* **B) When the blocks are randomized:** LRU's performance is generally average in this case. It's not ideal but not terrible.\n* **D) None of the mentioned:** This is incorrect since option C highlights a key weakness.\n\n**In summary:** The \"extremes\" pattern mentioned in option C is a classic example of how to make LRU perform poorly. \n"
            }
        ]
    },
    {
        "doc_id": 839,
        "doc": {
            "question_id": -1,
            "question": "The algorithm which removes the recently used page first is ____",
            "options": [
                "LRU",
                "MRU",
                "OFM",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The algorithm which removes the recently used page first is ____",
                "Expected Label": "B",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 840,
        "doc": {
            "question_id": -1,
            "question": "In LRU, the referenced blocks counter is set to\u20190\u2032 and that of the previous blocks are incremented by one and others remain same, in the case of ____",
            "options": [
                "Hit",
                "Miss",
                "Delay",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In LRU, the referenced blocks counter is set to\u20190\u2032 and that of the previous blocks are incremented by one and others remain same, in the case of ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 841,
        "doc": {
            "question_id": -1,
            "question": "The counter that keeps track of how many times a block is most likely used is ____",
            "options": [
                "Count",
                "Reference counter",
                "Use counter",
                "Probable counter"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The counter that keeps track of how many times a block is most likely used is ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) Use counter**. \n\nHere's why:\n\n* **Use counter** is a term commonly used in cache management to track the frequency of block access. This information is crucial for cache replacement algorithms (like LRU - Least Recently Used) to determine which block to evict when a new one needs to be loaded. \n\nLet's look at why the other options aren't the best fit:\n\n* **Count** is too generic.  While it implies counting something, it doesn't specify what.\n* **Reference counter** usually tracks how many pointers are referencing a specific memory block. This is important for garbage collection, not cache management.\n* **Probable counter** is not a standard term in computer architecture. \n\n**Therefore, \"Use counter\" is the most accurate and widely accepted term for tracking block usage frequency in cache memory.** \n"
            }
        ]
    },
    {
        "doc_id": 842,
        "doc": {
            "question_id": -1,
            "question": "The key factor/s in commercial success of a computer is/are ____",
            "options": [
                "Performance",
                "Cost",
                "Speed",
                "Both Performance and Cost"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The key factor/s in commercial success of a computer is/are ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 843,
        "doc": {
            "question_id": -1,
            "question": "The main objective of the computer system is ____",
            "options": [
                "To provide optimal power operation",
                "To provide the best performance at low cost",
                "To provide speedy operation at low power consumption",
                "All of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The main objective of the computer system is ____",
                "Expected Label": "B",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 844,
        "doc": {
            "question_id": -1,
            "question": "A common measure of performance is ____",
            "options": [
                "Price/performance ratio",
                "Performance/price ratio",
                "Operation/price ratio",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A common measure of performance is ____",
                "Expected Label": "A",
                "Predicted Label": "B) Performance/price ratio \n\n**Explanation:**\n\nWhile price/performance (A) might seem intuitive, the industry standard emphasizes **performance per unit of cost**.  A higher performance/price ratio means you get more computational power for every dollar spent. \n"
            }
        ]
    },
    {
        "doc_id": 845,
        "doc": {
            "question_id": -1,
            "question": "The performance depends on ____",
            "options": [
                "The speed of execution only",
                "The speed of fetch and execution",
                "The speed of fetch only",
                "The hardware of the system only"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The performance depends on ____",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 846,
        "doc": {
            "question_id": -1,
            "question": "The main purpose of having memory hierarchy is to ____",
            "options": [
                "Reduce access time",
                "Provide large capacity",
                "Reduce propagation time",
                "Reduce access time & Provide large capacity"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The main purpose of having memory hierarchy is to ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 847,
        "doc": {
            "question_id": -1,
            "question": "An effective to introduce parallelism in memory access is by ____",
            "options": [
                "Memory interleaving",
                "TLB",
                "Pages",
                "Frames"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "An effective to introduce parallelism in memory access is by ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 848,
        "doc": {
            "question_id": -1,
            "question": "Two processors A and B have clock frequencies of 700 Mhz and 900 Mhz respectively. Suppose A can execute an instruction with an average of 3 steps and B can execute with an average of 5 steps. For the execution of the same instruction which processor is faster.",
            "options": [
                "A",
                "B",
                "Both take the same time",
                "Insufficient information"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Two processors A and B have clock frequencies of 700 Mhz and 900 Mhz respectively. Suppose A can execute an instruction with an average of 3 steps and B can execute with an average of 5 steps. For the execution of the same instruction which processor is faster.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 849,
        "doc": {
            "question_id": -1,
            "question": "If the instruction Add R1, R2, R3 is executed in a system which is pipelined, then the value of S is (Where S is a term of the Basic performance equation).",
            "options": [
                "3",
                "~2",
                "~1",
                "6"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "If the instruction Add R1, R2, R3 is executed in a system which is pipelined, then the value of S is (Where S is a term of the Basic performance equation).",
                "Expected Label": "C",
                "Predicted Label": "**C) ~1**\n\n**Explanation:**\n\nIn a pipelined system, the goal is to achieve an ideal throughput of one instruction per clock cycle. This means that while multiple instructions are being processed in different stages of the pipeline simultaneously, on average, one instruction completes execution per cycle.  \n\nThe variable 'S' in the basic performance equation typically represents the number of clock cycles per instruction (CPI).  In a well-designed pipelined system, the CPI approaches 1 because of the overlapping execution. \n\nLet's break down why the other options are incorrect:\n\n* **A) 3:** This would imply that the instruction takes three clock cycles to complete, which is not ideal for a pipelined architecture.\n* **B) ~2:**  Similar to option A, this suggests the pipeline isn't achieving its optimal throughput.\n* **D) 6:**  This is highly unlikely and suggests a very inefficient pipeline. \n\n**Important Note:** While the ideal CPI in a pipelined system is 1, various factors like pipeline hazards (data hazards, control hazards, structural hazards) can cause stalls and increase the actual CPI slightly above 1. \n"
            }
        ]
    },
    {
        "doc_id": 850,
        "doc": {
            "question_id": -1,
            "question": "The program is divided into operable parts called as ____",
            "options": [
                "Frames",
                "Segments",
                "Pages",
                "Sheets"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The program is divided into operable parts called as ____",
                "Expected Label": "B",
                "Predicted Label": "B) Segments \n"
            }
        ]
    },
    {
        "doc_id": 851,
        "doc": {
            "question_id": -1,
            "question": "The techniques which move the program blocks to or from the physical memory is called as ____",
            "options": [
                "Paging",
                "Virtual memory organisation",
                "Overlays",
                "Framing"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The techniques which move the program blocks to or from the physical memory is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B) Virtual memory organisation \n\nLet's break down why:\n\n* **Virtual memory organization** is the overarching concept that allows us to use secondary storage (like the hard drive) as an extension of RAM. It involves techniques like paging and swapping to move data between these different levels of memory.\n\nHere's why the other options aren't as accurate:\n\n* **Paging:** A specific technique *within* virtual memory management. It divides memory into fixed-size blocks (pages).\n* **Overlays:** A programming technique where only necessary parts of a program are loaded into memory at a time.  While it manages memory, it doesn't directly move blocks to/from physical memory in the same way virtual memory does.\n* **Framing:**  Similar to paging, it refers to dividing physical memory into fixed-size blocks. \n\n**Therefore, \"Virtual memory organization\" best describes the overall process of moving program blocks between physical memory and secondary storage.** \n"
            }
        ]
    },
    {
        "doc_id": 852,
        "doc": {
            "question_id": -1,
            "question": "The binary address issued to data or instructions are called as ____",
            "options": [
                "Physical address",
                "Location",
                "Relocatable address",
                "Logical address"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The binary address issued to data or instructions are called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 853,
        "doc": {
            "question_id": -1,
            "question": "____ is used to implement virtual memory organisation.",
            "options": [
                "Page table",
                "Frame table",
                "MMU",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is used to implement virtual memory organisation.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **MMU (Memory Management Unit)** is the hardware component responsible for translating virtual addresses to physical addresses, a core function in implementing virtual memory. \n\nLet's break down why the other options are not the best fit:\n\n* **Page Table:** A data structure *used by* the MMU to store the mapping between virtual and physical addresses. \n* **Frame Table:** Tracks the allocation status of physical memory frames. While related to memory management, it's not the primary mechanism for implementing virtual memory.\n* **None of the mentioned:** Incorrect, as the MMU is essential. \n"
            }
        ]
    },
    {
        "doc_id": 854,
        "doc": {
            "question_id": -1,
            "question": "____ translates the logical address into a physical address.",
            "options": [
                "MMU",
                "Translator",
                "Compiler",
                "Linker"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ translates the logical address into a physical address.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 855,
        "doc": {
            "question_id": -1,
            "question": "The main aim of virtual memory organisation is ____",
            "options": [
                "To provide effective memory access",
                "To provide better memory transfer",
                "To improve the execution of the program",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The main aim of virtual memory organisation is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 856,
        "doc": {
            "question_id": -1,
            "question": "The virtual memory basically stores the next segment of data to be executed on the ____",
            "options": [
                "Secondary storage",
                "Disks",
                "RAM",
                "ROM"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The virtual memory basically stores the next segment of data to be executed on the ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile virtual memory uses secondary storage (like disks) to hold the full data, the \"next segment\" to be executed *must* be in RAM.  Here's why:\n\n* **CPU Access:** The CPU can only directly access data in RAM.\n* **Virtual Memory Mapping:** Virtual memory translates logical addresses (used by programs) to physical addresses (in RAM). This translation ensures the required data segment is loaded into RAM before execution.\n\n**Therefore, while options B and C are related to virtual memory, option A (Secondary storage) is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 857,
        "doc": {
            "question_id": -1,
            "question": "The associatively mapped virtual memory makes use of ____",
            "options": [
                "TLB",
                "Page table",
                "Frame table",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The associatively mapped virtual memory makes use of ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Translation Lookaside Buffer (TLB)** is a specialized cache used in virtual memory systems to speed up the translation of virtual addresses to physical addresses. Associative mapping is a key technique employed in TLBs to allow for a more flexible and efficient lookup process. \n\nLet's break down why the other options are incorrect:\n\n* **Page table:** While page tables are integral to virtual memory, they are not inherently tied to associative mapping. They can be implemented with different mapping techniques like direct or set-associative.\n* **Frame table:** Frame tables track the allocation of physical memory frames and are not directly involved in address translation using associative mapping.\n* **None of the mentioned:** This is incorrect, as TLBs are a fundamental component of associative mapping in virtual memory. \n"
            }
        ]
    },
    {
        "doc_id": 858,
        "doc": {
            "question_id": -1,
            "question": "The main reason for the discontinuation of semi conductor based storage devices for providing large storage space is ____",
            "options": [
                "Lack of sufficient resources",
                "High cost per bit value",
                "Lack of speed of operation",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The main reason for the discontinuation of semi conductor based storage devices for providing large storage space is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nWhile semiconductor-based storage (like SSDs) offers superior speed, the primary reason for not using it for massive storage solutions boils down to **cost per bit**.  Hard Disk Drives (HDDs), while slower, remain significantly cheaper when storing large amounts of data. \n"
            }
        ]
    },
    {
        "doc_id": 859,
        "doc": {
            "question_id": -1,
            "question": "The digital information is stored on the hard disk by ____",
            "options": [
                "Applying a suitable electric pulse",
                "Applying a suitable magnetic field",
                "Applying a suitable nuclear field",
                "By using optic waves"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The digital information is stored on the hard disk by ____",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 860,
        "doc": {
            "question_id": -1,
            "question": "For the synchronization of the read head, we make use of a ____",
            "options": [
                "Framing bit",
                "Synchronization bit",
                "Clock",
                "Dirty bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "For the synchronization of the read head, we make use of a ____",
                "Expected Label": "C",
                "Predicted Label": "C) Clock \n\n**Explanation:**\n\n* **Clock synchronization** is fundamental in computer architecture to align data transfer between components. The read head, responsible for reading data from storage, relies on a clock signal to accurately sample the data at the correct intervals.\n\nLet's look at why the other options are incorrect:\n\n* **Framing bit:** Used in networking to delimit data frames within a continuous bitstream.\n* **Synchronization bit:**  A more general term, often used within a frame (like networking) to denote the start of data.  While related to synchronization, it's not the mechanism used for read head timing.\n* **Dirty bit:** Indicates whether data in a cache has been modified and needs to be written back to the main memory. \n\n**In summary:** The clock provides the precise timing signal essential for the read head's operation. \n"
            }
        ]
    },
    {
        "doc_id": 861,
        "doc": {
            "question_id": -1,
            "question": "One of the most widely used schemes of encoding used is ____",
            "options": [
                "NRZ-polar",
                "RZ-polar",
                "Manchester",
                "Block encoding"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "One of the most widely used schemes of encoding used is ____",
                "Expected Label": "C",
                "Predicted Label": "**C) Manchester** \n\nHere's why:\n\n* **Manchester encoding** is widely used in Ethernet and other networking standards because it offers a good balance of simplicity and reliability. It ensures clock synchronization and is relatively easy to implement.\n\nLet's look at the other options:\n\n* **NRZ-polar and RZ-polar:** While used in some applications, they are not as widely used as Manchester encoding, particularly in common networking standards. They can be more susceptible to synchronization issues.\n* **Block encoding:** This is a broader category of encoding schemes (which can include Manchester) and not a specific encoding method itself.\n\n**Therefore, Manchester encoding (C) is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 862,
        "doc": {
            "question_id": -1,
            "question": "The drawback of Manchester encoding is ____",
            "options": [
                "The cost of the encoding scheme",
                "The speed of encoding the data",
                "The Latency offered",
                "The low bit storage density provided"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The drawback of Manchester encoding is ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 863,
        "doc": {
            "question_id": -1,
            "question": "____ pushes the heads away from the surface as they rotate at their standard rates.",
            "options": [
                "Magnetic tension",
                "Electric force",
                "Air pressure",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ pushes the heads away from the surface as they rotate at their standard rates.",
                "Expected Label": "C",
                "Predicted Label": "C) Air pressure \n\n**Explanation:**\n\nHard disk drives (HDDs) store data on spinning platters with a magnetic surface.  The read/write heads hover extremely close to the platter surface without actually touching it.  As the platter spins at high speed, **air pressure** builds up beneath the heads, providing the lift needed to keep them from contacting the platter and causing damage. \n"
            }
        ]
    },
    {
        "doc_id": 864,
        "doc": {
            "question_id": -1,
            "question": "The air pressure can be countered by putting ____ in the head-disc surface arrangement.",
            "options": [
                "Air filter",
                "Spring mechanism",
                "coolant",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The air pressure can be countered by putting ____ in the head-disc surface arrangement.",
                "Expected Label": "B",
                "Predicted Label": "**B) Spring mechanism** \n\n**Explanation:**\n\nHard disk drives need a way to keep the read/write heads incredibly close to the platter surface without touching. A spring mechanism provides the necessary downforce to counteract air pressure (which could cause the head to float). Here's why the other options are incorrect:\n\n* **A) Air filter:** Air filters protect the drive from dust but don't address air pressure issues.\n* **C) Coolant:**  Coolant manages temperature and doesn't impact the head-disk gap.\n* **D) None of the mentioned:** This is incorrect, as a spring mechanism is the correct solution. \n"
            }
        ]
    },
    {
        "doc_id": 865,
        "doc": {
            "question_id": -1,
            "question": "The method of placing the heads and the discs in an air tight environment is also called as ____",
            "options": [
                "RAID Arrays",
                "ATP tech",
                "Winchester technology",
                "Fleming reduction"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The method of placing the heads and the discs in an air tight environment is also called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Winchester technology** is the historical term for the sealed, air-filtered enclosure used for hard disk drives. \n\nLet's eliminate the other options:\n\n* **RAID Arrays (A):**  RAID (Redundant Array of Independent Disks) deals with data storage organization across multiple disks for redundancy and/or performance, not the physical enclosure.\n* **ATP Tech (B):**  ATP is not a recognized term in the context of hard drive technology. \n* **Fleming Reduction (D):** This is not a real term associated with hard drives. \n\nLet me know if you have any other computer architecture questions! \n"
            }
        ]
    },
    {
        "doc_id": 866,
        "doc": {
            "question_id": -1,
            "question": "A hard disk with 20 surfaces will have ____ heads.",
            "options": [
                "10",
                "5",
                "1",
                "20"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A hard disk with 20 surfaces will have ____ heads.",
                "Expected Label": "D",
                "Predicted Label": "D) 20 \n\n**Explanation:**\n\nIn a hard disk drive, there's one read/write head **per** platter surface.  Since the question states there are 20 surfaces, there must be 20 heads. \n"
            }
        ]
    },
    {
        "doc_id": 867,
        "doc": {
            "question_id": -1,
            "question": "The set of corresponding tracks on all surfaces of a stack of disks form a ____",
            "options": [
                "Cluster",
                "Cylinder",
                "Group",
                "Set"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The set of corresponding tracks on all surfaces of a stack of disks form a ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nA **cylinder** in disk storage refers to the set of all tracks that are at the same vertical position across multiple platters. Imagine a stack of disks \u2013 a cylinder would be like drawing a cylinder through the same track number on each platter.  \n\nLet's break down why the other options are incorrect:\n\n* **Cluster:** A cluster is a group of contiguous sectors on a single track of a disk, used for efficient storage allocation.\n* **Group:**  This is a generic term and doesn't have a specific meaning in disk storage terminology.\n* **Set:** Similar to \"group,\" \"set\" is too general and lacks specific relevance to disk structure. \n"
            }
        ]
    },
    {
        "doc_id": 868,
        "doc": {
            "question_id": -1,
            "question": "The data can be accessed from the disk using ____",
            "options": [
                "Surface number",
                "Sector number",
                "Track number",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The data can be accessed from the disk using ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 869,
        "doc": {
            "question_id": -1,
            "question": "The read and write operations usually start at ____ of the sector.",
            "options": [
                "Center",
                "Middle",
                "From the last used point",
                "Boundaries"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The read and write operations usually start at ____ of the sector.",
                "Expected Label": "D",
                "Predicted Label": "D) Boundaries \n"
            }
        ]
    },
    {
        "doc_id": 870,
        "doc": {
            "question_id": -1,
            "question": "To distinguish between two sectors we make use of ____",
            "options": [
                "Inter sector gap",
                "Splitting bit",
                "Numbering bit",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "To distinguish between two sectors we make use of ____",
                "Expected Label": "A",
                "Predicted Label": "C. \n"
            }
        ]
    },
    {
        "doc_id": 871,
        "doc": {
            "question_id": -1,
            "question": "The ____ process divides the disk into sectors and tracks.",
            "options": [
                "Creation",
                "Initiation",
                "Formatting",
                "Modification"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ process divides the disk into sectors and tracks.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 872,
        "doc": {
            "question_id": -1,
            "question": "The access time is composed of ____",
            "options": [
                "Seek time",
                "Rotational delay",
                "Latency",
                "Both Seek time and Rotational delay"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The access time is composed of ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 873,
        "doc": {
            "question_id": -1,
            "question": "The disk drive is connected to the system by using the ____",
            "options": [
                "PCI bus",
                "SCSI bus",
                "HDMI",
                "ISA"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The disk drive is connected to the system by using the ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 874,
        "doc": {
            "question_id": -1,
            "question": "____ is used to deal with the difference in the transfer rates between the drive and the bus.",
            "options": [
                "Data repeaters",
                "Enhancers",
                "Data buffers",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is used to deal with the difference in the transfer rates between the drive and the bus.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Data buffers** are used to temporarily store data during a transfer between devices with different transfer rates. This allows the faster device to continue operating without waiting for the slower device to catch up. \n"
            }
        ]
    },
    {
        "doc_id": 875,
        "doc": {
            "question_id": -1,
            "question": "____ is used to detect and correct the errors that may occur during data transfers.",
            "options": [
                "ECC",
                "CRC",
                "Checksum",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is used to detect and correct the errors that may occur during data transfers.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**ECC (Error Correction Code)** is specifically designed to not only detect but also correct errors in data. While the other options offer error detection, ECC goes a step further by enabling correction without requiring data retransmission. \n"
            }
        ]
    },
    {
        "doc_id": 876,
        "doc": {
            "question_id": -1,
            "question": "The logic operations are implemented using ____ circuits.",
            "options": [
                "Bridge",
                "Logical",
                "Combinatorial",
                "Gate"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The logic operations are implemented using ____ circuits.",
                "Expected Label": "C",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 877,
        "doc": {
            "question_id": -1,
            "question": "The carry generation function: ci + 1 = yici + xici + xiyi, is implemented in ____",
            "options": [
                "Half adders",
                "Full adders",
                "Ripple adders",
                "Fast adders"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The carry generation function: ci + 1 = yici + xici + xiyi, is implemented in ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Full adders**. Here's why:\n\n* **Full Adders** are designed to handle carry-in. The equation you provided (ci+1 = yici + xici + xiyi) is the fundamental logic for calculating the carry-out (ci+1) based on the current bit inputs (xi, yi) and the carry-in (ci). \n\nLet's break down why the other options are incorrect:\n\n* **Half Adders:**  These only handle the sum of two bits and generate a carry-out. They don't have a mechanism for a carry-in.\n* **Ripple Adders:** These are built using a chain of Full Adders. While they utilize the carry generation function, they are not the fundamental unit implementing it.\n* **Fast Adders:** This is a broad term for adder designs optimized for speed (like Carry Lookahead Adders). They still rely on the carry generation function within their building blocks. \n"
            }
        ]
    },
    {
        "doc_id": 878,
        "doc": {
            "question_id": -1,
            "question": "Which option is true regarding the carry in the ripple adders?",
            "options": [
                "Are generated at the beginning only",
                "Must travel through the configuration",
                "Is generated at the end of each operation",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Which option is true regarding the carry in the ripple adders?",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Must travel through the configuration**.\n\nHere's why:\n\n* **Ripple Carry Adders** get their name from how the carry bit propagates.  The carry generated by each full adder (representing a bit position) is fed as input to the next full adder in the chain.\n\n* **A) Are generated at the beginning only:** Incorrect. Carries are generated at each stage of the addition process where the sum of bits and the incoming carry require it.\n* **C) Is generated at the end of each operation:** Incorrect. While the final carry out represents the overall carry of the addition, individual carries are generated at each stage.\n* **D) None of the mentioned:** Incorrect. Option B accurately describes the carry propagation in ripple adders.\n\n**In summary:** The carry bit in a ripple adder \"ripples\" through the circuit from one full adder to the next, hence the name. \n"
            }
        ]
    },
    {
        "doc_id": 879,
        "doc": {
            "question_id": -1,
            "question": "In full adders the sum circuit is implemented using ____",
            "options": [
                "And & or gates",
                "NAND gate",
                "XOR",
                "XNOR"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In full adders the sum circuit is implemented using ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 880,
        "doc": {
            "question_id": -1,
            "question": "The usual implementation of the carry circuit involves ____",
            "options": [
                "And & or gates",
                "XOR",
                "NAND",
                "XNOR"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The usual implementation of the carry circuit involves ____",
                "Expected Label": "B",
                "Predicted Label": "A \n\n**Explanation:**\n\nCarry circuits, essential in adders, are typically constructed using **AND** and **OR** gates. Here's a simplified breakdown:\n\n* **AND Gate:** Detects when both input bits are '1', indicating a carry generation in that bit position.\n* **OR Gate:**  Handles carry propagation. If a carry is generated at a lower bit position OR an incoming carry exists, the OR gate outputs a '1' to propagate the carry forward. \n\nLet me know if you'd like a more detailed explanation of carry generation and propagation! \n"
            }
        ]
    },
    {
        "doc_id": 881,
        "doc": {
            "question_id": -1,
            "question": "A ____ gate is used to detect the occurrence of an overflow.",
            "options": [
                "NAND",
                "XOR",
                "XNOR",
                "AND"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ gate is used to detect the occurrence of an overflow.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 882,
        "doc": {
            "question_id": -1,
            "question": "In a normal adder circuit, the delay obtained in a generation of the output is ____",
            "options": [
                "2n + 2",
                "2n",
                "n + 2",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a normal adder circuit, the delay obtained in a generation of the output is ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **(D) None of the mentioned**. Here's why:\n\n* **Adder Delay Complexity:** The delay of a normal adder circuit (specifically a ripple-carry adder) is generally proportional to the number of bits being added. However, it's not a simple linear relationship like the options suggest. \n\n* **Ripple-Carry Adder Delay:** In a ripple-carry adder, each full adder stage needs the carry-in from the previous stage. This \"rippling\" of the carry bit through the circuit is what causes the delay. The delay is roughly proportional to 'n' (number of bits), but the exact delay depends on the specific gate delays within each full adder stage.\n\n**In summary:** While the delay increases with the number of bits, there's no fixed formula like the ones provided in the options. \n"
            }
        ]
    },
    {
        "doc_id": 883,
        "doc": {
            "question_id": -1,
            "question": "The final addition sum of the numbers, 0110 & 0110 is ____",
            "options": [
                "1101",
                "1111",
                "1001",
                "1010"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The final addition sum of the numbers, 0110 & 0110 is ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\n0110 (decimal 6) + 0110 (decimal 6) = 1100 (decimal 12). However, the question asks for the \"final addition sum,\" implying we should represent the answer in binary as well. \n\n1100 in binary is equivalent to 1010. \n"
            }
        ]
    },
    {
        "doc_id": 884,
        "doc": {
            "question_id": -1,
            "question": "The delay reduced to in the carry look ahead adder is ____",
            "options": [
                "5",
                "8",
                "10",
                "2n"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The delay reduced to in the carry look ahead adder is ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **(A) 5**. Here's why:\n\n* **Carry Lookahead Adders (CLAs)** are designed to overcome the propagation delay limitation of ripple-carry adders.  \n* In a CLA, the carry bits for each bit position are calculated in parallel, significantly reducing the time it takes for the carry to propagate through the adder. \n* While the exact delay reduction can vary slightly based on implementation details, a well-designed 4-bit CLA can reduce the critical path delay to around **5 gate delays**.\n\nLet me know if you'd like a more detailed explanation of how CLAs achieve this speedup! \n"
            }
        ]
    },
    {
        "doc_id": 885,
        "doc": {
            "question_id": -1,
            "question": "The product of 1101 & 1011 is ____",
            "options": [
                "10001111",
                "10101010",
                "11110000",
                "11001100"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The product of 1101 & 1011 is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 886,
        "doc": {
            "question_id": -1,
            "question": "We make use of ____ circuits to implement multiplication.",
            "options": [
                "Flip flops",
                "Combinatorial",
                "Fast adders",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "We make use of ____ circuits to implement multiplication.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(B) Combinatorial**. Here's why:\n\n* **Combinatorial Circuits:**  These circuits produce an output based solely on the current input values. Multiplication, at its core, is a series of additions determined by the input bits. Combinatorial circuits are well-suited for this.\n\nLet's look at why the other options are incorrect:\n\n* **Flip flops (A):** Flip-flops are memory elements used for storing state information. They are essential for sequential circuits but don't directly implement the logic of multiplication.\n* **Fast adders (C):** While fast adders (like carry-lookahead adders) are used *within* multiplication circuits to speed up the addition steps, they don't represent the entire multiplication process.\n* **None of the mentioned (D):** This is incorrect, as combinatorial circuits are fundamental to multiplication implementation. \n\n**Therefore, the most accurate answer is (B).** \n"
            }
        ]
    },
    {
        "doc_id": 887,
        "doc": {
            "question_id": -1,
            "question": "The multiplier is stored in ____",
            "options": [
                "PC Register",
                "Shift register",
                "Cache",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The multiplier is stored in ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **D) None of the mentioned**. Here's why:\n\n* **PC Register (Program Counter):** Holds the address of the next instruction to be fetched.\n* **Shift Register:** Used for bitwise operations, particularly shifting bits left or right.\n* **Cache:** A small, fast memory that stores frequently used data for quicker access.\n\n**Where is the multiplier stored?**\n\nThe multiplier, like other data involved in arithmetic operations, is typically stored in a **register** within the **Arithmetic Logic Unit (ALU)**.  The ALU might have dedicated registers for operands like the multiplier and multiplicand, or it might use general-purpose registers.  \n\nLet me know if you'd like more details about computer architecture! \n"
            }
        ]
    },
    {
        "doc_id": 888,
        "doc": {
            "question_id": -1,
            "question": "The ____ is used to coordinate the operation of the multiplier.",
            "options": [
                "Controller",
                "Coordinator",
                "Control sequencer",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is used to coordinate the operation of the multiplier.",
                "Expected Label": "C",
                "Predicted Label": "C. \n\n**Control sequencer** is the most specific and accurate term for the component that manages the steps involved in multiplication within a computer's arithmetic logic unit (ALU). \n\nHere's why:\n\n* **Controller** is too general. While a controller oversees overall system operations, it doesn't specifically imply management of arithmetic processes.\n* **Coordinator** is similarly too broad and doesn't have a specific meaning in computer architecture.\n* **None of the mentioned** is incorrect because a dedicated component is necessary to handle the complex sequencing of multiplication. \n\nThe **control sequencer** ensures the correct fetching of operands, partial product generation, shifting, and final accumulation, forming the basis of the multiplication algorithm implemented in hardware. \n"
            }
        ]
    },
    {
        "doc_id": 889,
        "doc": {
            "question_id": -1,
            "question": "The multiplicand and the control signals are passed through to the n-bit adder via ____",
            "options": [
                "MUX",
                "DEMUX",
                "Encoder",
                "Decoder"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The multiplicand and the control signals are passed through to the n-bit adder via ____",
                "Expected Label": "A",
                "Predicted Label": "A) MUX \n\n**Explanation:**\n\n* **MUX (Multiplexer):** Selects one of several input signals and forwards the selected input to a single output line. In the context of multiplication, a MUX is used to select either the multiplicand or zero to be passed to the adder based on the control signals (usually derived from analyzing the multiplier bits).\n\nLet's eliminate the other options:\n\n* **DEMUX (Demultiplexer):** Performs the opposite function of a MUX, taking one input and routing it to one of several output lines.\n* **Encoder:**  Converts a set of input signals (like a decimal number) into a coded output (like binary).\n* **Decoder:** Performs the opposite function of an encoder.\n\n**In summary:** The MUX plays a crucial role in directing the correct data (multiplicand or zero) to the adder during different stages of the multiplication process. \n"
            }
        ]
    },
    {
        "doc_id": 890,
        "doc": {
            "question_id": -1,
            "question": "The product of -13 & 11 is ____",
            "options": [
                "1100110011",
                "1101110001",
                "1010101010",
                "1111111000"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The product of -13 & 11 is ____",
                "Expected Label": "B",
                "Predicted Label": "None of the provided options are the product of -13 & 11 in binary.  Here's why:\n\n* **The question asks for a product:** That means multiplication.\n* **The numbers are signed:**  -13 indicates we're likely dealing with two's complement representation.\n\nLet's calculate:\n\n1. **Convert to binary:** \n   * 13 in binary is 1101 \n   * -13 in two's complement is 10011 (invert the bits and add 1)\n   * 11 in binary is 1011\n\n2. **Multiply:**  Binary multiplication is more involved than the options suggest. The product will be negative and require more than 8 bits to represent.\n\n**Since none of the options are correct, there's no single letter answer (A, B, C, or D) to provide.** \n"
            }
        ]
    },
    {
        "doc_id": 891,
        "doc": {
            "question_id": -1,
            "question": "The bits 1 & 1 are recorded as ____ in bit-pair recording.",
            "options": [
                "-1",
                "0",
                "+1",
                "both -1 and 0"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The bits 1 & 1 are recorded as ____ in bit-pair recording.",
                "Expected Label": "D",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 892,
        "doc": {
            "question_id": -1,
            "question": "The multiplier -6(11010) is recorded as ____",
            "options": [
                "0-1-2",
                "0-1+1-10",
                "-2-10",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The multiplier -6(11010) is recorded as ____",
                "Expected Label": "A",
                "Predicted Label": "The correct answer is **B) 0-1+1-10**\n\nHere's why:\n\n* **Booth's Algorithm:**  The format \"0-1+1-10\" represents the recoded version of the multiplier (-6) using Booth's algorithm for signed multiplication. \n\nLet me explain how Booth's algorithm works in this case:\n\n1. **Two's Complement:**  First, we need the two's complement representation of -6:\n   * 6 in binary: 0110\n   * Invert bits:  1001\n   * Add 1:       1010 \n\n2. **Booth's Recoding:** Now, we apply Booth's recoding, examining the bits of the two's complement representation (1010) from right to left, including an implicit '0' at the end:\n\n   * **010:** This pattern becomes \"-1\" \n   * **101:** This pattern becomes \"+1\"\n   * **010:** This pattern becomes \"-1\"\n\nTherefore, -6 is recorded as 0-1+1-10 for multiplication using Booth's algorithm. \n"
            }
        ]
    },
    {
        "doc_id": 893,
        "doc": {
            "question_id": -1,
            "question": "CSA stands for?",
            "options": [
                "Computer Speed Addition",
                "Carry Save Addition",
                "Computer Service Architecture",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "CSA stands for?",
                "Expected Label": "A",
                "Predicted Label": "Answer: B \n\n**B) Carry Save Addition** \n\nCSA stands for Carry Save Addition, a technique used in computer arithmetic to speed up the addition of multiple numbers. \n"
            }
        ]
    },
    {
        "doc_id": 894,
        "doc": {
            "question_id": -1,
            "question": "The numbers written to the power of 10 in the representation of decimal numbers are called as ____",
            "options": [
                "Height factors",
                "Size factors",
                "Scale factors",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The numbers written to the power of 10 in the representation of decimal numbers are called as ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 895,
        "doc": {
            "question_id": -1,
            "question": "If the decimal point is placed to the right of the first significant digit, then the number is called ____",
            "options": [
                "Orthogonal",
                "Normalized",
                "Determinate",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If the decimal point is placed to the right of the first significant digit, then the number is called ____",
                "Expected Label": "B",
                "Predicted Label": "B) Normalized \n"
            }
        ]
    },
    {
        "doc_id": 896,
        "doc": {
            "question_id": -1,
            "question": "____ constitute the representation of the floating number.",
            "options": [
                "Sign",
                "Significant digits",
                "Scale factor",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ constitute the representation of the floating number.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 897,
        "doc": {
            "question_id": -1,
            "question": "The sign followed by the string of digits is called as ____",
            "options": [
                "Significant",
                "Determinant",
                "Mantissa",
                "Exponent"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The sign followed by the string of digits is called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 898,
        "doc": {
            "question_id": -1,
            "question": "In IEEE 32-bit representations, the mantissa of the fraction is said to occupy ____ bits.",
            "options": [
                "24",
                "23",
                "20",
                "16"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In IEEE 32-bit representations, the mantissa of the fraction is said to occupy ____ bits.",
                "Expected Label": "B",
                "Predicted Label": "B) 23 \n"
            }
        ]
    },
    {
        "doc_id": 899,
        "doc": {
            "question_id": -1,
            "question": "The 32 bit representation of the decimal number is called as ____",
            "options": [
                "Double-precision",
                "Single-precision",
                "Extended format",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The 32 bit representation of the decimal number is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 900,
        "doc": {
            "question_id": -1,
            "question": "In 32 bit representation the scale factor as a range of ____",
            "options": [
                "-128 to 127",
                "-256 to 255",
                "0 to 255",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In 32 bit representation the scale factor as a range of ____",
                "Expected Label": "A",
                "Predicted Label": "A) -128 to 127 \n\n**Explanation:**\n\nThe scale factor in 32-bit representation typically refers to the exponent in floating-point numbers (like IEEE 754).  In that standard, the exponent is stored with a bias. For 32-bit floats, the 8-bit exponent has a bias of 127, resulting in a range of:\n\n* **-127 (00000001) to 128 (11111110)**  before considering special cases like zero and infinity.\n\nWhile not exactly -128 to 127, option A is the closest and most likely answer in a general computer architecture context. \n"
            }
        ]
    },
    {
        "doc_id": 901,
        "doc": {
            "question_id": -1,
            "question": "In double precision format, the size of the mantissa is ____",
            "options": [
                "32 bit",
                "52 bit",
                "64 bit",
                "72 bit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In double precision format, the size of the mantissa is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 902,
        "doc": {
            "question_id": -1,
            "question": "____  have been developed specifically for pipelined systems.",
            "options": [
                "Utility software",
                "Speed up utilities",
                "Optimizing compilers",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____  have been developed specifically for pipelined systems.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 903,
        "doc": {
            "question_id": -1,
            "question": "The pipelining process is also called as ____",
            "options": [
                "Superscalar operation",
                "Assembly line operation",
                "Von Neumann cycle",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The pipelining process is also called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 904,
        "doc": {
            "question_id": -1,
            "question": "The fetch and execution cycles are interleaved with the help of ____",
            "options": [
                "Modification in processor architecture",
                "Clock",
                "Special unit",
                "Control unit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The fetch and execution cycles are interleaved with the help of ____",
                "Expected Label": "B",
                "Predicted Label": "D \n\n**Explanation:**\n\nThe **control unit** is the component responsible for orchestrating the fetch-execute cycle. It handles:\n\n* **Fetching:** Sending the address of the next instruction to memory and retrieving it.\n* **Decoding:** Interpreting the fetched instruction to determine the operation and operands.\n* **Executing:** Sending signals to the relevant parts of the processor (like the ALU) to perform the operation.\n\nWhile the clock **(B)** provides timing signals, it doesn't directly manage the interleaving. Modifications in processor architecture **(A)** might improve efficiency but don't define the core interleaving mechanism. A special unit **(C)** isn't a standard concept in this context. \n"
            }
        ]
    },
    {
        "doc_id": 905,
        "doc": {
            "question_id": -1,
            "question": "Each stage in pipelining should be completed within ____ cycle.",
            "options": [
                "1",
                "2",
                "3",
                "4"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Each stage in pipelining should be completed within ____ cycle.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 906,
        "doc": {
            "question_id": -1,
            "question": "If a unit completes its task before the allotted time period, then ____",
            "options": [
                "It\u2019ll perform some other task in the remaining time",
                "Its time gets reallocated to a different task",
                "It\u2019ll remain idle for the remaining time",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "If a unit completes its task before the allotted time period, then ____",
                "Expected Label": "C",
                "Predicted Label": "Answer: **C) It\u2019ll remain idle for the remaining time** \n\n**Explanation:**\n\nIn most computer architectures, if a unit (like a processing unit or a memory controller) finishes its task early, it doesn't automatically pick up another task or reallocate its remaining time. Instead, it enters an idle state, waiting for the next instruction or clock cycle to begin its next assigned task. \n"
            }
        ]
    },
    {
        "doc_id": 907,
        "doc": {
            "question_id": -1,
            "question": "To increase the speed of memory access in pipelining, we make use of ____",
            "options": [
                "Special memory locations",
                "Special purpose registers",
                "Cache",
                "Buffers"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To increase the speed of memory access in pipelining, we make use of ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Cache** is a small, fast memory that stores frequently accessed data and instructions from the main memory. By keeping this data readily available, the CPU can access it much faster, significantly improving pipeline performance. \n\nLet's look at why the other options aren't the best fit:\n\n* **A) Special memory locations:** While specific memory regions might be reserved for certain purposes, they don't inherently speed up pipelined memory access.\n* **B) Special purpose registers:** Registers are extremely fast, but their primary role is for holding data immediately used by the CPU, not for generally speeding up memory access in a pipeline.\n* **D) Buffers:** Buffers temporarily store data during transfers, which can help with speed differences between components. However, they aren't the primary mechanism for accelerating memory access in pipelining. \n"
            }
        ]
    },
    {
        "doc_id": 908,
        "doc": {
            "question_id": -1,
            "question": "The periods of time when the unit is idle is called as ____",
            "options": [
                "Stalls",
                "Bubbles",
                "Hazards",
                "Both Stalls and Bubbles"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The periods of time when the unit is idle is called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 909,
        "doc": {
            "question_id": -1,
            "question": "The contention for the usage of a hardware device is called ____",
            "options": [
                "Structural hazard",
                "Stalk",
                "Deadlock",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The contention for the usage of a hardware device is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Structural hazard** occurs when multiple instructions in a pipeline require the same hardware resource simultaneously, leading to contention and execution stalls.\n\nLet's break down why the other options are incorrect:\n\n* **Stalk** is not a recognized term in computer architecture.\n* **Deadlock** refers to a situation where two or more processes are blocked indefinitely, each waiting for a resource held by another. This is more of an operating system concept.\n* **None of the mentioned** is incorrect because structural hazard is the accurate term. \n"
            }
        ]
    },
    {
        "doc_id": 910,
        "doc": {
            "question_id": -1,
            "question": "The situation wherein the data of operands are not available is called ____",
            "options": [
                "Data hazard",
                "Stock",
                "Deadlock",
                "Structural hazard"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The situation wherein the data of operands are not available is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 911,
        "doc": {
            "question_id": -1,
            "question": "The throughput of a super scalar processor is ____",
            "options": [
                "less than 1",
                "1",
                "More than 1",
                "Not Known"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The throughput of a super scalar processor is ____",
                "Expected Label": "C",
                "Predicted Label": "**C) More than 1** \n\n**Explanation:**\n\nSuperscalar processors are designed to execute multiple instructions per clock cycle, achieving a throughput greater than 1. This is accomplished through techniques like instruction pipelining and multiple execution units. \n"
            }
        ]
    },
    {
        "doc_id": 912,
        "doc": {
            "question_id": -1,
            "question": "When the processor executes multiple instructions at a time it is said to use ____",
            "options": [
                "single issue",
                "Multiplicity",
                "Visualization",
                "Multiple issues"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When the processor executes multiple instructions at a time it is said to use ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 913,
        "doc": {
            "question_id": -1,
            "question": "The ____ plays a very vital role in case of super scalar processors.",
            "options": [
                "Compilers",
                "Motherboard",
                "Memory",
                "Peripherals"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ plays a very vital role in case of super scalar processors.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nSuper scalar processors rely heavily on compilers to identify and schedule instructions that can be executed in parallel. Here's why:\n\n* **Instruction-Level Parallelism (ILP):** Superscalar architectures exploit ILP, meaning they can execute multiple instructions simultaneously if there are no dependencies.\n* **Compiler's Role:** Compilers analyze the code and rearrange instructions (while preserving program order) to maximize the ILP that the superscalar processor can exploit. They bundle independent instructions together for parallel execution.\n\nLet's look at why the other options are incorrect:\n\n* **Motherboard:** Provides the physical connections between components but doesn't directly influence superscalar execution.\n* **Memory:**  While important for performance, memory systems are not the primary factor defining superscalar processor behavior.\n* **Peripherals:**  External devices that interact with the processor but are not central to superscalar execution. \n"
            }
        ]
    },
    {
        "doc_id": 914,
        "doc": {
            "question_id": -1,
            "question": "If an exception is raised and the succeeding instructions are executed completely, then the processor is said to have ____",
            "options": [
                "Exception handling",
                "Imprecise exceptions",
                "Error correction",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If an exception is raised and the succeeding instructions are executed completely, then the processor is said to have ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Imprecise exceptions** occur when the processor doesn't stop immediately upon encountering an exception. Instead, it might continue executing subsequent instructions, leading to an undefined state where it's unclear which instructions completed before the exception.\n\nLet's look at why the other options are incorrect:\n\n* **A) Exception handling:** This is the general process of dealing with exceptions, not specifically the scenario described. \n* **C) Error correction:**  This refers to mechanisms that detect and fix errors, not the behavior during an exception.\n* **D) None of the mentioned:** Option B accurately describes the situation. \n\n**Therefore, \"Imprecise exceptions\" (B) is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 915,
        "doc": {
            "question_id": -1,
            "question": "In super-scalar processors, ____ mode of execution is used.",
            "options": [
                "In-order",
                "Post order",
                "Out of order",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In super-scalar processors, ____ mode of execution is used.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C\n"
            }
        ]
    },
    {
        "doc_id": 916,
        "doc": {
            "question_id": -1,
            "question": "Since it uses the out of order mode of execution, the results are stored in ____",
            "options": [
                "Buffers",
                "Special memory locations",
                "Temporary registers",
                "TLB"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Since it uses the out of order mode of execution, the results are stored in ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 917,
        "doc": {
            "question_id": -1,
            "question": "The step where in the results stored in the temporary register is transferred into the permanent register is called as ____",
            "options": [
                "Final step",
                "Commitment step",
                "Last step",
                "Inception step"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The step where in the results stored in the temporary register is transferred into the permanent register is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 918,
        "doc": {
            "question_id": -1,
            "question": "A special unit used to govern the out of order execution of the instructions is called as ____",
            "options": [
                "Commitment unit",
                "Temporal unit",
                "Monitor",
                "Supervisory unit"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A special unit used to govern the out of order execution of the instructions is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 919,
        "doc": {
            "question_id": -1,
            "question": "The commitment unit uses a queue called ____",
            "options": [
                "Record buffer",
                "Commitment buffer",
                "Storage buffer",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The commitment unit uses a queue called ____",
                "Expected Label": "A",
                "Predicted Label": "B. Commitment buffer \n"
            }
        ]
    },
    {
        "doc_id": 920,
        "doc": {
            "question_id": -1,
            "question": "The CISC stands for ____",
            "options": [
                "Computer Instruction Set Compliment",
                "Complete Instruction Set Compliment",
                "Computer Indexed Set Components",
                "Complex Instruction set computer"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The CISC stands for ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 921,
        "doc": {
            "question_id": -1,
            "question": "The computer architecture aimed at reducing the time of execution of instructions is ____",
            "options": [
                "CISC",
                "RISC",
                "ISA",
                "ANNA"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The computer architecture aimed at reducing the time of execution of instructions is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 922,
        "doc": {
            "question_id": -1,
            "question": "The Sun micro systems processors usually follow ____ architecture.",
            "options": [
                "CISC",
                "ISA",
                "ULTRA SPARC",
                "RISC"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The Sun micro systems processors usually follow ____ architecture.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **RISC (Reduced Instruction Set Computing)** is a processor design philosophy emphasizing simpler and faster instructions. Sun Microsystems' SPARC processors, including ULTRA SPARC, are well-known examples of RISC architecture. \n\nLet's break down why the other options are incorrect:\n\n* **CISC (Complex Instruction Set Computing)** is the opposite of RISC, using more complex instructions. While some might consider older SPARC generations having some CISC traits, it's not their defining characteristic.\n* **ISA (Instruction Set Architecture)** is a broader term referring to the set of instructions a processor understands.  While SPARC has its own ISA, it doesn't define the underlying architectural philosophy (RISC).\n* **ULTRA SPARC** is a specific implementation within the SPARC family, not the overarching architectural approach. \n"
            }
        ]
    },
    {
        "doc_id": 923,
        "doc": {
            "question_id": -1,
            "question": "The iconic feature of the RISC machine among the following is ____",
            "options": [
                "Reduced number of addressing modes",
                "Increased memory size",
                "Having a branch delay slot",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The iconic feature of the RISC machine among the following is ____",
                "Expected Label": "C",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 924,
        "doc": {
            "question_id": -1,
            "question": "Both the CISC and RISC architectures have been developed to reduce the ____",
            "options": [
                "Cost",
                "Time delay",
                "Semantic gap",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Both the CISC and RISC architectures have been developed to reduce the ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 925,
        "doc": {
            "question_id": -1,
            "question": "Out of the following which is not a CISC machine.",
            "options": [
                "IBM 370/168",
                "VAX 11/780",
                "Intel 80486",
                "Motorola A567"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Out of the following which is not a CISC machine.",
                "Expected Label": "D",
                "Predicted Label": "D) Motorola A567 \n\n**Explanation:**\n\nThe Motorola A567 is a Digital Signal Processor (DSP), typically associated with a Reduced Instruction Set Computing (RISC) architecture.  The other options are known for their Complex Instruction Set Computing (CISC) architectures. \n"
            }
        ]
    },
    {
        "doc_id": 926,
        "doc": {
            "question_id": -1,
            "question": "Pipe-lining is a unique feature of ____",
            "options": [
                "RISC",
                "CISC",
                "ISA",
                "IANA"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Pipe-lining is a unique feature of ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 927,
        "doc": {
            "question_id": -1,
            "question": "In CISC architecture most of the complex instructions are stored in ____",
            "options": [
                "Register",
                "Diodes",
                "CMOS",
                "Transistors"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In CISC architecture most of the complex instructions are stored in ____",
                "Expected Label": "D",
                "Predicted Label": "The answer is **A) Register**. Here's why:\n\n* **Registers** are small, extremely fast memory locations within the CPU.  CISC architectures often utilize microcode to implement complex instructions. This microcode, which is essentially a sequence of simpler instructions, is stored in control memory, a specific type of register. \n\nLet's look at why the other options are incorrect:\n\n* **Diodes** are basic electronic components that allow current flow in one direction. They are not used for storing instructions.\n* **CMOS** (Complementary Metal-Oxide-Semiconductor) is a technology used for manufacturing transistors, which are the building blocks of processors. While CMOS plays a role in the physical implementation of a CPU, it's not where instructions are stored.\n* **Transistors** are the fundamental switching elements within a CPU. They are used to create logic gates and memory cells, but individual transistors don't store complex instructions. \n\n**Therefore, the most accurate answer is A) Register.** \n"
            }
        ]
    },
    {
        "doc_id": 928,
        "doc": {
            "question_id": -1,
            "question": "Which of the architecture is power efficient?",
            "options": [
                "CISC",
                "RISC",
                "ISA",
                "IANA"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Which of the architecture is power efficient?",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **RISC (Reduced Instruction Set Computing)** architectures are generally more power efficient than CISC architectures due to their simpler instructions and emphasis on efficient pipeline execution.\n\nLet's break down why the other options are incorrect:\n\n* **CISC (Complex Instruction Set Computing)** architectures tend to use more complex instructions, which can require more clock cycles and power to execute.\n* **ISA (Instruction Set Architecture)** is a specification that defines the instructions a processor can execute. It's not an architecture itself and doesn't directly relate to power efficiency.\n* **IANA (Internet Assigned Numbers Authority)** is an organization responsible for allocating IP addresses and other internet resources. It's not related to computer architecture or power efficiency. \n"
            }
        ]
    },
    {
        "doc_id": 929,
        "doc": {
            "question_id": -1,
            "question": "Any condition that causes a processor to stall is called as ____",
            "options": [
                "Hazard",
                "Page fault",
                "System error",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Any condition that causes a processor to stall is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 930,
        "doc": {
            "question_id": -1,
            "question": "The periods of time when the unit is idle is called as ____",
            "options": [
                "Stalls",
                "Bubbles",
                "Hazards",
                "Both Stalls and Bubbles"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The periods of time when the unit is idle is called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 931,
        "doc": {
            "question_id": -1,
            "question": "The contention for the usage of a hardware device is called ____",
            "options": [
                "Structural hazard",
                "Stalk",
                "Deadlock",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The contention for the usage of a hardware device is called ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Structural hazard**. Here's why:\n\n* **Structural hazard:** Occurs when multiple instructions in a pipeline need the same hardware resource (like a memory bus or ALU) at the same time. This causes a stall, preventing the pipeline from operating at its full potential.\n* **Stalk:** Not a recognized term in computer architecture related to hardware contention.\n* **Deadlock:**  A situation where two or more processes are blocked indefinitely, each waiting for the other to release the resources that it needs. While related to resource contention, it's a broader concept than just hardware. \n* **None of the mentioned:** Incorrect, as structural hazard is the correct term.\n\n**Therefore, the most accurate answer is A.** \n"
            }
        ]
    },
    {
        "doc_id": 932,
        "doc": {
            "question_id": -1,
            "question": "The situation wherein the data of operands are not available is called ____",
            "options": [
                "Data hazard",
                "Stock",
                "Deadlock",
                "Structural hazard"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The situation wherein the data of operands are not available is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 933,
        "doc": {
            "question_id": -1,
            "question": "The stalling of the processor due to the unavailability of the instructions is called as ____",
            "options": [
                "Control hazard",
                "structural hazard",
                "Input hazard",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The stalling of the processor due to the unavailability of the instructions is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 934,
        "doc": {
            "question_id": -1,
            "question": "The time lost due to the branch instruction is often referred to as ____",
            "options": [
                "Latency",
                "Delay",
                "Branch penalty",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The time lost due to the branch instruction is often referred to as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 935,
        "doc": {
            "question_id": -1,
            "question": "____ method is used in centralized systems to perform out of order execution.",
            "options": [
                "Scorecard",
                "Score boarding",
                "Optimizing",
                "Redundancy"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ method is used in centralized systems to perform out of order execution.",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 936,
        "doc": {
            "question_id": -1,
            "question": "The algorithm followed in most of the systems to perform out of order execution is ____",
            "options": [
                "Tomasulo algorithm",
                "Score carding",
                "Reader-writer algorithm",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The algorithm followed in most of the systems to perform out of order execution is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nThe Tomasulo algorithm is a widely used approach for implementing out-of-order execution in computer architectures. It's known for its efficient handling of data dependencies and resource allocation, making it a fundamental concept in modern processor design. \n\nLet's look at why the other options are incorrect:\n\n* **Scoreboarding** is another technique for out-of-order execution, but it's less sophisticated than Tomasulo's algorithm.\n* **Reader-writer algorithm** is related to concurrency control in operating systems and databases, not directly to out-of-order execution.\n* **None of the mentioned** is incorrect because the Tomasulo algorithm is a specific and relevant answer. \n"
            }
        ]
    },
    {
        "doc_id": 937,
        "doc": {
            "question_id": -1,
            "question": "The problem where process concurrency becomes an issue is called as ____",
            "options": [
                "Philosophers problem",
                "Bakery problem",
                "Bankers problem",
                "Reader-writer problem"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The problem where process concurrency becomes an issue is called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 938,
        "doc": {
            "question_id": -1,
            "question": "The set of loosely connected computers are called as ____",
            "options": [
                "LAN",
                "WAN",
                "Workstation",
                "Cluster"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The set of loosely connected computers are called as ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 939,
        "doc": {
            "question_id": -1,
            "question": "Each computer in a cluster is connected using ____",
            "options": [
                "UTP",
                "Rj-45",
                "STP",
                "Coaxial cable"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Each computer in a cluster is connected using ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **A) UTP**. Here's why:\n\n* **UTP (Unshielded Twisted Pair):** This is the most common cabling used in Ethernet networks, which are standard for connecting computers in a cluster. \n\nLet's look at why the other options are less likely:\n\n* **B) RJ-45:** This is a connector type (the plastic plug), not the cable itself. UTP cables often use RJ-45 connectors.\n* **C) STP (Shielded Twisted Pair):** While STP offers better protection against interference, it's more expensive and less common than UTP, especially in standard cluster setups.\n* **D) Coaxial cable:**  Coaxial cable was used in older Ethernet networks but has largely been replaced by UTP due to its lower cost and ease of installation. \n\n**Therefore, UTP is the most common and practical cabling solution for connecting computers in a cluster.** \n"
            }
        ]
    },
    {
        "doc_id": 940,
        "doc": {
            "question_id": -1,
            "question": "The computer cluster architecture emerged as a result of ____",
            "options": [
                "ISA",
                "Workstation",
                "Super computers",
                "Distributed systems"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The computer cluster architecture emerged as a result of ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 941,
        "doc": {
            "question_id": -1,
            "question": "The software which governs the group of computers is ____",
            "options": [
                "Driver Rd45",
                "Interface UI",
                "Clustering middleware",
                "Distributor"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The software which governs the group of computers is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 942,
        "doc": {
            "question_id": -1,
            "question": "The simplest form of a cluster is ____ approach.",
            "options": [
                "Beowolf",
                "Sequoia",
                "Stone",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The simplest form of a cluster is ____ approach.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 943,
        "doc": {
            "question_id": -1,
            "question": "The cluster formation in which the work is divided equally among the systems is ____",
            "options": [
                "Load-configuration",
                "Load-Division",
                "Light head",
                "Both Load-configuration and Load-Division"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The cluster formation in which the work is divided equally among the systems is ____",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 944,
        "doc": {
            "question_id": -1,
            "question": "In the client server model of the cluster ____ approach is used.",
            "options": [
                "Load configuration",
                "FIFO",
                "Bankers algorithm",
                "Round robin"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In the client server model of the cluster ____ approach is used.",
                "Expected Label": "D",
                "Predicted Label": "**D) Round robin** \n\n**Explanation:**\n\nThe round-robin approach is commonly used in cluster computing to distribute workload evenly across available servers. It ensures fairness and helps prevent one server from becoming overloaded while others remain idle. \n\nLet's look at why the other options are less suitable:\n\n* **A) Load configuration:** While load configuration is important in a cluster, it's a broader concept and doesn't specifically describe the client-server interaction model.\n* **B) FIFO (First-In, First-Out):** FIFO is a queuing algorithm, not a client-server distribution method. It might be used within a server but not for distributing tasks across a cluster.\n* **C) Banker's algorithm:** The Banker's algorithm is used for deadlock avoidance in resource allocation, not specifically for client-server communication in a cluster. \n\nTherefore, the most accurate answer is **D) Round robin**. \n"
            }
        ]
    },
    {
        "doc_id": 945,
        "doc": {
            "question_id": -1,
            "question": "The beowolf structure follows the ____ approach of a relationship between the systems.",
            "options": [
                "Master-slave",
                "Asynchronous",
                "Synchronous",
                "Isochronous"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The beowolf structure follows the ____ approach of a relationship between the systems.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nBeowulf clusters are known for their master-slave architecture.  Here's why:\n\n* **Master-Slave:** The master node controls and distributes tasks to the slave nodes, which perform the computations. This hierarchical relationship is fundamental to Beowulf's design.\n\nLet's look at why the other options are incorrect:\n\n* **Asynchronous/Synchronous:**  These terms refer to how timing and communication are handled between components. While relevant to cluster operation, they don't define the core relationship between systems like the master-slave model does. \n* **Isochronous:** This term relates to data transmission with guaranteed timing, often used in multimedia streaming. It's not directly applicable to the architecture of Beowulf clusters. \n"
            }
        ]
    },
    {
        "doc_id": 946,
        "doc": {
            "question_id": -1,
            "question": "The most common modes of communication in clusters are ____",
            "options": [
                "Message queues",
                "Message passing interface",
                "PVm",
                "Both Message passing interface and PVm"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The most common modes of communication in clusters are ____",
                "Expected Label": "D",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 947,
        "doc": {
            "question_id": -1,
            "question": "The method followed in case of node failure, wherein the node gets disabled is ____",
            "options": [
                "STONITH",
                "Fibre channel",
                "Fencing",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The method followed in case of node failure, wherein the node gets disabled is ____",
                "Expected Label": "A",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 948,
        "doc": {
            "question_id": -1,
            "question": "VLIW stands for?",
            "options": [
                "Very Long Instruction Word",
                "Very Long Instruction Width",
                "Very Large Instruction Word",
                "Very Long Instruction Width"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "VLIW stands for?",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 949,
        "doc": {
            "question_id": -1,
            "question": "The important feature of the VLIW is ____",
            "options": [
                "ILP",
                "Cost effectiveness",
                "Performance",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The important feature of the VLIW is ____",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 950,
        "doc": {
            "question_id": -1,
            "question": "The main difference between the VLIW and the other approaches to improve performance is ____",
            "options": [
                "Cost effectiveness",
                "Increase in performance",
                "Lack of complex hardware design",
                "All of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The main difference between the VLIW and the other approaches to improve performance is ____",
                "Expected Label": "C",
                "Predicted Label": "**C) Lack of complex hardware design**\n\n**Explanation:**\n\nWhile VLIW aims to improve performance, its defining characteristic compared to other approaches like superscalar or out-of-order execution is the **simplification of hardware**.  VLIW pushes the complexity of instruction scheduling onto the compiler. This means:\n\n* **Less complex hardware:** No dynamic scheduling logic is needed in the CPU, making it potentially cheaper and more energy-efficient.\n* **Compiler burden:**  The compiler has the challenging job of analyzing code and packing independent instructions into VLIW bundles.\n\nLet me know if you'd like a more detailed explanation of VLIW architecture! \n"
            }
        ]
    },
    {
        "doc_id": 951,
        "doc": {
            "question_id": -1,
            "question": "The parallel execution of operations in VLIW is done according to the schedule determined by ____",
            "options": [
                "Task scheduler",
                "Interpreter",
                "Compiler",
                "Encoder"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The parallel execution of operations in VLIW is done according to the schedule determined by ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\nHere's why:\n\n* **VLIW (Very Long Instruction Word)** architectures rely heavily on the **compiler** to analyze code and identify instructions that can be executed in parallel. \n* The compiler then packs these independent instructions into very long instruction words, effectively creating the parallel execution schedule.\n\nLet me know if you'd like more detail on VLIW or any other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 952,
        "doc": {
            "question_id": -1,
            "question": "The VLIW processors are much simpler as they do not require of ____",
            "options": [
                "Computational register",
                "Complex logic circuits",
                "SSD slots",
                "Scheduling hardware"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The VLIW processors are much simpler as they do not require of ____",
                "Expected Label": "D",
                "Predicted Label": "D) Scheduling hardware \n\n**Explanation:**\n\nVLIW (Very Long Instruction Word) processors are designed to achieve high performance through instruction-level parallelism. Unlike traditional processors, VLIW architectures rely on the compiler to schedule instructions in parallel and bundle them into long instruction words.  This means the hardware itself does **not** need complex scheduling logic. \n \nHere's why the other options are incorrect:\n\n* **A) Computational register:**  VLIW processors still require computational registers to store data and instructions.\n* **B) Complex logic circuits:** While simplified in scheduling, VLIW processors still require complex logic for other functions like execution units and memory access.\n* **C) SSD slots:**  SSD slots are a storage interface and completely unrelated to VLIW processor architecture. \n"
            }
        ]
    },
    {
        "doc_id": 953,
        "doc": {
            "question_id": -1,
            "question": "The VLIW architecture follows ____ approach to achieve parallelism.",
            "options": [
                "MISD",
                "SISD",
                "SIMD",
                "MIMD"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The VLIW architecture follows ____ approach to achieve parallelism.",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 954,
        "doc": {
            "question_id": -1,
            "question": "To compute the direction of the branch the VLIW uses ____",
            "options": [
                "Seekers",
                "Heuristics",
                "Direction counter",
                "Compass"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To compute the direction of the branch the VLIW uses ____",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Heuristics**. Here's why:\n\n* **VLIW (Very Long Instruction Word)** architectures rely on the compiler to determine instruction dependencies and branch directions at compile time. \n\n* **Heuristics** are algorithms that use practical rules of thumb or estimations to make decisions, often used in situations with incomplete information. Compilers use heuristics to predict branch directions based on code analysis and probability.\n\nLet's look at why the other options are incorrect:\n\n* **Seekers, Direction counters, and Compasses** are not concepts used in computer architecture for branch prediction. \n\n**In summary:** VLIW architectures offload the burden of branch prediction to the compiler, which uses heuristics to make educated guesses about branch directions during the compilation process. \n"
            }
        ]
    },
    {
        "doc_id": 955,
        "doc": {
            "question_id": -1,
            "question": "EPIC stands for?",
            "options": [
                "Explicitly Parallel Instruction Computing",
                "External Peripheral Integrating Component",
                "External Parallel Instruction Computing",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "EPIC stands for?",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 956,
        "doc": {
            "question_id": -1,
            "question": "For converting a virtual address into the physical address, the programs are divided into ____",
            "options": [
                "Pages",
                "Frames",
                "Segments",
                "Blocks"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "For converting a virtual address into the physical address, the programs are divided into ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 957,
        "doc": {
            "question_id": -1,
            "question": "The pages size shouldn\u2019t be too small, as this would lead to ____",
            "options": [
                "Transfer errors",
                "Increase in operation time",
                "Increase in access time",
                "Decrease in performance"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The pages size shouldn\u2019t be too small, as this would lead to ____",
                "Expected Label": "C",
                "Predicted Label": "D) Decrease in performance \n\n**Explanation:**\n\nWhile all the options might seem plausible at first glance, here's why the most accurate answer is **D**:\n\n* **Smaller page sizes lead to more pages:**  A program needs a certain amount of memory to run. With smaller page sizes, the same program is divided into many more pages. \n* **More pages lead to more page faults:** When a program needs data not currently in physical memory (RAM), a page fault occurs. The operating system then has to retrieve the needed page from secondary storage (hard drive), which is much slower. \n* **More page faults decrease performance:**  The constant swapping of pages between RAM and the hard drive (due to increased page faults) significantly slows down program execution, leading to decreased overall performance.\n\n**Why other options are less accurate:**\n\n* **A) Transfer errors:**  Page size doesn't directly correlate with transfer errors. Errors are more likely related to hardware malfunctions or data corruption.\n* **B) Increase in operation time:**  While technically correct, \"operation time\" is too general. The more specific and significant impact is on performance due to increased page faults.\n* **C) Increase in access time:** Similar to option B, this is true but doesn't capture the full picture. Increased page faults lead to longer access times, ultimately impacting overall performance. \n"
            }
        ]
    },
    {
        "doc_id": 958,
        "doc": {
            "question_id": -1,
            "question": "The cache bridges the speed gap between ____ and ____",
            "options": [
                "RAM and ROM",
                "RAM and Secondary memory",
                "Processor and RAM",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The cache bridges the speed gap between ____ and ____",
                "Expected Label": "C",
                "Predicted Label": "C) Processor and RAM \n"
            }
        ]
    },
    {
        "doc_id": 959,
        "doc": {
            "question_id": -1,
            "question": "The virtual memory bridges the size and speed gap between ____ and ____",
            "options": [
                "RAM and ROM",
                "RAM and Secondary memory",
                "Processor and RAM",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The virtual memory bridges the size and speed gap between ____ and ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 960,
        "doc": {
            "question_id": -1,
            "question": "The higher order bits of the virtual address generated by the processor forms the ____",
            "options": [
                "Table number",
                "Frame number",
                "List number",
                "Page number"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The higher order bits of the virtual address generated by the processor forms the ____",
                "Expected Label": "D",
                "Predicted Label": "A) Table number \n\nWhile the concept is more nuanced in modern architectures, the core idea is that the higher order bits are used to index into a page table.  So \"Table Number\" is the closest fit among the choices. \n"
            }
        ]
    },
    {
        "doc_id": 961,
        "doc": {
            "question_id": -1,
            "question": "The page length shouldn\u2019t be too long because ____",
            "options": [
                "It reduces the program efficiency",
                "It increases the access time",
                "It leads to wastage of memory",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The page length shouldn\u2019t be too long because ____",
                "Expected Label": "C",
                "Predicted Label": "C) It leads to wastage of memory \n\n**Explanation:**\n\nWhile overly long page lengths can have negative impacts on program efficiency and access time, the most direct consequence is **memory wastage (internal fragmentation)**.  Here's why:\n\n* **Internal Fragmentation:** When a process is loaded into memory, it's allocated a whole number of pages. If the page size is too large, and the process doesn't fully utilize the last allocated page, you end up with wasted memory within that page.\n\nLet's illustrate: Imagine a page size of 4KB. If a process needs 5KB, it will be allocated two pages, but 3KB of the second page will remain unused.\n\n**Other options:**\n\n* **A) It reduces the program efficiency:**  Large pages can actually improve efficiency by reducing the number of page faults (when the system needs to access data not currently in physical memory). However, this benefit diminishes as page size increases excessively.\n* **B) It increases the access time:** Larger pages can lead to slightly higher access times due to increased search time within a page. This effect is usually minimal. \n* **D) None of the mentioned:**  This is incorrect, as memory wastage is a significant concern with large page sizes. \n"
            }
        ]
    },
    {
        "doc_id": 962,
        "doc": {
            "question_id": -1,
            "question": "The lower order bits of the virtual address forms the ____",
            "options": [
                "Page number",
                "Frame number",
                "Block number",
                "Offset"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The lower order bits of the virtual address forms the ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 963,
        "doc": {
            "question_id": -1,
            "question": "The area in the main memory that can hold one page is called as ____",
            "options": [
                "Page entry",
                "Page frame",
                "Frame",
                "Block"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The area in the main memory that can hold one page is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Page frame** is the correct term for the area in main memory that holds a single page. \n\nHere's why the other options are incorrect:\n\n* **Page entry:**  This refers to a record in the page table that contains information about a specific virtual page, including its location in physical memory (if it's loaded).\n* **Frame:** While sometimes used interchangeably with \"page frame,\" it's less precise. \"Frame\" can refer to any fixed-size chunk of memory, not just those used for paging.\n* **Block:** This term is often used in the context of disk storage or caching, referring to a fixed-size unit of data. \n\nLet me know if you'd like more details on virtual memory or paging! \n"
            }
        ]
    },
    {
        "doc_id": 964,
        "doc": {
            "question_id": -1,
            "question": "The starting address of the page table is stored in ____",
            "options": [
                "TLB",
                "R0",
                "Page table base register",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The starting address of the page table is stored in ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe **Page Table Base Register (PTBR)** is a specialized register within the Memory Management Unit (MMU) that holds the starting address of the page table.  This is crucial for virtual memory translation:\n\n* When a virtual address needs to be translated to a physical address, the MMU uses the PTBR to locate the page table in memory. \n\nLet's look at why the other options are incorrect:\n\n* **A) TLB (Translation Lookaside Buffer):** The TLB is a cache that stores recent virtual-to-physical address translations to speed up the process. It doesn't store the page table's starting address.\n* **B) R0 (Register 0):**  R0 is a general-purpose register and typically doesn't have a dedicated role in storing the page table's base address.\n* **D) None of the mentioned:** This is incorrect, as the PTBR specifically serves this purpose. \n"
            }
        ]
    },
    {
        "doc_id": 965,
        "doc": {
            "question_id": -1,
            "question": "The bits used to indicate the status of the page in the memory is called ____",
            "options": [
                "Control bits",
                "Status bits",
                "Progress bit",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The bits used to indicate the status of the page in the memory is called ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Control bits** within a page table entry hold information about the page's status, such as whether it's valid, modified, or being accessed. \n"
            }
        ]
    },
    {
        "doc_id": 966,
        "doc": {
            "question_id": -1,
            "question": "The ____ bit is used to indicate the validity of the page.",
            "options": [
                "Valid bit",
                "Invalid bit",
                "Correct bit",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ bit is used to indicate the validity of the page.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 967,
        "doc": {
            "question_id": -1,
            "question": "The bit used to store whether the page has been modified or not is called as ____",
            "options": [
                "Dirty bit",
                "Modify bit",
                "Relocation bit",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The bit used to store whether the page has been modified or not is called as ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 968,
        "doc": {
            "question_id": -1,
            "question": "The page table should be ideally situated within ____",
            "options": [
                "Processor",
                "TLB",
                "MMU",
                "Cache"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The page table should be ideally situated within ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile all the components listed play a role in memory management, the page table itself is most closely associated with the **Memory Management Unit (MMU)**. \n\n* **MMU (Correct)**: The MMU is responsible for translating virtual addresses to physical addresses, and the page table is the data structure it uses to perform this translation.\n* **Processor**: The processor initiates memory requests, but doesn't typically store the entire page table.\n* **TLB**: The Translation Lookaside Buffer is a cache within the MMU that stores recent translations for faster access. \n* **Cache**: The cache stores frequently used data and instructions, but not the page table itself. \n"
            }
        ]
    },
    {
        "doc_id": 969,
        "doc": {
            "question_id": -1,
            "question": "If the page table is large then it is stored in ____",
            "options": [
                "Processor",
                "Main memory",
                "Disk",
                "Secondary storage"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "If the page table is large then it is stored in ____",
                "Expected Label": "B",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 970,
        "doc": {
            "question_id": -1,
            "question": "When the page table is placed in the main memory, the ____ is used to store the recently accessed pages.",
            "options": [
                "MMU",
                "TLB",
                "R0",
                "Table"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "When the page table is placed in the main memory, the ____ is used to store the recently accessed pages.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **TLB (Translation Lookaside Buffer)** is a specialized cache within the MMU that stores recent page table entries. This speeds up address translation as the MMU can check the TLB first before accessing the slower main memory. \n\nLet's break down the other options:\n\n* **MMU (Memory Management Unit)**: The MMU is the hardware component responsible for address translation, but it doesn't specifically store recently accessed pages.\n* **R0**: This likely refers to a general-purpose register and is not related to page table storage.\n* **Table**:  This is too generic. While the page table itself stores the mapping, it doesn't specify the mechanism for storing recently accessed entries. \n"
            }
        ]
    },
    {
        "doc_id": 971,
        "doc": {
            "question_id": -1,
            "question": "The TLB is incorporated as part of the ____",
            "options": [
                "Processor",
                "MMU",
                "Disk",
                "RAM"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The TLB is incorporated as part of the ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe **Translation Lookaside Buffer (TLB)** is a critical component of the **Memory Management Unit (MMU)**. It acts as a cache for recently accessed page table entries, speeding up the translation from virtual to physical addresses. \n"
            }
        ]
    },
    {
        "doc_id": 972,
        "doc": {
            "question_id": -1,
            "question": "Whenever a request to the page that is not present in the main memory is accessed ____ is triggered.",
            "options": [
                "Interrupt",
                "Request",
                "Page fault",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Whenever a request to the page that is not present in the main memory is accessed ____ is triggered.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 973,
        "doc": {
            "question_id": -1,
            "question": "The general purpose registers are combined into a block called as ____",
            "options": [
                "Register bank",
                "Register Case",
                "Register file",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The general purpose registers are combined into a block called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 974,
        "doc": {
            "question_id": -1,
            "question": "What does the RUN signal do?",
            "options": [
                "It causes the termination of a signal",
                "It causes a particular signal to perform its operation",
                "It causes a particular signal to end",
                "It increments the step counter by one"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "What does the RUN signal do?",
                "Expected Label": "D",
                "Predicted Label": "**B) It causes a particular signal to perform its operation** \n\n**Explanation:**\n\nThe RUN signal, often found in computer control units, acts as an enabling command. When active, it allows a specific operation associated with a signal to be executed. It doesn't terminate or end the signal itself, but rather triggers its intended action. \n"
            }
        ]
    },
    {
        "doc_id": 975,
        "doc": {
            "question_id": -1,
            "question": "____ register is designated to point to the 68000 processor stack.",
            "options": [
                "A7 register",
                "B2 register",
                "There is no such designation",
                "Any general purpose register is selected at random"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ register is designated to point to the 68000 processor stack.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 976,
        "doc": {
            "question_id": -1,
            "question": "The word length in the 68000 computer is ____",
            "options": [
                "32 bit",
                "64 bit",
                "16 bit",
                "8 bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The word length in the 68000 computer is ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 977,
        "doc": {
            "question_id": -1,
            "question": "The register in 68000 can contain up to ____ bits.",
            "options": [
                "24",
                "32",
                "16",
                "64"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The register in 68000 can contain up to ____ bits.",
                "Expected Label": "B",
                "Predicted Label": "C) 16 \n"
            }
        ]
    },
    {
        "doc_id": 978,
        "doc": {
            "question_id": -1,
            "question": "The 68000 has a max of how many data registers?",
            "options": [
                "16",
                "20",
                "10",
                "8"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The 68000 has a max of how many data registers?",
                "Expected Label": "D",
                "Predicted Label": "D) 8 \n"
            }
        ]
    },
    {
        "doc_id": 979,
        "doc": {
            "question_id": -1,
            "question": "When an operand is stored in a register it is ____",
            "options": [
                "Stored in the lower order bits of the register",
                "Stored in the higher order bits of the register",
                "Stored in any of the bits at random",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "When an operand is stored in a register it is ____",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\nOperands are stored in registers without any specific bias towards higher or lower bits. The way data is stored and aligned within a register depends on factors like:\n\n* **Data type:** Integers, floating-point numbers, etc., have different storage requirements.\n* **Architecture:**  Architectures might have specific alignment rules.\n* **Instruction Set:** Instructions dictate how data is fetched and interpreted from registers.\n\n**Why other options are incorrect:**\n\n* **A & B:** There's no inherent preference for storing operands in higher or lower bits. \n* **C:**  While data corruption could lead to seemingly random storage, it's not how operands are *intended* to be stored. \n"
            }
        ]
    },
    {
        "doc_id": 980,
        "doc": {
            "question_id": -1,
            "question": "The status register of the 68000 has ____ condition codes.",
            "options": [
                "7",
                "4",
                "5",
                "8"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The status register of the 68000 has ____ condition codes.",
                "Expected Label": "C",
                "Predicted Label": "C) 5 \n"
            }
        ]
    },
    {
        "doc_id": 981,
        "doc": {
            "question_id": -1,
            "question": "The 68000 uses ____ address assignment.",
            "options": [
                "Big Endian",
                "Little Endian",
                "X-Little Endian",
                "X-Big Endian"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The 68000 uses ____ address assignment.",
                "Expected Label": "A",
                "Predicted Label": "A) Big Endian \n"
            }
        ]
    },
    {
        "doc_id": 982,
        "doc": {
            "question_id": -1,
            "question": "The addresses generated by the 68000 is ____ bit.",
            "options": [
                "32",
                "16",
                "24",
                "42"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The addresses generated by the 68000 is ____ bit.",
                "Expected Label": "C",
                "Predicted Label": "C) 24 \n"
            }
        ]
    },
    {
        "doc_id": 983,
        "doc": {
            "question_id": -1,
            "question": "Instructions which can handle any type of addressing mode are said to be ____",
            "options": [
                "Omniscient",
                "Orthogonal",
                "Versatile",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Instructions which can handle any type of addressing mode are said to be ____",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 984,
        "doc": {
            "question_id": -1,
            "question": "As the instructions can deal with variable size operands we use ____ to resolve this.",
            "options": [
                "Delimiter",
                "Size indicator mnemonic",
                "Special assemblers",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "As the instructions can deal with variable size operands we use ____ to resolve this.",
                "Expected Label": "B",
                "Predicted Label": "B \n\nHere's why:\n\n* **Size indicator mnemonics** are used within assembly language instructions to specify the size of the operand. For example, you might have instructions like `MOVW` (move word) or `MOVB` (move byte). \n\nLet's look at why the other options aren't the best fit:\n\n* **Delimiters** are used to separate data elements, not indicate their size.\n* **Special assemblers** aren't strictly necessary for handling variable operand sizes. Standard assemblers can handle this with the right syntax (size indicator mnemonics).\n* **None of the mentioned** is incorrect because size indicator mnemonics are a common solution. \n"
            }
        ]
    },
    {
        "doc_id": 985,
        "doc": {
            "question_id": -1,
            "question": "The starting address is denoted using ____ directive.",
            "options": [
                "EQU",
                "ORIGIN",
                "ORG",
                "PLACE"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The starting address is denoted using ____ directive.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 986,
        "doc": {
            "question_id": -1,
            "question": "The constant can be declared using ____ directive.",
            "options": [
                "DATAWORD",
                "PLACE",
                "CONS",
                "DC"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The constant can be declared using ____ directive.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 987,
        "doc": {
            "question_id": -1,
            "question": "To allocate a block of memory we use ____ directive.",
            "options": [
                "RESERVE",
                "DS",
                "DATAWORD",
                "PLACE"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "To allocate a block of memory we use ____ directive.",
                "Expected Label": "B",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 988,
        "doc": {
            "question_id": -1,
            "question": "The Branch instruction in 68000 provides how many types of offsets?",
            "options": [
                "3",
                "1",
                "0",
                "2"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The Branch instruction in 68000 provides how many types of offsets?",
                "Expected Label": "D",
                "Predicted Label": "D) 2 \n\n**Explanation:**\n\nThe Motorola 68000 (m68k) architecture uses two types of branch offsets:\n\n* **Byte Offset:** Used for shorter jumps, typically within the range of -128 to +127 bytes from the current instruction.\n* **Word Offset:**  Used for longer jumps, allowing a wider range of addresses to be reached.\n\nLet me know if you'd like more detail on how these offsets work within the 68000 instruction set! \n"
            }
        ]
    },
    {
        "doc_id": 989,
        "doc": {
            "question_id": -1,
            "question": "The purpose of using DBcc as a branch instruction is ____",
            "options": [
                "It provides two conditions to be satisfied for a branch to occur",
                "It provides a counter to check the number of times the branch as taken place",
                "It is used to check the condition along with the branch condition",
                "None of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The purpose of using DBcc as a branch instruction is ____",
                "Expected Label": "D",
                "Predicted Label": "**C) It is used to check the condition along with the branch condition**\n\n**Explanation:**\n\nDBcc (Decrement and Branch if Condition Code) instructions are commonly found in architectures like Motorola 68k.  Here's how they work:\n\n* **Decrement:** A specified counter register is decremented by 1.\n* **Branch Condition:**  The branch is taken if the condition code (specified by \"cc\") is true **AND** the counter register is **not equal to zero** after decrementing.\n\n**In essence, DBcc combines a loop counter with a conditional branch.** \n"
            }
        ]
    },
    {
        "doc_id": 990,
        "doc": {
            "question_id": -1,
            "question": "The 68000 uses ____ method to access I/O devices buffers.",
            "options": [
                "Memory mapped",
                "I/O mapped",
                "Buffer mapped",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The 68000 uses ____ method to access I/O devices buffers.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 991,
        "doc": {
            "question_id": -1,
            "question": "____ instruction is used to set up a frame pointer for the subroutines in 68000.",
            "options": [
                "CREATE",
                "LINK",
                "UNLK",
                "FRAME"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ instruction is used to set up a frame pointer for the subroutines in 68000.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 992,
        "doc": {
            "question_id": -1,
            "question": "The LINK instruction is always followed by ____ instruction.",
            "options": [
                "MOV",
                "UNLK",
                "ORG",
                "MOVEM"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The LINK instruction is always followed by ____ instruction.",
                "Expected Label": "D",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **LINK** instruction in assembly languages is used to create a stack frame for a subroutine. It typically stores the current stack pointer and allocates space for local variables on the stack.\n* **UNLK** instruction is the counterpart used to undo what LINK does. It deallocates the stack frame created by LINK, effectively restoring the stack pointer to its state before the subroutine call.\n\n**The other options are incorrect because:**\n\n* **MOV:** A general-purpose move instruction, not specifically tied to stack frame management.\n* **ORG:**  Used to set the origin address for subsequent code assembly, unrelated to stack operations. \n* **MOVEM:**  Moves multiple registers to or from memory, but not directly involved in stack frame setup/teardown. \n"
            }
        ]
    },
    {
        "doc_id": 993,
        "doc": {
            "question_id": -1,
            "question": "ARM stands for ____",
            "options": [
                "Advanced Rate Machines",
                "Advanced RISC Machines",
                "Artificial Running Machines",
                "Aviary Running Machines"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "ARM stands for ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 994,
        "doc": {
            "question_id": -1,
            "question": "The main importance of ARM micro-processors is providing operation with ____",
            "options": [
                "Low cost and low power consumption",
                "Higher degree of multi-tasking",
                "Lower error or glitches",
                "Efficient memory management"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The main importance of ARM micro-processors is providing operation with ____",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 995,
        "doc": {
            "question_id": -1,
            "question": "ARM processors where basically designed for ____",
            "options": [
                "Main frame systems",
                "Distributed systems",
                "Mobile systems",
                "Super computers"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "ARM processors where basically designed for ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 996,
        "doc": {
            "question_id": -1,
            "question": "The address system supported by ARM systems is/are ____",
            "options": [
                "Little Endian",
                "Big Endian",
                "X-Little Endian",
                "Both Little & Big Endian"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The address system supported by ARM systems is/are ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 997,
        "doc": {
            "question_id": -1,
            "question": "Memory can be accessed in ARM systems by ____ instructions. \ni) Store \nii) MOVE \niii) Load \niv) Arithmetic \nv) logical",
            "options": [
                "i, ii, iii",
                "i, ii",
                "i, iv, v",
                "iii, iv, v"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Memory can be accessed in ARM systems by ____ instructions. \ni) Store \nii) MOVE \niii) Load \niv) Arithmetic \nv) logical",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 998,
        "doc": {
            "question_id": -1,
            "question": "RISC stands for ____",
            "options": [
                "Restricted Instruction Sequencing Computer",
                "Restricted Instruction Sequential Compiler",
                "Reduced Instruction Set Computer",
                "Reduced Induction Set Computer"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "RISC stands for ____",
                "Expected Label": "C",
                "Predicted Label": "C"
            }
        ]
    },
    {
        "doc_id": 999,
        "doc": {
            "question_id": -1,
            "question": "In the ARM, PC is implemented using ____",
            "options": [
                "Caches",
                "Heaps",
                "General purpose register",
                "Stack"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the ARM, PC is implemented using ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe Program Counter (PC) in ARM architecture is implemented using a **general-purpose register** (specifically, register R15). \n\nLet's break down why the other options are incorrect:\n\n* **A) Caches:** Caches store frequently accessed data and instructions to speed up memory access, they don't hold the PC.\n* **B) Heaps:** Heaps are memory regions for dynamic memory allocation, not for storing the PC.\n* **D) Stack:** The stack is used for function calls, local variables, and return addresses. While it's related to program flow, it doesn't directly store the PC. \n"
            }
        ]
    },
    {
        "doc_id": 1000,
        "doc": {
            "question_id": -1,
            "question": "The additional duplicate register used in ARM machines are called as ____",
            "options": [
                "Copied-registers",
                "Banked registers",
                "EXtra registers",
                "Extential registers"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The additional duplicate register used in ARM machines are called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Banked registers** are the additional, duplicate registers used in ARM architectures, particularly in the context of handling different processor modes and exceptions. \n"
            }
        ]
    },
    {
        "doc_id": 1001,
        "doc": {
            "question_id": -1,
            "question": "The banked registers are used for ____",
            "options": [
                "Switching between supervisor and interrupt mode",
                "Extended storing",
                "Same as other general purpose registers",
                "None of the mentioned"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The banked registers are used for ____",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Switching between supervisor and interrupt mode**. Here's why:\n\n* **Banked registers** are a set of duplicate registers within a CPU.  They are primarily used to speed up context switching during interrupts and exceptions.\n\nLet's break down the other options:\n\n* **B) Extended storing:** While some systems might use banked registers for specific storage purposes, this isn't their primary function.\n* **C) Same as other general purpose registers:** Banked registers have a specific purpose different from general-purpose registers.\n* **D) None of the mentioned:**  Option A accurately describes the primary use of banked registers.\n\n**Therefore, A is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 1002,
        "doc": {
            "question_id": -1,
            "question": "Each instruction in ARM machines is encoded into ____ Word.",
            "options": [
                "2 byte",
                "3 byte",
                "4 byte",
                "8 byte"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Each instruction in ARM machines is encoded into ____ Word.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1003,
        "doc": {
            "question_id": -1,
            "question": "The addressing mode where the EA of the operand is the contents of Rn is ____",
            "options": [
                "Pre-indexed mode",
                "Pre-indexed with write back mode",
                "Post-indexed mode",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The addressing mode where the EA of the operand is the contents of Rn is ____",
                "Expected Label": "C",
                "Predicted Label": "**D) None of the mentioned**\n\nThe addressing mode described is **Register Indirect Addressing**. \n\nLet's break down why the other options are incorrect:\n\n* **Pre-indexed mode:** The EA is calculated by adding the contents of a register to an offset *before* fetching the operand.\n* **Pre-indexed with write back mode:**  Similar to pre-indexed, but the calculated EA is also written back to the index register.\n* **Post-indexed mode:** The EA is the contents of a register, but an offset is added *after* fetching the operand.\n\nNone of these options accurately describe the scenario where the EA is simply the contents of a register (Rn). \n"
            }
        ]
    },
    {
        "doc_id": 1004,
        "doc": {
            "question_id": -1,
            "question": "The effective address of the instruction written in Post-indexed mode, MOVE[Rn]+Rm is ____",
            "options": [
                "EA = [Rn]",
                "EA = [Rn  + Rm]",
                "EA = [Rn] + Rm",
                "EA = [Rm] + Rn"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The effective address of the instruction written in Post-indexed mode, MOVE[Rn]+Rm is ____",
                "Expected Label": "A",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1005,
        "doc": {
            "question_id": -1,
            "question": "____ symbol is used to signify write back mode.",
            "options": [
                "#",
                "^",
                "&",
                "!"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ symbol is used to signify write back mode.",
                "Expected Label": "D",
                "Predicted Label": "D) ! \n\nThe exclamation mark (!) is often used in computer architecture diagrams and documentation to signify the write-back caching policy. \n"
            }
        ]
    },
    {
        "doc_id": 1006,
        "doc": {
            "question_id": -1,
            "question": "The instructions which are used to load or store multiple operands are called as ____",
            "options": [
                "Banked instructions",
                "Lump transfer instructions",
                "Block transfer instructions",
                "DMA instructions"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instructions which are used to load or store multiple operands are called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1007,
        "doc": {
            "question_id": -1,
            "question": "The Instruction, LDM R10!, {R0,R1,R6,R7}  ____",
            "options": [
                "Loads the contents of R10 into R1, R0, R6 and R7",
                "Creates a copy of the contents of R10 in the other registers except for the above mentioned ones",
                "Loads the contents of the registers R1, R0, R6 and R7 to R10",
                "Writes the contents of R10 into the above mentioned registers and clears R10"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The Instruction, LDM R10!, {R0,R1,R6,R7}  ____",
                "Expected Label": "A",
                "Predicted Label": "**D) Writes the contents of R10 into the above mentioned registers and clears R10** \n\n**Explanation:**\n\nThe instruction you provided is a typical example of a \"Load Multiple and Decrement\" instruction in ARM assembly. Let's break down why D is correct:\n\n* **LDM (Load Multiple):** This indicates that we're loading values from memory *into* registers.\n* **R10!:**  R10 is the base register, and the \"!\" means it will be updated (decremented in this case) after the operation.\n* **{R0, R1, R6, R7}:**  This list specifies the destination registers where the loaded values will be placed.\n\n**In essence, the instruction does the following:**\n\n1. Writes the contents of the memory location pointed to by R10 into register R0.\n2. Decrements R10.\n3. Writes the contents of the memory location pointed to by R10 into register R1.\n4. Decrements R10.\n5. Repeats for R6 and R7.\n\n**Important Note:** This explanation assumes a specific addressing mode (likely pre-decrement). Different ARM architectures might have slight variations. \n"
            }
        ]
    },
    {
        "doc_id": 1008,
        "doc": {
            "question_id": -1,
            "question": "The instruction, MLA R0,R1,R2,R3  performs ____",
            "options": [
                "R0<-[R1]+[R2]+[R3]",
                "R3<-[R0]+[R1]+[R2]",
                "R0<-[R1]*[R2]+[R3]",
                "R3<-[R0]*[R1]+[R2]"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instruction, MLA R0,R1,R2,R3  performs ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1009,
        "doc": {
            "question_id": -1,
            "question": "The ability to shift or rotate in the same instruction along with other operation is performed with the help of ____",
            "options": [
                "Switching circuit",
                "Barrel switcher circuit",
                "Integrated Switching circuit",
                "Multiplexer circuit"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ability to shift or rotate in the same instruction along with other operation is performed with the help of ____",
                "Expected Label": "B",
                "Predicted Label": "B) Barrel switcher circuit \n\nLet me explain why:\n\n* **Barrel shifters** are specifically designed to perform fast, variable shifts and rotates within a single clock cycle. This makes them ideal for use within instructions where these operations need to occur simultaneously with other processing. \n\nHere's why the other options aren't the best fit:\n\n* **Switching circuits** are a broad term and don't specifically imply the combined functionality of shifting/rotating with other operations.\n* **Integrated Switching circuit**  is too general. While barrel shifters are integrated circuits, this term doesn't specify their specialized purpose.\n* **Multiplexer circuits** are used for selecting one input from multiple sources, not for performing shifts or rotates. \n"
            }
        ]
    },
    {
        "doc_id": 1010,
        "doc": {
            "question_id": -1,
            "question": "____ instruction is used to get the 1\u2019s complement of the operand.",
            "options": [
                "COMP",
                "BIC",
                "~CMP",
                "MVN"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ instruction is used to get the 1\u2019s complement of the operand.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1011,
        "doc": {
            "question_id": -1,
            "question": "The offset used in the conditional branching is ____ bit.",
            "options": [
                "24",
                "32",
                "16",
                "8"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The offset used in the conditional branching is ____ bit.",
                "Expected Label": "A",
                "Predicted Label": "C) 16 \n\n**Explanation:**\n\nWhile the specific size of the offset can vary slightly depending on the architecture, conditional branching instructions typically use a 16-bit offset. This allows for a reasonable range of branch targets relative to the current instruction address. \n"
            }
        ]
    },
    {
        "doc_id": 1012,
        "doc": {
            "question_id": -1,
            "question": "The BEQ instructions is used ____",
            "options": [
                "To check the equality condition between the operands and then branch",
                "To check if the Operand is greater than the condition value and then branch",
                "To check if the flag Z is set to 1 and then causes branch",
                "None of the mentioned"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The BEQ instructions is used ____",
                "Expected Label": "C",
                "Predicted Label": "C) To check if the flag Z is set to 1 and then causes branch \n\n**Explanation:**\n\n* **BEQ (Branch if Equal)** is a conditional branch instruction that checks the status of the Zero Flag (Z flag). \n* If the Z flag is set (meaning the previous comparison resulted in equality), the BEQ instruction redirects the program counter to a new instruction address, effectively branching to a different part of the code.\n\nLet me know if you'd like more details on conditional branching or other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1013,
        "doc": {
            "question_id": -1,
            "question": "The condition to check whether the branch should happen or not is given by ____",
            "options": [
                "The lower order 8 bits of the instruction",
                "The higher order 4 bits of the instruction",
                "The lower order 4 bits of the instruction",
                "The higher order 8 bits of the instruction"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The condition to check whether the branch should happen or not is given by ____",
                "Expected Label": "B",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1014,
        "doc": {
            "question_id": -1,
            "question": "Which of the two instructions sets the condition flag upon execution? \ni) ADDS R0,R1,R2 \nii) ADD R0,R1,R2",
            "options": [
                "i",
                "ii",
                "Both i and ii",
                "Insufficient data"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "Which of the two instructions sets the condition flag upon execution? \ni) ADDS R0,R1,R2 \nii) ADD R0,R1,R2",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 1015,
        "doc": {
            "question_id": -1,
            "question": "____ directive is used to indicate the beginning of the program instruction or data.",
            "options": [
                "EQU",
                "START",
                "AREA",
                "SPACE"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ directive is used to indicate the beginning of the program instruction or data.",
                "Expected Label": "C",
                "Predicted Label": "**C) AREA** \n\nThe `AREA` directive in assembly language is used to define a segment or section of code or data. It marks the beginning of a specific memory region with specific attributes. \n"
            }
        ]
    },
    {
        "doc_id": 1016,
        "doc": {
            "question_id": -1,
            "question": "____ directive specifies the start of the execution.",
            "options": [
                "START",
                "ENTRY",
                "MAIN",
                "ORIGIN"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ directive specifies the start of the execution.",
                "Expected Label": "B",
                "Predicted Label": "C) MAIN \n\n**Explanation:**\n\nWhile assembly languages can vary, the **MAIN** directive (or a similar label like `_start` in some cases) is commonly used to indicate the program's entry point.  Here's why the other options aren't as suitable:\n\n* **START** and **ORIGIN** are often used for memory allocation and specifying addresses, not the program's beginning.\n* **ENTRY** might be used in some assembly languages to define entry points for subroutines or procedures, but it's less common for the overall program start. \n"
            }
        ]
    },
    {
        "doc_id": 1017,
        "doc": {
            "question_id": -1,
            "question": "____ directives are used to initialize operands.",
            "options": [
                "INT",
                "DATAWORD",
                "RESERVE",
                "DCD"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ directives are used to initialize operands.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**DCD** (Define Constant Data) is an assembler directive specifically used to initialize operands (usually with constant values). \n\nLet me know if you'd like more explanation on assembler directives! \n"
            }
        ]
    },
    {
        "doc_id": 1018,
        "doc": {
            "question_id": -1,
            "question": "____ directive is used to name the register used for execution of an instruction.",
            "options": [
                "ASSIGN",
                "RN",
                "NAME",
                "DECLARE"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ directive is used to name the register used for execution of an instruction.",
                "Expected Label": "B",
                "Predicted Label": "**A) ASSIGN**\n\nThe `ASSIGN` directive is used in some assembly languages (like VHDL) to assign a specific register to be used for the execution of an instruction. \n\nLet me know if you'd like more examples or explanations about computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1019,
        "doc": {
            "question_id": -1,
            "question": "The pseudo instruction used to load an address into the register is ____",
            "options": [
                "LOAD",
                "ADR",
                "ASSIGN",
                "PSLOAD"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The pseudo instruction used to load an address into the register is ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1020,
        "doc": {
            "question_id": -1,
            "question": "The address space of the IA-32 is ____",
            "options": [
                "2^16",
                "2^32",
                "2^64",
                "2^8"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The address space of the IA-32 is ____",
                "Expected Label": "B",
                "Predicted Label": "B) 2^32 \n"
            }
        ]
    },
    {
        "doc_id": 1021,
        "doc": {
            "question_id": -1,
            "question": "The addressing method used in IA-32 is ____",
            "options": [
                "Little Endian",
                "Big Endian",
                "X-Little Endian",
                "Both Little and Big Endian"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The addressing method used in IA-32 is ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1022,
        "doc": {
            "question_id": -1,
            "question": "The Floating point registers of IA-32 can operate on operands up to ____",
            "options": [
                "128 bit",
                "256 bit",
                "80 bit",
                "64 bit"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The Floating point registers of IA-32 can operate on operands up to ____",
                "Expected Label": "D",
                "Predicted Label": "C) 80 bit \n"
            }
        ]
    },
    {
        "doc_id": 1023,
        "doc": {
            "question_id": -1,
            "question": "The size of the floating registers can be extended upto ____",
            "options": [
                "128 bit",
                "256 bit",
                "80 bit",
                "64 bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The size of the floating registers can be extended upto ____",
                "Expected Label": "C",
                "Predicted Label": "C) 80 bit \n\nWhile options A, B, and D are also valid floating-point sizes in some contexts, the most common and historically significant extension of floating-point registers is to 80 bits (often referred to as \"extended precision\" or \"long double\"). \n"
            }
        ]
    },
    {
        "doc_id": 1024,
        "doc": {
            "question_id": -1,
            "question": "The IA-32 architecture associates different parts of memory called ____ with different usages.",
            "options": [
                "Frames",
                "Pages",
                "Tables",
                "Segments"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The IA-32 architecture associates different parts of memory called ____ with different usages.",
                "Expected Label": "D",
                "Predicted Label": "D) Segments \n"
            }
        ]
    },
    {
        "doc_id": 1025,
        "doc": {
            "question_id": -1,
            "question": "IOPL stands for ____",
            "options": [
                "Input/Output Privilege level",
                "Input Output Process Link",
                "Internal Output Process Link",
                "Internal Offset Privilege Level"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "IP Design and Manufacturing"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "IOPL stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1026,
        "doc": {
            "question_id": -1,
            "question": "In IA-32 architecture along with the general flags, the other conditional flags provided are ____",
            "options": [
                "IOPL",
                "IF",
                "TF",
                "All of the mentioned"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In IA-32 architecture along with the general flags, the other conditional flags provided are ____",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1027,
        "doc": {
            "question_id": -1,
            "question": "The register used to serve as PC is called as ____",
            "options": [
                "Indirection register",
                "Instruction pointer",
                "R-32",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The register used to serve as PC is called as ____",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe **Instruction Pointer (IP)**, also sometimes called the Program Counter (PC), is the register that holds the memory address of the *next* instruction to be executed. \n\nLet's look at why the other options are incorrect:\n\n* **A) Indirection register:** This register is used in address calculations and often holds a memory address that points to the actual operand.\n* **C) R-32:** This is a generic term for a 32-bit register and not specific to the program counter's function. \n* **D) None of the mentioned:** This is incorrect as the Instruction Pointer is the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 1028,
        "doc": {
            "question_id": -1,
            "question": "The Bit extension of the register is denoted with the help of ____ symbol.",
            "options": [
                "$",
                "`",
                "E",
                "~"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Bit extension of the register is denoted with the help of ____ symbol.",
                "Expected Label": "C",
                "Predicted Label": "B) ` \n"
            }
        ]
    },
    {
        "doc_id": 1029,
        "doc": {
            "question_id": -1,
            "question": "The instruction, ADD R1, R2, R3 is decoded as ____",
            "options": [
                "R1<-[R1]+[R2]+[R3]",
                "R3<-[R1]+[R2]",
                "R3<-[R1]+[R2]+[R3]",
                "R1<-[R2]+[R3]"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The instruction, ADD R1, R2, R3 is decoded as ____",
                "Expected Label": "D",
                "Predicted Label": "Answer: **B) R3<-[R1]+[R2]** \n\n**Explanation:**\n\nIn most assembly language conventions, an instruction like `ADD R1, R2, R3` follows this pattern:\n\n* **ADD:** The operation to perform (addition in this case).\n* **R1, R2:**  The source operands. The values stored in registers R1 and R2 will be used for the addition.\n* **R3:** The destination operand. The result of the addition will be stored in register R3.\n\nThe notation `[Rx]` usually signifies the *contents* of the register Rx. \n"
            }
        ]
    },
    {
        "doc_id": 1030,
        "doc": {
            "question_id": -1,
            "question": "The instruction JG loop does ____",
            "options": [
                "jumps to the memory location loop if the result of the most recent arithmetic op is even",
                "jumps to the memory location loop if the result of the most recent arithmetic op is greater than 0",
                "jumps to the memory location loop if the test condition is satisfied with the value of loop",
                "none of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The instruction JG loop does ____",
                "Expected Label": "B",
                "Predicted Label": "**B) jumps to the memory location loop if the result of the most recent arithmetic op is greater than 0**\n\n**Explanation:**\n\n* **JG** stands for \"Jump if Greater\". This type of instruction is conditional, meaning it only executes the jump if a specific condition is met. \n* In most architectures, the JG instruction checks the processor's status flags, particularly the **Zero Flag (ZF)** and the **Sign Flag (SF)**, which reflect the outcome of the previous arithmetic operation.\n* The jump is taken if the result of the last arithmetic operation was greater than zero (not zero and not negative).\n\nLet me know if you'd like a deeper dive into how processor flags work with conditional jumps! \n"
            }
        ]
    },
    {
        "doc_id": 1031,
        "doc": {
            "question_id": -1,
            "question": "The LEA mnemonic is used to ____",
            "options": [
                "Load the effective address of an instruction",
                "Load the values of operands onto an accumulator",
                "Declare the values as global constants",
                "Store the outcome of the operation at a memory location"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The LEA mnemonic is used to ____",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **LEA (Load Effective Address)** is specifically designed to calculate and load the *address* of a memory operand, not the data stored at that address.\n\nLet's break down why the other options are incorrect:\n\n* **B) Load the values of operands onto an accumulator:** This describes a typical \"load\" instruction (like `MOV` in x86), not `LEA`.\n* **C) Declare the values as global constants:** This is a task for assemblers and linkers, handled through directives (like `.global` or `.equ`), not instructions like `LEA`.\n* **D) Store the outcome of the operation at a memory location:** This describes a \"store\" instruction (like `MOV` with a destination operand in memory), not `LEA`. \n"
            }
        ]
    },
    {
        "doc_id": 1032,
        "doc": {
            "question_id": -1,
            "question": "The instructions of IA-32 machines are of length up to ____",
            "options": [
                "4 bytes",
                "8 bytes",
                "16 bytes",
                "12 bytes"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The instructions of IA-32 machines are of length up to ____",
                "Expected Label": "D",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1033,
        "doc": {
            "question_id": -1,
            "question": "The bit present in the op code, indicating which of the operands is the source is called as ____",
            "options": [
                "SRC bit",
                "Indirection bit",
                "Direction bit",
                "FRM bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The bit present in the op code, indicating which of the operands is the source is called as ____",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Direction bit** explicitly indicates the direction of data flow within an instruction. If the bit is set, it might signify that the operand is the source. If clear, the operand might be the destination. \n\nLet's look at why the other options are incorrect:\n\n* **SRC bit:** While it implies \"source,\" it's not a standard term used specifically for identifying source operands in opcodes.\n* **Indirection bit:**  This typically signals that the operand itself isn't the data but a pointer/address to the actual data. \n* **FRM bit:** This acronym isn't commonly associated with computer architecture or operand identification. \n\n**Therefore, \"C) Direction bit\" is the most fitting answer.** \n"
            }
        ]
    },
    {
        "doc_id": 1034,
        "doc": {
            "question_id": -1,
            "question": "The ____ directive is used to allocate 4 bytes of memory.",
            "options": [
                "DD",
                "ALLOC",
                "RESERVE",
                "SPACE"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ directive is used to allocate 4 bytes of memory.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1035,
        "doc": {
            "question_id": -1,
            "question": ".data directive is used ____",
            "options": [
                "To indicate the ending of the data section",
                "To indicate the beginning of the data section",
                "To declare all the source operands",
                "To Initialize the operands"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": ".data directive is used ____",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1036,
        "doc": {
            "question_id": -1,
            "question": "The instruction used to cause unconditional jump is ____",
            "options": [
                "UJG",
                "JG",
                "JMP",
                "GOTO"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The instruction used to cause unconditional jump is ____",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1037,
        "doc": {
            "question_id": -1,
            "question": "____ instruction is used to check the bit of the condition flags.",
            "options": [
                "TEST",
                "TB",
                "CHECK",
                "BT"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ instruction is used to check the bit of the condition flags.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1038,
        "doc": {
            "question_id": -1,
            "question": "REPINS instruction is used to ____",
            "options": [
                "Transfer a block of data serially from an Input device to the processor",
                "Transfer a block of data parallelly from Input device to the processor",
                "Transfer a block of data serially from an Input device to the output device",
                "Transfer a block of data parallelly from Input device to the output device"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "REPINS instruction is used to ____",
                "Expected Label": "B",
                "Predicted Label": "**B)** Transfer a block of data parallelly from Input device to the processor \n\n**Explanation:**\n\nREPINS (Repeat String Input) is an x86 assembly instruction used for fast data transfer from I/O ports to memory. It utilizes the processor's string instructions to move data efficiently in parallel, making option B the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 1039,
        "doc": {
            "question_id": -1,
            "question": "Which of the following statements regarding Stacks is/are True? \ni) The stack always grows towards higher addresses \nii) The stack always grows towards lower addresses \niii) The stack has a fixed size \niv)The width of the stack is 32 bits ",
            "options": [
                "i and iii",
                "i and iv",
                "ii and iv",
                "iii and iv"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Which of the following statements regarding Stacks is/are True? \ni) The stack always grows towards higher addresses \nii) The stack always grows towards lower addresses \niii) The stack has a fixed size \niv)The width of the stack is 32 bits ",
                "Expected Label": "C",
                "Predicted Label": "**D) iii and iv**\n\n**Explanation:**\n\n* **iii) The stack has a fixed size:** This is generally true.  The stack is typically allocated a fixed amount of memory at program startup. \n* **iv) The width of the stack is 32 bits:**  The width of the stack depends on the architecture. On a 32-bit system, it's likely 32 bits. On a 64-bit system, it's likely 64 bits.  However, the question doesn't specify the system architecture, making this statement potentially true.\n\n**Why other options are incorrect:**\n\n* **i) The stack always grows towards higher addresses:** This is not always true. Stack growth direction varies depending on the architecture.\n* **ii) The stack always grows towards lower addresses:** This is also not always true and depends on the architecture. \n"
            }
        ]
    },
    {
        "doc_id": 1040,
        "doc": {
            "question_id": -1,
            "question": "The instruction used to multiply operands yielding a double integer outcome is ____",
            "options": [
                "MUL",
                "IMUL",
                "DMUL",
                "EMUL"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The instruction used to multiply operands yielding a double integer outcome is ____",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1041,
        "doc": {
            "question_id": -1,
            "question": "SIMD stands for ____",
            "options": [
                "Single Instruction Multiple Data",
                "Simple Instruction Multiple Decoding",
                "Sequential Instruction Multiple Decoding",
                "System Information Mutable Data"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "SIMD stands for ____",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1042,
        "doc": {
            "question_id": -1,
            "question": "The IA-32 system follows ____ design.",
            "options": [
                "RISC",
                "CISC",
                "SIMD",
                "None of the mentioned"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The IA-32 system follows ____ design.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **CISC (Complex Instruction Set Computing)** is characterized by having a large set of instructions, many of which can perform complex operations in a single instruction. IA-32 (also known as x86) is a prime example of CISC architecture.\n\nLet's look at why the other options are incorrect:\n\n* **RISC (Reduced Instruction Set Computing)** uses a smaller set of simpler instructions, making it easier to pipeline and optimize.\n* **SIMD (Single Instruction, Multiple Data)** is a type of parallel processing where a single instruction operates on multiple data elements simultaneously. While IA-32 supports SIMD extensions (like SSE and AVX), it's not the core architectural design. \n"
            }
        ]
    },
    {
        "doc_id": 1043,
        "doc": {
            "question_id": -1,
            "question": "Which architecture is suitable for a wide range of data types?",
            "options": [
                "ARM",
                "68000",
                "IA-32",
                "ASUS firebird"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Which architecture is suitable for a wide range of data types?",
                "Expected Label": "C",
                "Predicted Label": "C) IA-32 \n\nHere's why:\n\n* **IA-32 (x86):**  Known for its backward compatibility and support for a wide range of data types, including various integer sizes, floating-point formats, and specialized data structures.\n\nLet's break down why the other options are less suitable:\n\n* **ARM:** While highly efficient, ARM architectures typically have a stronger focus on reduced instruction set computing (RISC), which can sometimes mean a slightly less diverse set of directly supported data types.\n* **68000:** An older architecture with a more limited set of data types compared to modern options.\n* **ASUS firebird:** This refers to a brand or series of computer systems, not a specific architecture that defines data type support. \n\n**Therefore, IA-32 stands out as the architecture designed to handle a wider array of data types directly.** \n"
            }
        ]
    },
    {
        "doc_id": 1044,
        "doc": {
            "question_id": -1,
            "question": "In case of multimedia extension instructions, the pixels are encoded into a data item of ____",
            "options": [
                "16 bit",
                "32 bit",
                "24 bit",
                "8 bit"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In case of multimedia extension instructions, the pixels are encoded into a data item of ____",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1045,
        "doc": {
            "question_id": -1,
            "question": "The MMX (Multimedia Extension) operands are stored in ____",
            "options": [
                "General purpose registers",
                "Banked registers",
                "Float point registers",
                "Graphic registers"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The MMX (Multimedia Extension) operands are stored in ____",
                "Expected Label": "C",
                "Predicted Label": "C) Float point registers \n"
            }
        ]
    },
    {
        "doc_id": 1046,
        "doc": {
            "question_id": -1,
            "question": "The division operation in IA-32 is a single operand instruction so it is assumed that ____",
            "options": [
                "The divisor is stored in the EAX register",
                "The dividend is stored in the EAC register",
                "The divisor is stored in the accumulator",
                "The dividend is stored in the accumulator"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The division operation in IA-32 is a single operand instruction so it is assumed that ____",
                "Expected Label": "A",
                "Predicted Label": "D. \n"
            }
        ]
    },
    {
        "doc_id": 1047,
        "doc": {
            "question_id": 123121057,
            "question": "____ is the technology that allowed microprocessors to achieve higher performance improvement rates compared to earlier computers.",
            "options": [
                "Integrated circuit",
                "Assembly language",
                "Virtual memory",
                "Superscalar processing"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the technology that allowed microprocessors to achieve higher performance improvement rates compared to earlier computers.",
                "Expected Label": "A",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1048,
        "doc": {
            "question_id": 123121058,
            "question": "____ is the primary factor that has led to the dominance of microprocessor-based computers across the entire range of computer design.",
            "options": [
                "Minicomputer obsolescence",
                "Continuing improvement of semiconductor manufacturing",
                "Rise of mobile computing",
                "Increased internet connectivity"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is the primary factor that has led to the dominance of microprocessor-based computers across the entire range of computer design.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nWhile the other options have played a role in shaping the computing landscape, the driving force behind the dominance of microprocessors is the **continuous improvement in semiconductor manufacturing (Moore's Law)**.  This has allowed for exponentially more transistors to be packed onto a single chip, leading to dramatic increases in processing power, memory capacity, and energy efficiency at decreasing costs. \n"
            }
        ]
    },
    {
        "doc_id": 1049,
        "doc": {
            "question_id": 123121059,
            "question": "_____ parallelism requires explicit programming attention, unlike instruction-level parallelism which is handled implicitly by compilers and hardware.",
            "options": [
                "Instruction-level",
                "Loop-level",
                "Thread-level",
                "Pipeline-level"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ parallelism requires explicit programming attention, unlike instruction-level parallelism which is handled implicitly by compilers and hardware.",
                "Expected Label": "C",
                "Predicted Label": "C) Thread-level \n"
            }
        ]
    },
    {
        "doc_id": 1050,
        "doc": {
            "question_id": 123121060,
            "question": "____ is a prime concern for personal mobile devices given the consumer price for the whole product is a few hundred dollars.",
            "options": [
                "Cost",
                "Performance",
                "Size",
                "Durability"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a prime concern for personal mobile devices given the consumer price for the whole product is a few hundred dollars.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile all the options are factors for personal mobile devices, **cost (A)** is the most critical concern, especially at lower price points.  Here's why:\n\n* **Cost Sensitivity:** Consumers are highly price-sensitive in this market segment. A small difference in price can significantly impact purchasing decisions.\n* **Profit Margins:** Manufacturers operate on tight profit margins, making cost control essential.\n* **Component Trade-offs:** To hit a specific price point, manufacturers often need to make trade-offs, prioritizing cost-effective components over top-tier performance or premium materials.\n\nWhile performance, size, and durability are important, cost is the primary driver in this price-sensitive market. \n"
            }
        ]
    },
    {
        "doc_id": 1051,
        "doc": {
            "question_id": 123121061,
            "question": "____ is a key feature of server systems that allows them to grow in response to increasing demand or functional requirements.",
            "options": [
                "Availability",
                "Scalability",
                "Responsiveness",
                "Throughput"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a key feature of server systems that allows them to grow in response to increasing demand or functional requirements.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1052,
        "doc": {
            "question_id": 123121062,
            "question": "____ are typically used as the building blocks for warehouse-scale computers to achieve fault tolerance at a large scale.",
            "options": [
                "Custom-designed processors",
                "High-end enterprise servers",
                "Redundant inexpensive components",
                "Specialized supercomputer nodes"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ are typically used as the building blocks for warehouse-scale computers to achieve fault tolerance at a large scale.",
                "Expected Label": "C",
                "Predicted Label": "C) Redundant inexpensive components \n\n**Explanation:**\n\nWarehouse-scale computers prioritize low cost and high fault tolerance. Using numerous redundant, inexpensive components allows for:\n\n* **Cost-effectiveness:**  Individually cheaper components are more economical at scale.\n* **Fault tolerance:** If one component fails, redundant ones maintain system operation.\n\nLet me know if you'd like more detail on any of the options! \n"
            }
        ]
    },
    {
        "doc_id": 1053,
        "doc": {
            "question_id": 123121063,
            "question": "____ exploits parallelism among largely decoupled tasks specified by the programmer or the operating system.",
            "options": [
                "Instruction-Level Parallelism",
                "Vector Architectures",
                "Thread-Level Parallelism",
                "Request-Level Parallelism"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ exploits parallelism among largely decoupled tasks specified by the programmer or the operating system.",
                "Expected Label": "D",
                "Predicted Label": "D) Request-Level Parallelism \n\n**Explanation:**\n\n* **Request-Level Parallelism (RLP)** is about executing multiple, relatively independent tasks (like requests to a web server) concurrently. These tasks are often large and can be divided and assigned to different processing units. \n\nThe other options are not as fitting:\n\n* **Instruction-Level Parallelism (ILP)** focuses on extracting parallelism within a single instruction stream.\n* **Vector Architectures** use specialized instructions to operate on large arrays of data in parallel.\n* **Thread-Level Parallelism (TLP)** exploits parallelism among multiple threads of execution within a single process. While related to parallelism, it's more about closer cooperation between threads than the decoupled tasks in RLP. \n"
            }
        ]
    },
    {
        "doc_id": 1054,
        "doc": {
            "question_id": 123121064,
            "question": "____ is a type of parallel architecture where each processor fetches its own instructions and operates on its own data.",
            "options": [
                "MIMD",
                "SISD",
                "SIMD",
                "MISD"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a type of parallel architecture where each processor fetches its own instructions and operates on its own data.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1055,
        "doc": {
            "question_id": 123121065,
            "question": "The ____ serves as the boundary between the software and hardware.",
            "options": [
                "CPU",
                "Instruction Set Architecture",
                "Operating System",
                "Compiler"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ serves as the boundary between the software and hardware.",
                "Expected Label": "B",
                "Predicted Label": "B) Instruction Set Architecture \n\n**Explanation:**\n\nThe Instruction Set Architecture (ISA) defines the interface between the software (programs) and the hardware (CPU). It specifies the set of instructions that the CPU can execute, the data types it supports, memory addressing modes, and other essential details.  \n\nHere's why the other options are not the best fit:\n\n* **A) CPU:** The CPU executes instructions, but the ISA defines *what* instructions it can execute.\n* **C) Operating System:**  The OS manages system resources and provides an interface for user programs, but it sits on top of the ISA.\n* **D) Compiler:** The compiler translates high-level code into machine code (instructions) according to the rules of the ISA. \n"
            }
        ]
    },
    {
        "doc_id": 1056,
        "doc": {
            "question_id": 123121066,
            "question": "____ is typically used to store the return address in the MIPS architecture.",
            "options": [
                "$sp",
                "$fp",
                "$ra",
                "$gp"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is typically used to store the return address in the MIPS architecture.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **$ra (Register Address):** The $ra register is specifically designed to hold the return address during function calls in MIPS. When a function is called using the \"jal\" (jump and link) instruction, the address of the next instruction after the \"jal\" is automatically stored in $ra. This allows the called function to return to the correct location in the program after it completes execution. \n\nLet me know if you'd like more details on any of the other registers! \n"
            }
        ]
    },
    {
        "doc_id": 1057,
        "doc": {
            "question_id": 123121067,
            "question": "In MIPS64, ____ instructions are used to transfer data between integer and floating-point registers.",
            "options": [
                "MOV.S and MOV.D",
                "LW and SW",
                "L.S and S.S",
                "MFC1 and MTC1"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In MIPS64, ____ instructions are used to transfer data between integer and floating-point registers.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1058,
        "doc": {
            "question_id": 123121068,
            "question": "____ refers to the specifics of a computer, including the detailed logic design and the packaging technology of the computer.",
            "options": [
                "Architecture",
                "Hardware",
                "Organization",
                "Instruction set"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ refers to the specifics of a computer, including the detailed logic design and the packaging technology of the computer.",
                "Expected Label": "B",
                "Predicted Label": "C) Organization \n"
            }
        ]
    },
    {
        "doc_id": 1059,
        "doc": {
            "question_id": 123121069,
            "question": "____ is a key consideration for personal mobile devices when designing their computer architecture.",
            "options": [
                "Database support",
                "Throughput performance",
                "Energy efficiency",
                "Error correction"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a key consideration for personal mobile devices when designing their computer architecture.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1060,
        "doc": {
            "question_id": 123121070,
            "question": "____ is typically 15 to 25 times cheaper per bit than Flash memory.",
            "options": [
                "Magnetic disk technology",
                "DRAM",
                "Semiconductor Flash",
                "Network technology"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically 15 to 25 times cheaper per bit than Flash memory.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nMagnetic disk technology (hard disk drives) is significantly cheaper per bit compared to Flash memory. While slower, its low cost makes it ideal for large-capacity storage. \n"
            }
        ]
    },
    {
        "doc_id": 1061,
        "doc": {
            "question_id": 123121071,
            "question": "In integrated circuit processes, the ____ is the minimum size of a transistor or a wire in either the x or y dimension.",
            "options": [
                "gate length",
                "feature size",
                "die area",
                "channel width"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In integrated circuit processes, the ____ is the minimum size of a transistor or a wire in either the x or y dimension.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1062,
        "doc": {
            "question_id": 123121072,
            "question": "In modern microprocessor design, ____ has become the biggest challenge facing computer designers for nearly every class of computer.",
            "options": [
                "Wire delay",
                "Transistor switching speed",
                "Power consumption",
                "Clock frequency"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In modern microprocessor design, ____ has become the biggest challenge facing computer designers for nearly every class of computer.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1063,
        "doc": {
            "question_id": 123121073,
            "question": "____ is the primary form of energy consumption in CMOS chips.",
            "options": [
                "Dynamic energy",
                "Static energy",
                "Thermal energy",
                "Kinetic energy"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the primary form of energy consumption in CMOS chips.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1064,
        "doc": {
            "question_id": 123121074,
            "question": "The primary factor limiting further increases in clock frequency for modern microprocessors is ____.",
            "options": [
                "transistor size",
                "heat dissipation",
                "power supply",
                "memory bandwidth"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The primary factor limiting further increases in clock frequency for modern microprocessors is ____.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1065,
        "doc": {
            "question_id": 123121075,
            "question": "In modern microprocessors, ____ is now considered the major constraint to using transistors.",
            "options": [
                "Silicon area",
                "Clock speed",
                "Power",
                "Transistor size"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In modern microprocessors, ____ is now considered the major constraint to using transistors.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1066,
        "doc": {
            "question_id": 123121076,
            "question": "The ____ strategy involves using a faster, less energy-efficient processor to allow the rest of the system to enter a sleep mode.",
            "options": [
                "power gating",
                "Turbo mode",
                "emergency slowdown",
                "race-to-halt"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ strategy involves using a faster, less energy-efficient processor to allow the rest of the system to enter a sleep mode.",
                "Expected Label": "D",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1067,
        "doc": {
            "question_id": 123121077,
            "question": "____ is a key factor in determining cost, as it decreases the time needed to get down the learning curve and increases purchasing and manufacturing efficiency.",
            "options": [
                "Volume",
                "Yield",
                "Price",
                "Commoditization"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor in determining cost, as it decreases the time needed to get down the learning curve and increases purchasing and manufacturing efficiency.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Volume**. Here's why:\n\n* **Volume:** Higher production volumes lead to economies of scale. This means the cost per unit decreases as more units are produced. This is due to factors like:\n    * **Spread out fixed costs:**  Design, development, and tooling costs are distributed over a larger number of units.\n    * **Negotiating power:** Bulk purchases of components grant manufacturers better pricing.\n    * **Learning curve:**  Production processes become more efficient as workers gain experience and identify optimizations. \n\nLet's examine why the other options are less accurate:\n\n* **Yield:** While yield (the percentage of usable manufactured units) impacts cost, it's not the primary driver of cost reduction related to the learning curve and efficiency.\n* **Price:** Price is a consequence of cost and other market factors. It's not a direct determinant of the factors mentioned in the question.\n* **Commoditization:** This refers to products becoming standardized and interchangeable, leading to price competition. While it can indirectly influence cost over time, it's not the core factor described in the question. \n"
            }
        ]
    },
    {
        "doc_id": 1068,
        "doc": {
            "question_id": 123121078,
            "question": "The ____ of producing a functional die is the most significant cost factor for low-volume integrated circuit production.",
            "options": [
                "Testing",
                "Packaging",
                "Wafer processing",
                "Mask set"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ of producing a functional die is the most significant cost factor for low-volume integrated circuit production.",
                "Expected Label": "D",
                "Predicted Label": "D) Mask set \n\n**Explanation:**\n\nThe creation of a mask set, used for etching the intricate circuitry onto the silicon wafers, represents a substantial upfront cost in IC fabrication. This cost remains relatively fixed regardless of the production volume. For low-volume production, the high cost of the mask set is distributed over fewer units, making it the most significant cost contributor. \n\nLet me know if you'd like more details on any of the other options! \n"
            }
        ]
    },
    {
        "doc_id": 1069,
        "doc": {
            "question_id": 123121079,
            "question": "The Mean Time To Failure (MTTF) of a system is calculated by ____ the failure rate.",
            "options": [
                "multiplying",
                "adding",
                "inverting",
                "squaring"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The Mean Time To Failure (MTTF) of a system is calculated by ____ the failure rate.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) inverting**.\n\n**Explanation:**\n\nMTTF (Mean Time To Failure) represents the average time until a component or system is expected to fail. Failure rate, on the other hand, indicates how often failures occur over a specific time period. These two concepts are inversely related:\n\n* **Higher failure rate:**  Leads to a lower MTTF (components fail more often, so they fail sooner on average).\n* **Lower failure rate:** Leads to a higher MTTF (components fail less often, so they last longer on average). \n"
            }
        ]
    },
    {
        "doc_id": 1070,
        "doc": {
            "question_id": 123121080,
            "question": "____ is the total amount of work done in a given time.",
            "options": [
                "Response time",
                "Execution time",
                "CPU time",
                "Throughput"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is the total amount of work done in a given time.",
                "Expected Label": "D",
                "Predicted Label": "D) Throughput \n\n**Explanation:**\n\n* **Throughput** measures how much work is done per unit of time.  It's a common measure of system efficiency.\n\nLet's look at why the other options are incorrect:\n\n* **Response Time:**  This is the time between a request being made and a response being received. It's about how quickly a task completes from the user's perspective.\n* **Execution Time:** This is the time a specific process takes to complete its execution, not the total work done in a given time.\n* **CPU Time:** This measures how long the CPU is actively working on a task or set of tasks. It doesn't reflect the overall amount of work completed. \n"
            }
        ]
    },
    {
        "doc_id": 1071,
        "doc": {
            "question_id": 123121081,
            "question": "____ are small, key pieces of real applications used as benchmarks.",
            "options": [
                "Kernels",
                "Toy programs",
                "Synthetic benchmarks",
                "Real applications"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are small, key pieces of real applications used as benchmarks.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Kernels** are small, specific parts of real applications that are extracted and used as benchmarks to assess the performance of computer systems. \n"
            }
        ]
    },
    {
        "doc_id": 1072,
        "doc": {
            "question_id": 123121082,
            "question": "____ is a widely used benchmark suite for evaluating processor performance across various application domains.",
            "options": [
                "EEMBC",
                "SPEC",
                "Dhrystone",
                "Windows"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a widely used benchmark suite for evaluating processor performance across various application domains.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1073,
        "doc": {
            "question_id": 123121083,
            "question": "In the SPEC2006 benchmark suite, ____ is the most commonly used programming language for integer programs.",
            "options": [
                "C++",
                "Fortran",
                "C",
                "Java"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the SPEC2006 benchmark suite, ____ is the most commonly used programming language for integer programs.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1074,
        "doc": {
            "question_id": 123121084,
            "question": "The SPECvirt_Sc2010 benchmark evaluates end-to-end performance of ____ servers.",
            "options": [
                "web",
                "database",
                "file",
                "virtualized datacenter"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The SPECvirt_Sc2010 benchmark evaluates end-to-end performance of ____ servers.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1075,
        "doc": {
            "question_id": 123121085,
            "question": "____ is a key principle in reporting performance measurements to ensure that results can be replicated by other experimenters.",
            "options": [
                "Reproducibility",
                "Optimization",
                "Standardization",
                "Verification"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key principle in reporting performance measurements to ensure that results can be replicated by other experimenters.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1076,
        "doc": {
            "question_id": 123121086,
            "question": "The ____ mean is used to compute the average of SPECRatios in benchmark suites.",
            "options": [
                "Arithmetic",
                "Geometric",
                "Harmonic",
                "Weighted"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ mean is used to compute the average of SPECRatios in benchmark suites.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1077,
        "doc": {
            "question_id": 123121087,
            "question": "The ____ of performance ratios ensures that the choice of reference computer is irrelevant when comparing computer performance.",
            "options": [
                "arithmetic mean",
                "harmonic mean",
                "geometric mean",
                "median"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ of performance ratios ensures that the choice of reference computer is irrelevant when comparing computer performance.",
                "Expected Label": "C",
                "Predicted Label": "C) geometric mean \n\nHere's why:\n\n* **Geometric Mean and Performance Ratios:** The geometric mean is specifically designed to normalize ratios, making it ideal for comparing performance across different systems. When you use the geometric mean of performance ratios (relative to a reference machine), the choice of the reference machine mathematically cancels out. This leads to a consistent performance comparison regardless of the baseline system.\n\nLet me know if you'd like a more detailed explanation of how this works mathematically! \n"
            }
        ]
    },
    {
        "doc_id": 1078,
        "doc": {
            "question_id": 123121088,
            "question": "____ is a valuable asset for servers that allows expanding memory and the number of processors and disks.",
            "options": [
                "Parallelism",
                "Virtualization",
                "Caching",
                "Scalability"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a valuable asset for servers that allows expanding memory and the number of processors and disks.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1079,
        "doc": {
            "question_id": 123121089,
            "question": "____ is a key principle of computer design that suggests favoring frequent scenarios over infrequent ones when making design trade-offs.",
            "options": [
                "Focus on the common case",
                "Principle of locality",
                "Instruction-level parallelism",
                "Data-level parallelism"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key principle of computer design that suggests favoring frequent scenarios over infrequent ones when making design trade-offs.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1080,
        "doc": {
            "question_id": 123121090,
            "question": "In Amdahl's law, the ____ is always less than or equal to 1.",
            "options": [
                "Speedupenhanced",
                "Fractionenhanced",
                "Execution timenew",
                "Execution timeold"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In Amdahl's law, the ____ is always less than or equal to 1.",
                "Expected Label": "B",
                "Predicted Label": "B) Fractionenhanced \n\n**Explanation:**\n\nAmdahl's Law focuses on the potential speedup achievable from optimizing a portion of a system. The \"Fractionenhanced\" represents the portion of the original execution time that the improvement affects. Since this portion cannot be larger than the whole execution time, it must be less than or equal to 1. \n"
            }
        ]
    },
    {
        "doc_id": 1081,
        "doc": {
            "question_id": 123121091,
            "question": "The goal in applying Amdahl's law is to spend resources _____ to where time is spent.",
            "options": [
                "inversely",
                "exponentially",
                "proportional",
                "logarithmically"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The goal in applying Amdahl's law is to spend resources _____ to where time is spent.",
                "Expected Label": "C",
                "Predicted Label": "C) proportional \n\n**Explanation:**\n\nAmdahl's Law focuses on optimizing the parts of a system that have the biggest impact on overall performance. It states that the speedup you gain from optimizing a component is limited by how much that component is actually used. \n\nTherefore, you want to allocate resources **proportionally** to where time is spent. If a component takes up 80% of the execution time, you should focus your optimization efforts there rather than a component that only takes 5%. \n"
            }
        ]
    },
    {
        "doc_id": 1082,
        "doc": {
            "question_id": 123121092,
            "question": "The ____ is a fundamental component in computer design that runs at a constant rate and determines the timing of discrete events in the system.",
            "options": [
                "CPU",
                "RAM",
                "Hard drive",
                "Clock"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is a fundamental component in computer design that runs at a constant rate and determines the timing of discrete events in the system.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1083,
        "doc": {
            "question_id": 123121093,
            "question": "____ is the product of instruction count and cycles per instruction.",
            "options": [
                "Clock cycles",
                "CPU time",
                "Clock rate",
                "Instructions per clock"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the product of instruction count and cycles per instruction.",
                "Expected Label": "A",
                "Predicted Label": "B) CPU time \n\n**Explanation:**\n\n* **CPU Time** is the total time the CPU spends executing a program. It's directly proportional to the number of instructions and the time taken to execute each instruction (cycles per instruction).\n\nLet's analyze the other options:\n\n* **Clock Cycles:** The number of clock pulses during which the CPU executes instructions. While related to CPU time, it's not the product of instruction count and CPI.\n* **Clock Rate:** The frequency at which the clock cycles, measured in GHz (gigahertz).  It determines how fast instructions are executed.\n* **Instructions Per Clock (IPC):** This is the average number of instructions executed per clock cycle, often used to measure CPU performance. \n\n**Therefore, the correct answer is (B).** \n"
            }
        ]
    },
    {
        "doc_id": 1084,
        "doc": {
            "question_id": 123121094,
            "question": "____ is typically used to measure both instructions executed and clock cycles in modern processors.",
            "options": [
                "Software profilers",
                "Hardware counters",
                "Instruction trace buffers",
                "Cache monitors"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically used to measure both instructions executed and clock cycles in modern processors.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1085,
        "doc": {
            "question_id": 123121095,
            "question": "SPECpower uses a software stack written in ____ to measure server performance.",
            "options": [
                "C++",
                "Python",
                "Java",
                "Ruby"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "SPECpower uses a software stack written in ____ to measure server performance.",
                "Expected Label": "C",
                "Predicted Label": "C) Java \n"
            }
        ]
    },
    {
        "doc_id": 1086,
        "doc": {
            "question_id": 123121097,
            "question": "In fault-tolerant systems, making sure that every component is ____ helps avoid single points of failure.",
            "options": [
                "optimized",
                "redundant",
                "miniaturized",
                "overclocked"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In fault-tolerant systems, making sure that every component is ____ helps avoid single points of failure.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1087,
        "doc": {
            "question_id": 123121098,
            "question": "In computer architecture, ____ is typically used to protect against transient faults in memory systems.",
            "options": [
                "Parity",
                "Checksums",
                "Redundancy",
                "ECC"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ is typically used to protect against transient faults in memory systems.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**ECC (Error Correcting Code)** is specifically designed to detect and correct single-bit errors, which are common transient faults in memory.\n\nHere's why the other options are less suitable:\n\n* **Parity (A):** Detects single-bit errors but cannot correct them.\n* **Checksums (B):** More general-purpose error detection, not as effective for single-bit errors.\n* **Redundancy (C):** A broader term encompassing various techniques, ECC being one of them. While redundancy is the principle behind protection, ECC is the specific implementation. \n"
            }
        ]
    },
    {
        "doc_id": 1088,
        "doc": {
            "question_id": 123121099,
            "question": "____ is considered the simplest and most common form of instruction-level parallelism.",
            "options": [
                "Pipelining",
                "Multithreading",
                "Superscalar execution",
                "Out-of-order execution"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered the simplest and most common form of instruction-level parallelism.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1089,
        "doc": {
            "question_id": 123121100,
            "question": "In computer storage systems, ____ is used to achieve both high performance and high availability by using redundant disks.",
            "options": [
                "OLTP",
                "RAID",
                "VLIW",
                "EPIC"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In computer storage systems, ____ is used to achieve both high performance and high availability by using redundant disks.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **RAID (Redundant Array of Independent Disks)** is designed specifically for high performance and high availability through data redundancy across multiple disks. \n\nLet's look at why the other options are incorrect:\n\n* **OLTP (Online Transaction Processing)** is a type of workload, not a storage system.\n* **VLIW (Very Long Instruction Word)** is an instruction set architecture, not related to storage.\n* **EPIC (Explicitly Parallel Instruction Computing)** is also an instruction set architecture, not related to storage. \n"
            }
        ]
    },
    {
        "doc_id": 1090,
        "doc": {
            "question_id": 123121101,
            "question": "_____ is a key factor in determining the price of a computer chip.",
            "options": [
                "Clock speed",
                "Cache size",
                "Fabrication yield",
                "Instruction set"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ is a key factor in determining the price of a computer chip.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Fabrication yield**. Here's why:\n\n* **Fabrication yield:** This refers to the percentage of usable chips produced from a silicon wafer.  A lower yield means fewer working chips, increasing the cost per chip to cover manufacturing expenses. \n* **Clock speed, Cache size, and Instruction set** are important factors in performance but don't directly dictate the manufacturing cost as much as yield does.\n\nLet me know if you'd like more explanation on any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1091,
        "doc": {
            "question_id": 123121104,
            "question": "The primary purpose of ____ is to provide protection and isolation between different processes or virtual machines running on the same physical hardware.",
            "options": [
                "Cache coherence",
                "Branch prediction",
                "Prefetching",
                "Virtual memory"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary purpose of ____ is to provide protection and isolation between different processes or virtual machines running on the same physical hardware.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1092,
        "doc": {
            "question_id": 123121107,
            "question": "____ is a measure that factors in both the frequency and cost of cache misses.",
            "options": [
                "Average memory access time",
                "Miss rate",
                "Hit time",
                "Execution time"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a measure that factors in both the frequency and cost of cache misses.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Average memory access time (AMAT)** is directly influenced by both the frequency of cache misses (miss rate) and the time it takes to handle those misses (miss penalty).  A higher miss rate or a longer penalty will increase the AMAT.\n\nLet's analyze the other options:\n\n* **Miss rate:** Only represents the frequency of cache misses, not the cost associated with them.\n* **Hit time:** Represents the time taken to access data found in the cache, not the cost of misses.\n* **Execution time:** While influenced by memory access time, it's a broader metric encompassing all aspects of program execution. \n\n**Therefore, the most accurate answer is (A).** \n"
            }
        ]
    },
    {
        "doc_id": 1093,
        "doc": {
            "question_id": 123121110,
            "question": "____ is a program used to estimate the access time and energy consumption of alternative cache structures on CMOS microprocessors.",
            "options": [
                "CACTI",
                "SPICE",
                "Verilog",
                "HSPICE"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a program used to estimate the access time and energy consumption of alternative cache structures on CMOS microprocessors.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1094,
        "doc": {
            "question_id": 123121111,
            "question": "In modern processors, the impact of ____ on cache design has led to increased use of higher associativity in first-level caches.",
            "options": [
                "branch prediction",
                "multithreading",
                "speculation",
                "out-of-order execution"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In modern processors, the impact of ____ on cache design has led to increased use of higher associativity in first-level caches.",
                "Expected Label": "B",
                "Predicted Label": "D) out-of-order execution \n\n**Explanation:**\n\nOut-of-order execution allows the processor to fetch and execute instructions that are not necessarily sequential, as long as their dependencies are met. This can lead to a higher rate of cache misses if the cache has low associativity, because instructions from different parts of the program might compete for the same cache lines. Higher associativity helps alleviate this by allowing more flexibility in where cache lines can be stored, reducing conflict misses and improving performance.\n\n**Why other options are incorrect:**\n\n* **A) Branch prediction:** While branch prediction is important for performance, it doesn't directly necessitate higher associativity in L1 caches. \n* **B) Multithreading:** Multithreading can increase pressure on the cache, but it doesn't specifically favor higher associativity in L1 caches over other levels.\n* **C) Speculation:** Similar to out-of-order execution, speculation can lead to a more unpredictable memory access pattern. However, out-of-order execution is the more direct driver for higher associativity in L1 caches. \n"
            }
        ]
    },
    {
        "doc_id": 1095,
        "doc": {
            "question_id": 123121113,
            "question": "Way selection in caches primarily aims to reduce ____ consumption.",
            "options": [
                "time",
                "area",
                "heat",
                "power"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Way selection in caches primarily aims to reduce ____ consumption.",
                "Expected Label": "D",
                "Predicted Label": "D) power \n\n**Explanation:**\n\nWhile way selection impacts other factors, the primary goal in modern architectures is to reduce **power consumption**. Here's why:\n\n* **Accessing memory is power-intensive:**  Each time the cache needs to check if data is present, it consumes power. Way selection helps narrow down the search within the cache.\n* **Smaller search space, less power:** By selecting the correct \"way\" (a set of cache lines), fewer comparisons are needed, leading to less switching activity in the cache circuitry and ultimately lower power usage. \n\nLet me know if you'd like more details on how caches work! \n"
            }
        ]
    },
    {
        "doc_id": 1096,
        "doc": {
            "question_id": 123121116,
            "question": "____ is a technique that requests the missed word first from memory and sends it to the processor as soon as it arrives, while filling the rest of the words in the block.",
            "options": [
                "Early restart",
                "Sequential interleaving",
                "Critical word first",
                "Block addressing"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique that requests the missed word first from memory and sends it to the processor as soon as it arrives, while filling the rest of the words in the block.",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Early restart** is a memory optimization technique where the processor doesn't wait for the entire memory block to be fetched when a cache miss occurs. Instead, it restarts execution as soon as the critical word (the one that caused the stall) is available. \n\nLet's look at the other options:\n\n* **Sequential interleaving:** A memory organization technique, not directly related to handling cache misses.\n* **Critical word first:**  Similar to early restart but doesn't imply fetching the rest of the block.\n* **Block addressing:** A fundamental addressing scheme, not a technique for handling cache misses. \n"
            }
        ]
    },
    {
        "doc_id": 1097,
        "doc": {
            "question_id": 123121121,
            "question": "____ prefetch loads data only into the cache and not the register.",
            "options": [
                "Cache",
                "Register",
                "Faulting",
                "Nonfaulting"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ prefetch loads data only into the cache and not the register.",
                "Expected Label": "A",
                "Predicted Label": "D) Nonfaulting \n\n**Explanation:**\n\n* **Nonfaulting prefetch** instructions are designed to load data into the cache without directly affecting the processor's registers. This allows the processor to continue executing other instructions while the data is being fetched in the background. If the data is needed later, it will already be in the cache, speeding up access.\n\nLet's look at why the other options are incorrect:\n\n* **Cache (A):**  While prefetching is related to the cache, \"cache\" itself doesn't describe the type of load.\n* **Register (B):** Register loads specifically bring data into registers for immediate use by the processor.\n* **Faulting (C):** Faulting prefetch would cause an exception if the data isn't available, interrupting the processor's flow. \n"
            }
        ]
    },
    {
        "doc_id": 1098,
        "doc": {
            "question_id": 123121125,
            "question": "In DDR SDRAM terminology, the ____ refers to the peak bandwidth of the DIMM in MB/sec.",
            "options": [
                "DRAM name",
                "Clock rate",
                "Transfer rate",
                "DIMM name"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In DDR SDRAM terminology, the ____ refers to the peak bandwidth of the DIMM in MB/sec.",
                "Expected Label": "D",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1099,
        "doc": {
            "question_id": 123121127,
            "question": "____ is a technology that protects against complete failure of a single memory chip in very large systems.",
            "options": [
                "RAID",
                "Chipkill",
                "ECC",
                "Parity"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technology that protects against complete failure of a single memory chip in very large systems.",
                "Expected Label": "B",
                "Predicted Label": "B) Chipkill \n\n**Explanation:**\n\n* **Chipkill** is specifically designed to handle the failure of an entire memory chip. It does this by distributing data across multiple chips, allowing the system to continue operating even if one chip completely fails.\n\nLet's look at why the other options are less suitable:\n\n* **RAID (Redundant Array of Independent Disks)** is used for storage devices (hard drives, SSDs), not memory chips. \n* **ECC (Error Correction Code)** can detect and correct single-bit errors in memory, but it might not handle the failure of an entire chip.\n* **Parity** is a simpler form of error detection than ECC and can only detect single-bit errors; it cannot correct them and wouldn't handle a whole chip failure. \n"
            }
        ]
    },
    {
        "doc_id": 1100,
        "doc": {
            "question_id": 123121128,
            "question": "____ is the primary mechanism that protects processes from each other in modern computer systems.",
            "options": [
                "Segmentation",
                "Paging",
                "Page-based virtual memory",
                "Translation lookaside buffer"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is the primary mechanism that protects processes from each other in modern computer systems.",
                "Expected Label": "C",
                "Predicted Label": "C) Page-based virtual memory \n\n**Explanation:**\n\nWhile the other options play a role in memory management, **page-based virtual memory** is the core mechanism enabling process isolation:\n\n* **Segmentation (A):**  Divides memory into logical segments, but doesn't inherently prevent access between processes unless combined with other protection mechanisms.\n* **Paging (B):** Breaks memory into fixed-size pages, a building block for virtual memory, but doesn't inherently enforce isolation.\n* **Page-based virtual memory (C):** Each process gets its own virtual address space, mapped to physical memory by the OS. This isolation prevents one process from directly accessing another's memory.\n* **Translation Lookaside Buffer (TLB) (D):** A cache speeding up address translation within virtual memory, but not the primary isolation mechanism. \n"
            }
        ]
    },
    {
        "doc_id": 1101,
        "doc": {
            "question_id": 123121130,
            "question": "The software that supports virtual machines is called a ____.",
            "options": [
                "virtual machine interface",
                "virtual machine monitor",
                "virtual machine translator",
                "virtual machine emulator"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The software that supports virtual machines is called a ____.",
                "Expected Label": "B",
                "Predicted Label": "B \n\nThe correct answer is **B) virtual machine monitor**. Here's why:\n\n* **Virtual Machine Monitor (VMM):** This is the software layer (also called a hypervisor) that sits between the hardware and the virtual machines. It manages and allocates resources, allowing multiple virtual machines to run simultaneously on a single physical host.\n\nLet's look at why the other options are not correct:\n\n* **Virtual Machine Interface (VMI):**  This refers to the communication interface between a virtual machine and the hypervisor (VMM), not the software itself.\n* **Virtual Machine Translator:** This term is not commonly used in the context of virtualization.\n* **Virtual Machine Emulator:** While emulation can be used in virtualization, it's not the general term for the supporting software. Emulation typically focuses on mimicking specific hardware. \n\n**In summary:** The virtual machine monitor (VMM) is the core software component that enables virtualization. \n"
            }
        ]
    },
    {
        "doc_id": 1102,
        "doc": {
            "question_id": 123121134,
            "question": "To prevent stale data issues in systems with caches, I/O operations typically occur directly to ____.",
            "options": [
                "the L1 cache",
                "the L2 cache",
                "main memory",
                "the CPU registers"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To prevent stale data issues in systems with caches, I/O operations typically occur directly to ____.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1103,
        "doc": {
            "question_id": 123121135,
            "question": "____ is typically used to reduce benchmark running times while maintaining a similar instruction mix for processor performance evaluation.",
            "options": [
                "Minnespec",
                "SPEC2000",
                "SPEC2006",
                "Dhrystone"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to reduce benchmark running times while maintaining a similar instruction mix for processor performance evaluation.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **B) SPEC2000**. Here's why:\n\n* **SPEC (Standard Performance Evaluation Corporation)** benchmarks are specifically designed for processor performance evaluation. \n* **SPEC2000** (and its successors like SPEC CPU2006) are industry-standard benchmark suites known for their focus on realistic workloads and a mix of instructions representative of real-world applications.\n\nLet's look at why the other options are incorrect:\n\n* **Minnespec:** This is not a standard term or benchmark in computer architecture.\n* **SPEC2006:** While similar to SPEC2000, the question implies a desire to *reduce* benchmark running times. SPEC2006 is a more demanding suite than SPEC2000, making it less likely to be chosen for that purpose.\n* **Dhrystone:** This is a synthetic benchmark primarily focused on integer performance. It's not known for representing a realistic mix of instructions found in modern applications. \n\n**Therefore, SPEC2000 strikes the best balance between representing a realistic instruction mix and offering a shorter runtime compared to more modern benchmark suites.** \n"
            }
        ]
    },
    {
        "doc_id": 1104,
        "doc": {
            "question_id": 123121136,
            "question": "In a typical multi-level cache hierarchy, the ____ cache is usually shared among all cores on a chip.",
            "options": [
                "L1",
                "L2",
                "L4",
                "L3"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a typical multi-level cache hierarchy, the ____ cache is usually shared among all cores on a chip.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1105,
        "doc": {
            "question_id": 123121142,
            "question": "The ____ approach to memory uses a glass that can be changed between amorphous and crystalline states.",
            "options": [
                "MRAM",
                "SDRAM",
                "DRAM",
                "PCRAM"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ approach to memory uses a glass that can be changed between amorphous and crystalline states.",
                "Expected Label": "D",
                "Predicted Label": "D) PCRAM \n\n**Explanation:**\n\n* **PCRAM (Phase-change RAM)** utilizes a special glass-like material called chalcogenide glass. This material can switch between two states: amorphous (random structure) and crystalline (ordered structure) when heated and cooled. These states represent 0 and 1, forming the basis for data storage.\n\nLet's look at why the other options are incorrect:\n\n* **MRAM (Magnetoresistive RAM)** uses magnetic elements to store data, not phase-changing glass.\n* **SDRAM (Synchronous Dynamic RAM)** and **DRAM (Dynamic RAM)** both store data as electrical charges in capacitors, requiring periodic refreshing. \n"
            }
        ]
    },
    {
        "doc_id": 1106,
        "doc": {
            "question_id": 123121144,
            "question": "In a cache memory hierarchy, ____ is typically used to determine which cache block to evict when the cache is full.",
            "options": [
                "First-In-First-Out (FIFO)",
                "Least Recently Used (LRU)",
                "Most Recently Used (MRU)",
                "Random Replacement"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a cache memory hierarchy, ____ is typically used to determine which cache block to evict when the cache is full.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1107,
        "doc": {
            "question_id": 123121146,
            "question": "____ is a tool used to estimate cache access times for different configurations in computer architecture research.",
            "options": [
                "CACTI",
                "Valgrind",
                "Gem5",
                "SimpleScalar"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a tool used to estimate cache access times for different configurations in computer architecture research.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1108,
        "doc": {
            "question_id": 123121150,
            "question": "____ is a technique that can reduce the performance overhead of virtualization compared to pure virtualization.",
            "options": [
                "Hyperthreading",
                "Paravirtualization",
                "Superscalar execution",
                "Branch prediction"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique that can reduce the performance overhead of virtualization compared to pure virtualization.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Paravirtualization (B)** modifies the guest operating system to cooperate with the hypervisor, reducing the need for hardware emulation and improving performance.\n\nLet's break down why the other options are incorrect:\n\n* **Hyperthreading (A)** is a technique that allows a single CPU core to execute multiple threads concurrently, improving CPU utilization but not directly addressing virtualization overhead.\n* **Superscalar execution (C)** is a technique where multiple instructions are executed simultaneously within a single CPU core, again not directly related to virtualization.\n* **Branch prediction (D)** is a technique used to improve pipeline performance by predicting the outcome of conditional branches, not related to virtualization overhead. \n"
            }
        ]
    },
    {
        "doc_id": 1109,
        "doc": {
            "question_id": 123121151,
            "question": "_____ is a technology that provides a second set of privilege levels for use by virtual machines.",
            "options": [
                "AMD-V",
                "IOMMU",
                "Intel VT-x",
                "SimpleScalar"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ is a technology that provides a second set of privilege levels for use by virtual machines.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) Intel VT-x**. Here's why:\n\n* **Intel VT-x (Virtualization Technology for x86)** and **AMD-V (AMD Virtualization)** are both hardware virtualization technologies. They introduce a new level of privilege below the operating system, allowing a hypervisor to run virtual machines with near-native performance.\n\nLet's look at the other options:\n\n* **IOMMU (Input/Output Memory Management Unit)**  is related to virtualization but focuses on managing DMA (Direct Memory Access) for devices, improving security and performance in virtualized environments.\n* **SimpleScalar** is a simulator for computer architecture research, not a hardware technology related to virtualization.\n\n**Therefore, while both Intel VT-x and AMD-V enable virtualization, the question specifically asks about a technology from Intel.** \n"
            }
        ]
    },
    {
        "doc_id": 1110,
        "doc": {
            "question_id": 123121153,
            "question": "____ is a technique used to improve uniprocessor throughput by exploiting thread-level parallelism.",
            "options": [
                "Instruction-level parallelism",
                "Multithreading",
                "Dynamic scheduling",
                "Branch prediction"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique used to improve uniprocessor throughput by exploiting thread-level parallelism.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1111,
        "doc": {
            "question_id": 123121154,
            "question": "The ____ market primarily uses processors that exploit instruction-level parallelism through static, compiler-based approaches.",
            "options": [
                "desktop",
                "server",
                "scientific computing",
                "personal mobile device"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ market primarily uses processors that exploit instruction-level parallelism through static, compiler-based approaches.",
                "Expected Label": "D",
                "Predicted Label": "D) personal mobile device \n\n**Explanation:**\n\nPersonal mobile devices (smartphones, tablets) prioritize power efficiency due to battery life constraints.  They often use simpler processor designs emphasizing static instruction-level parallelism, which is handled by the compiler during code generation. This approach is more power-efficient than dynamic techniques employed in larger processors.\n\nLet's analyze why other options are less suitable:\n\n* **A) Desktop:** Desktops require a balance of performance and power efficiency. They often use a mix of static and dynamic instruction-level parallelism.\n* **B) Server:** Servers prioritize high performance. They heavily rely on dynamic instruction-level parallelism, using complex hardware to identify and exploit parallelism during runtime.\n* **C) Scientific computing:** This field demands the highest performance, often using specialized processors with massive dynamic instruction-level parallelism. \n"
            }
        ]
    },
    {
        "doc_id": 1112,
        "doc": {
            "question_id": 123121155,
            "question": "____ is a measure of the maximum performance attainable by a pipeline implementation.",
            "options": [
                "Ideal pipeline CPI",
                "Dynamic branch frequency",
                "Basic block size",
                "Loop unrolling factor"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a measure of the maximum performance attainable by a pipeline implementation.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Ideal pipeline CPI** represents the theoretical minimum CPI (cycles per instruction) achievable if the pipeline operates at its maximum efficiency without any stalls or hazards. This is the ideal scenario and represents the peak performance a pipeline can reach.\n\nLet's look at why the other options are incorrect:\n\n* **Dynamic branch frequency** affects pipeline performance but doesn't directly measure the maximum attainable performance. High branch misprediction rates can significantly degrade performance, but even with perfect branch prediction, the ideal CPI dictates the upper limit.\n* **Basic block size** influences the amount of work available to be pipelined. Larger basic blocks can lead to better pipeline utilization, but again, the ideal CPI defines the performance ceiling.\n* **Loop unrolling factor** is a compiler optimization technique that can improve pipeline performance by reducing loop overhead and increasing instruction-level parallelism. However,  like the other factors, it doesn't directly quantify the maximum attainable performance. \n"
            }
        ]
    },
    {
        "doc_id": 1113,
        "doc": {
            "question_id": 123121157,
            "question": "An ____ occurs when an instruction writes to a register or memory location that a preceding instruction reads.",
            "options": [
                "output dependence",
                "data dependence",
                "flow dependence",
                "antidependence"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "An ____ occurs when an instruction writes to a register or memory location that a preceding instruction reads.",
                "Expected Label": "D",
                "Predicted Label": "D) antidependence \n"
            }
        ]
    },
    {
        "doc_id": 1114,
        "doc": {
            "question_id": 123121158,
            "question": "____ is a type of data hazard where an instruction tries to read a source before a previous instruction writes it.",
            "options": [
                "RAW (read after write)",
                "WAW (write after write)",
                "WAR (write after read)",
                "RWR (read while read)"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a type of data hazard where an instruction tries to read a source before a previous instruction writes it.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1115,
        "doc": {
            "question_id": 123121159,
            "question": "A ____ hazard occurs when there are some instructions that write results early in the instruction pipeline and other instructions that read a source late in the pipeline.",
            "options": [
                "RAW",
                "WAR",
                "WAW",
                "RAR"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ hazard occurs when there are some instructions that write results early in the instruction pipeline and other instructions that read a source late in the pipeline.",
                "Expected Label": "B",
                "Predicted Label": "A) RAW \n\n**Explanation:**\n\n* **RAW (Read-After-Write)** hazards occur when an instruction tries to read a value from a register before a previous instruction has finished writing to it. This is precisely the scenario described in the question, where early writes clash with late reads.\n\nLet's break down why the other options are incorrect:\n\n* **WAR (Write-After-Read):** This happens when an instruction tries to write to a register before a previous instruction has read from it.\n* **WAW (Write-After-Write):** This occurs when an instruction tries to write to a register before a previous instruction has finished writing to the same register.\n* **RAR (Read-After-Read):** This type of hazard doesn't exist. Multiple instructions reading the same value don't cause a hazard. \n"
            }
        ]
    },
    {
        "doc_id": 1116,
        "doc": {
            "question_id": 123121161,
            "question": "The property of whether a value will be used by an upcoming instruction is called ____.",
            "options": [
                "vitality",
                "persistence",
                "longevity",
                "liveness"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The property of whether a value will be used by an upcoming instruction is called ____.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1117,
        "doc": {
            "question_id": 123121162,
            "question": "____ is a key factor in a compiler's ability to schedule instructions to avoid pipeline stalls.",
            "options": [
                "Functional unit latency",
                "Cache size",
                "Register file size",
                "Branch prediction accuracy"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor in a compiler's ability to schedule instructions to avoid pipeline stalls.",
                "Expected Label": "A",
                "Predicted Label": "D) Branch prediction accuracy \n\n**Explanation:**\n\nWhile all the options have some impact on performance, branch prediction accuracy is the most directly related to pipeline stalls. Here's why:\n\n* **Branch prediction accuracy:**  Incorrect branch predictions force the pipeline to flush, wasting cycles fetching and decoding instructions that won't be used. Accurate predictions keep the pipeline full of useful instructions.\n* **Functional unit latency:**  High latency can cause stalls, but compilers try to schedule instructions to hide latency by interleaving independent operations.\n* **Cache size:** Larger caches improve overall performance by reducing memory access time, but don't directly address pipeline stalls caused by branches.\n* **Register file size:**  More registers can help compilers optimize code and reduce memory accesses, but their effect on branch-related stalls is indirect. \n"
            }
        ]
    },
    {
        "doc_id": 1118,
        "doc": {
            "question_id": 123121163,
            "question": "In a pipelined processor, ____ typically occur after a floating-point operation when the result is needed by a subsequent instruction.",
            "options": [
                "Branch hazards",
                "Pipeline stalls",
                "Cache misses",
                "Register spills"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a pipelined processor, ____ typically occur after a floating-point operation when the result is needed by a subsequent instruction.",
                "Expected Label": "B",
                "Predicted Label": "B) Pipeline stalls \n\n**Explanation:**\n\nPipeline stalls (also called bubbles) are specifically designed to handle situations where an instruction depends on the result of a previous instruction that hasn't finished executing. Floating-point operations often take multiple clock cycles to complete. If the very next instruction needs that result, the pipeline must stall until the floating-point operation is done. \n\nLet's look at why the other options are less likely:\n\n* **A) Branch hazards:**  While branch hazards can cause pipeline stalls, they are more directly related to the uncertainty of the next instruction's address.\n* **C) Cache misses:** Cache misses cause stalls, but they aren't specific to floating-point operations. They impact memory access in general.\n* **D) Register spills:** Register spills happen when there aren't enough registers to hold all the data a program needs.  They are related to register allocation, not directly to the pipeline's operation. \n"
            }
        ]
    },
    {
        "doc_id": 1119,
        "doc": {
            "question_id": 123121164,
            "question": "____ is a technique that replicates the loop body multiple times to increase the number of instructions relative to branch and overhead instructions.",
            "options": [
                "Loop fusion",
                "Software pipelining",
                "Loop unrolling",
                "Instruction prefetching"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a technique that replicates the loop body multiple times to increase the number of instructions relative to branch and overhead instructions.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1120,
        "doc": {
            "question_id": 123121167,
            "question": "Tournament predictors typically use a ____ to choose between different predictors.",
            "options": [
                "3-bit saturating counter",
                "1-bit flip-flop",
                "4-bit shift register",
                "2-bit saturating counter"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "Tournament predictors typically use a ____ to choose between different predictors.",
                "Expected Label": "D",
                "Predicted Label": "D) 2-bit saturating counter \n\n**Explanation:**\n\nTournament predictors work by keeping track of which predictor (e.g., a global predictor vs. a local predictor) has been more accurate recently. A 2-bit saturating counter is ideal for this:\n\n* **2-bits:**  Provides enough states to represent which predictor is \"winning\" and with what degree of confidence.\n* **Saturating:**  Ensures the counter doesn't overflow and misinterpret a long streak of correct predictions from one predictor.\n\nLet me know if you'd like more details on how tournament branch predictors work! \n"
            }
        ]
    },
    {
        "doc_id": 1121,
        "doc": {
            "question_id": 123121169,
            "question": "Dynamic scheduling helps overcome ____ hazards in pipelined processors.",
            "options": [
                "control",
                "structural",
                "data",
                "memory"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Dynamic scheduling helps overcome ____ hazards in pipelined processors.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1122,
        "doc": {
            "question_id": 123121171,
            "question": "____ is a technique used to avoid WAR and WAW hazards in out-of-order execution.",
            "options": [
                "Register renaming",
                "Branch prediction",
                "Speculation",
                "Pipelining"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used to avoid WAR and WAW hazards in out-of-order execution.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Register renaming** is specifically designed to address Write-After-Read (WAR) and Write-After-Write (WAW) hazards. It achieves this by logically renaming registers to eliminate false dependencies. \n\nLet's break down why the other options aren't the best fit:\n\n* **Branch prediction** deals with control hazards, predicting the outcome of branches to keep the pipeline filled.\n* **Speculation** is a broader concept that includes branch prediction and data speculation, aiming to execute instructions before knowing for sure if they should be executed.\n* **Pipelining** is a fundamental technique to improve instruction throughput but doesn't inherently solve WAR and WAW hazards.\n\n**In summary, register renaming is the most direct and targeted solution for WAR and WAW hazards in out-of-order execution.** \n"
            }
        ]
    },
    {
        "doc_id": 1123,
        "doc": {
            "question_id": 123121173,
            "question": "In Tomasulo's algorithm, ____ hazards are avoided by executing an instruction only when its operands are available.",
            "options": [
                "WAR",
                "WAW",
                "RAW",
                "All"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Tomasulo's algorithm, ____ hazards are avoided by executing an instruction only when its operands are available.",
                "Expected Label": "C",
                "Predicted Label": "C) RAW \n\n**Explanation:**\n\n* **RAW (Read After Write):** This hazard occurs when an instruction tries to read a register before a previous instruction has finished writing to it. Tomasulo's algorithm directly addresses this by using reservation stations and common data buses. An instruction only proceeds to execution when its operands are available in the common data bus, ensuring the read happens *after* the write.\n\n* **WAW (Write After Write) and WAR (Write After Read):**  Tomasulo's algorithm handles these hazards through register renaming using the common data bus and reservation stations.  It doesn't rely on waiting for operands to be available to solve these. \n"
            }
        ]
    },
    {
        "doc_id": 1124,
        "doc": {
            "question_id": 123121174,
            "question": "In Tomasulo's algorithm, ____ are used to provide register renaming.",
            "options": [
                "Register files",
                "Branch predictors",
                "Cache memories",
                "Reservation stations"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In Tomasulo's algorithm, ____ are used to provide register renaming.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1125,
        "doc": {
            "question_id": 123121175,
            "question": "____ are used to hold data or addresses coming from and going to memory in a dynamically scheduled processor.",
            "options": [
                "Load and store buffers",
                "Reservation stations",
                "Floating-point registers",
                "Functional units"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are used to hold data or addresses coming from and going to memory in a dynamically scheduled processor.",
                "Expected Label": "A",
                "Predicted Label": "**B) Reservation stations** \n\n**Explanation:**\n\n* **Reservation stations** act as a buffer in dynamically scheduled processors. They hold the instruction's opcode, operands (either values or register names), and information about the instruction's readiness to execute. This allows instructions to be processed out-of-order as soon as their dependencies are resolved.\n\nLet's look at why the other options are incorrect:\n\n* **Load and store buffers** specifically manage the movement of data between the processor and memory. While related to data flow, they are not the primary holders of data/addresses for all instructions.\n* **Floating-point registers** are specialized storage units within the processor designed specifically for holding floating-point numbers. \n* **Functional units** are the parts of the processor that actually perform operations (e.g., addition, multiplication). They receive data from reservation stations. \n"
            }
        ]
    },
    {
        "doc_id": 1126,
        "doc": {
            "question_id": 123121176,
            "question": "In a dynamically scheduled processor, the ____ stage renames registers, eliminating WAR and WAW hazards.",
            "options": [
                "Execute",
                "Issue",
                "Commit",
                "Fetch"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a dynamically scheduled processor, the ____ stage renames registers, eliminating WAR and WAW hazards.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1127,
        "doc": {
            "question_id": 123121177,
            "question": "In Tomasulo's algorithm, results are broadcast on a ____ which is monitored by the reservation stations.",
            "options": [
                "data bus",
                "address bus",
                "common data bus (CDB)",
                "instruction bus"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Tomasulo's algorithm, results are broadcast on a ____ which is monitored by the reservation stations.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1128,
        "doc": {
            "question_id": 123121179,
            "question": "In Tomasulo's algorithm, the ____ keeps the state of each operation that has issued.",
            "options": [
                "reservation station",
                "instruction status table",
                "register file",
                "reorder buffer"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In Tomasulo's algorithm, the ____ keeps the state of each operation that has issued.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1129,
        "doc": {
            "question_id": 123121180,
            "question": "In Tomasulo's algorithm, ____ occur in the write result stage, whether the destination is a register or memory.",
            "options": [
                "All reads",
                "All writes",
                "Only register writes",
                "Only memory writes"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In Tomasulo's algorithm, ____ occur in the write result stage, whether the destination is a register or memory.",
                "Expected Label": "B",
                "Predicted Label": "B) All writes \n\n**Explanation:**\n\nIn Tomasulo's algorithm, the write result stage is responsible for writing the computed result to its destination, regardless of whether it's a register or a memory location. This ensures data consistency and allows instructions waiting on that result to proceed. \n"
            }
        ]
    },
    {
        "doc_id": 1130,
        "doc": {
            "question_id": 123121183,
            "question": "____ is a key technique used in Tomasulo's scheme to resolve certain types of hazards.",
            "options": [
                "Branch prediction",
                "Register renaming",
                "Instruction prefetching",
                "Cache coherence"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a key technique used in Tomasulo's scheme to resolve certain types of hazards.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nTomasulo's algorithm heavily relies on **register renaming** to eliminate Write-After-Read (WAR) and Write-After-Write (WAW) hazards. It accomplishes this by using reservation stations and common data buses, allowing instructions with data dependencies to execute out-of-order without waiting for the previous instruction to complete writing to a register.\n\nLet's break down why the other options are incorrect:\n\n* **A) Branch prediction:** While important for pipelining, branch prediction doesn't directly address the hazards tackled by Tomasulo's algorithm.\n* **C) Instruction prefetching:** Prefetching improves instruction fetch efficiency but doesn't resolve the data hazards targeted by Tomasulo's algorithm. \n* **D) Cache coherence:** Cache coherence is crucial in multiprocessor systems to maintain data consistency but isn't the core technique used in Tomasulo's scheme. \n"
            }
        ]
    },
    {
        "doc_id": 1131,
        "doc": {
            "question_id": 123121185,
            "question": "The ____ holds the result of an instruction between the time the operation associated with the instruction completes and the time the instruction commits.",
            "options": [
                "Register File",
                "Reservation Station",
                "Store Buffer",
                "Reorder Buffer"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ holds the result of an instruction between the time the operation associated with the instruction completes and the time the instruction commits.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\n* **Reorder Buffer (ROB):**  The reorder buffer is specifically designed to hold the results of instructions *before* they are committed to the architectural state. This allows for out-of-order execution while maintaining the appearance of in-order completion to the programmer.\n\nLet's look at why the other options are incorrect:\n\n* **Register File:** Holds the architectural state of registers, not temporary results in flight.\n* **Reservation Station:** Holds instructions that are waiting for their operands to become available. \n* **Store Buffer:** Holds store instructions that are waiting for their target address to be calculated.\n\n**Therefore, the correct answer is D.** \n"
            }
        ]
    },
    {
        "doc_id": 1132,
        "doc": {
            "question_id": 123121187,
            "question": "In a processor with hardware-based speculation, if the ____ fills, the processor stops issuing instructions until an entry is made free.",
            "options": [
                "reservation station",
                "instruction queue",
                "reorder buffer",
                "branch prediction table"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a processor with hardware-based speculation, if the ____ fills, the processor stops issuing instructions until an entry is made free.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) reorder buffer**. Here's why:\n\n* **Reorder Buffer (ROB):** The reorder buffer is crucial in a processor with hardware-based speculation (out-of-order execution). It holds instructions in the order they were issued, even if they are executed out of order.  When the ROB fills up, it means the processor has speculated on many instructions and needs confirmations (results) to ensure correctness before retiring more instructions. This stall is necessary to maintain program order and handle potential exceptions or mispredictions.\n\nLet's look at why the other options are incorrect:\n\n* **Reservation Station:** Reservation stations hold instructions that are ready to execute but are waiting for their operands. While a full reservation station can stall instruction issue, it's not directly tied to speculation.\n* **Instruction Queue:** The instruction queue holds instructions fetched from memory before they are decoded and issued. A full instruction queue can stall fetching, but again, it's not the primary bottleneck caused by speculation.\n* **Branch Prediction Table:** The branch prediction table helps predict the outcome of branches. While mispredictions can impact speculation, the table filling up itself doesn't directly stall instruction issue.\n\n**In summary:**  The reorder buffer acts as a bottleneck when it fills up because it indicates the processor has reached its limit for handling speculated instructions until some are retired. \n"
            }
        ]
    },
    {
        "doc_id": 1133,
        "doc": {
            "question_id": 123121188,
            "question": "____ provides precise exceptions in addition to supporting speculative execution.",
            "options": [
                "Branch prediction",
                "Register renaming",
                "Out-of-order execution",
                "Reorder buffer with in-order instruction commit"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ provides precise exceptions in addition to supporting speculative execution.",
                "Expected Label": "D",
                "Predicted Label": "D) Reorder buffer with in-order instruction commit \n\nHere's why:\n\n* **Reorder buffers** are crucial for out-of-order execution. They hold instructions in the order they were fetched but allow them to complete (and write their results) out of order. \n* **In-order instruction commit** means that despite the out-of-order execution, the architectural state of the processor (registers, memory) is updated in the original program order.  \n\n**This combination is essential for precise exceptions:**\n\n* If an exception occurs, the reorder buffer allows the processor to determine exactly which instructions have been completed and which are still in flight.\n* By committing instructions in order, the processor can ensure that the exception handler sees the same state as if instructions had been executed sequentially.\n\n**Let's look at why the other options are not the best fit:**\n\n* **A) Branch prediction:** While important for performance, branch prediction primarily deals with guessing the outcome of branches and doesn't directly guarantee precise exceptions.\n* **B) Register renaming:**  Helps with out-of-order execution by avoiding false dependencies, but doesn't inherently ensure precise exception handling.\n* **C) Out-of-order execution:** This technique improves performance but introduces the challenge of handling exceptions precisely, which is addressed by the reorder buffer and in-order commit. \n"
            }
        ]
    },
    {
        "doc_id": 1134,
        "doc": {
            "question_id": 123121191,
            "question": "In out-of-order execution, ____ is used to track the status of registers and handle data dependencies between instructions.",
            "options": [
                "Program Counter",
                "Cache",
                "Register Status Table",
                "Branch Predictor"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In out-of-order execution, ____ is used to track the status of registers and handle data dependencies between instructions.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\n* **Register Status Table (RST)** is crucial in out-of-order execution. It keeps track of which registers are available (holding valid data) and which ones are awaiting results from instructions still in the pipeline. This helps determine if an instruction can execute or needs to stall due to data dependencies.\n\nLet's look at why the other options are incorrect:\n\n* **Program Counter (PC)** keeps track of the next instruction to be fetched, not the status of registers.\n* **Cache** is a high-speed memory that speeds up data access but doesn't manage register dependencies.\n* **Branch Predictor** predicts the outcome of branches to improve pipeline efficiency, but it doesn't handle register dependencies directly. \n"
            }
        ]
    },
    {
        "doc_id": 1135,
        "doc": {
            "question_id": 123121193,
            "question": "____ are inherently statically scheduled by the compiler.",
            "options": [
                "VLIW processors",
                "Superscalar processors",
                "RISC processors",
                "CISC processors"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are inherently statically scheduled by the compiler.",
                "Expected Label": "A",
                "Predicted Label": "A) VLIW processors \n\n**Explanation:**\n\n* **VLIW (Very Long Instruction Word)** processors are explicitly designed for static scheduling. The compiler bundles multiple independent operations into a single long instruction word, dictating the exact parallel execution order. \n\n* **Other options:**\n\n    * **Superscalar processors** rely on dynamic scheduling. Hardware determines instruction-level parallelism at runtime.\n    * **RISC and CISC**  refer to instruction set architectures and don't inherently dictate static or dynamic scheduling. Both types can be implemented with either scheduling approach. \n"
            }
        ]
    },
    {
        "doc_id": 1136,
        "doc": {
            "question_id": 123121195,
            "question": "The main technical challenge of VLIW processors is ____ compared to traditional architectures.",
            "options": [
                "reduced performance",
                "increased power consumption",
                "increased code size",
                "reduced instruction set"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The main technical challenge of VLIW processors is ____ compared to traditional architectures.",
                "Expected Label": "C",
                "Predicted Label": "C) increased code size \n\n**Explanation:**\n\nVLIW processors rely on compilers to group instructions into long bundles that can be executed in parallel. This often leads to larger code size because:\n\n* **Padding:** When the compiler can't fill a bundle completely with useful instructions, it inserts no-operation (NOP) instructions as padding.\n* **Multiple paths:**  Code compiled for different branch possibilities might be included directly within the instruction stream.\n\nLet me know if you'd like more details on any of the concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1137,
        "doc": {
            "question_id": 123121198,
            "question": "In a dynamically scheduled processor, ____ are used to hold instructions waiting for their operands to become available.",
            "options": [
                "Reorder buffers",
                "Register files",
                "Reservation stations",
                "Cache lines"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a dynamically scheduled processor, ____ are used to hold instructions waiting for their operands to become available.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Reservation stations** are the structures within a dynamically scheduled processor that hold instructions waiting for their operands. \n\nLet's break down why:\n\n* **Reservation stations** act as placeholders for instructions that are fetched but not yet ready to execute. They track the instruction, its operands (and their availability), and any dependencies. \n\n* **Reorder buffers** (A) are used to maintain the order of instructions after they have been executed out-of-order for committing results to registers in the original program order.\n\n* **Register files** (B) hold the actual data values that instructions operate on.\n\n* **Cache lines** (D) are units of data transfer between the processor and memory, used for faster data access. \n"
            }
        ]
    },
    {
        "doc_id": 1138,
        "doc": {
            "question_id": 123121199,
            "question": "In a dynamically scheduled processor, ____ is used to handle nonblocking cache misses.",
            "options": [
                "Reorder buffer",
                "Issue bundle",
                "Reservation station",
                "Load and store buffer"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a dynamically scheduled processor, ____ is used to handle nonblocking cache misses.",
                "Expected Label": "D",
                "Predicted Label": "D) Load and store buffer \n\n**Explanation:**\n\nWhile other components play roles in a dynamically scheduled processor, the Load and Store Buffer (LSQ) is specifically designed to handle non-blocking cache misses. Here's why:\n\n* **Load and Store Buffer (LSQ):** Holds memory access instructions (loads and stores) that are in progress.  It allows the processor to continue processing independent instructions while waiting for cache misses to be resolved.\n\nLet's look at the other options:\n\n* **Reorder Buffer:** Holds instructions that have been fetched and decoded but not yet completed. Its primary purpose is to maintain program order for exceptions and retirement.\n* **Issue Bundle:** A group of instructions fetched and issued together in a very long instruction word (VLIW) architecture.  Not directly related to handling cache misses.\n* **Reservation Station:** Holds information about instructions waiting for their operands to become available. Primarily involved in out-of-order execution but doesn't directly manage cache misses. \n"
            }
        ]
    },
    {
        "doc_id": 1139,
        "doc": {
            "question_id": 123121200,
            "question": "____ typically harms performance and dramatically lowers energy efficiency in speculative execution.",
            "options": [
                "Incorrect speculation",
                "Correct speculation",
                "Branch prediction",
                "Dynamic scheduling"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ typically harms performance and dramatically lowers energy efficiency in speculative execution.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1140,
        "doc": {
            "question_id": 123121201,
            "question": "____ is a technique that attempts to predict the result of a computation to further enhance ILP.",
            "options": [
                "Branch prediction",
                "Value prediction",
                "Register renaming",
                "Reorder buffering"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique that attempts to predict the result of a computation to further enhance ILP.",
                "Expected Label": "B",
                "Predicted Label": "**B) Value prediction** \n\n**Explanation:**\n\n* **Value prediction** aims to guess the outcome of a computation before it's actually executed. If the prediction is correct, the result can be used immediately, potentially skipping the entire calculation and improving ILP. \n\nLet's look at why the other options are not the best fit:\n\n* **Branch prediction** predicts the outcome of a conditional branch (e.g., if-else), not the result of a general computation.\n* **Register renaming** eliminates false dependencies between instructions caused by register reuse, enhancing ILP but not directly predicting computation results.\n* **Reorder buffering** allows instructions to execute out-of-order and be committed in the original program order. While it enhances ILP, it doesn't involve predicting computation results. \n"
            }
        ]
    },
    {
        "doc_id": 1141,
        "doc": {
            "question_id": 123121202,
            "question": "A ____ predicts the next instruction address for branches before decoding the instruction.",
            "options": [
                "branch predictor",
                "instruction cache",
                "branch-target buffer",
                "program counter"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ predicts the next instruction address for branches before decoding the instruction.",
                "Expected Label": "C",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Branch predictor** specifically analyzes branch instructions and attempts to predict whether the branch will be taken or not, and if so, to what address. \n\nLet's look at why the other options are incorrect:\n\n* **Instruction cache** stores recently fetched instructions to speed up access, but it doesn't predict branch addresses.\n* **Branch-target buffer (BTB)** caches the target addresses of recently taken branches, speeding up the process. However, it works *after* the branch has been decoded at least once.\n* **Program counter (PC)** holds the address of the next instruction to be fetched. It doesn't predict branches, but rather follows the determined path. \n"
            }
        ]
    },
    {
        "doc_id": 1142,
        "doc": {
            "question_id": 123121203,
            "question": "A ____ is used to predict the target address of branches before the branch instruction is decoded.",
            "options": [
                "Branch predictor",
                "Branch history table",
                "Return address stack",
                "Branch-target buffer"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "A ____ is used to predict the target address of branches before the branch instruction is decoded.",
                "Expected Label": "D",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1143,
        "doc": {
            "question_id": 123121205,
            "question": "An integrated instruction fetch unit typically includes ____ as part of its functionality.",
            "options": [
                "Cache coherence protocols",
                "Memory management",
                "Integrated branch prediction",
                "Register renaming"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "An integrated instruction fetch unit typically includes ____ as part of its functionality.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1144,
        "doc": {
            "question_id": 123121206,
            "question": "The primary purpose of ____ is to hide the cost of crossing cache blocks when fetching multiple instructions.",
            "options": [
                "Branch prediction",
                "Register renaming",
                "Reorder buffer",
                "Instruction prefetch"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary purpose of ____ is to hide the cost of crossing cache blocks when fetching multiple instructions.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1145,
        "doc": {
            "question_id": 123121210,
            "question": "____ is a technique that predicts whether two memory operations refer to the same address, allowing for safe instruction reordering if they do not conflict.",
            "options": [
                "Address aliasing prediction",
                "Value prediction",
                "Memory disambiguation",
                "Register renaming"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique that predicts whether two memory operations refer to the same address, allowing for safe instruction reordering if they do not conflict.",
                "Expected Label": "A",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1146,
        "doc": {
            "question_id": 123121211,
            "question": "____ is typically more abundant in floating-point programs compared to integer programs, allowing for greater exploitation of instruction-level parallelism.",
            "options": [
                "Loop-level parallelism",
                "Branch prediction accuracy",
                "Register renaming efficiency",
                "Cache hit rate"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically more abundant in floating-point programs compared to integer programs, allowing for greater exploitation of instruction-level parallelism.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1147,
        "doc": {
            "question_id": 123121212,
            "question": "The ____ of a processor is calculated by dividing the clock rate by the CPI.",
            "options": [
                "Instruction count",
                "Pipeline depth",
                "Instruction execution rate",
                "Cache miss rate"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ of a processor is calculated by dividing the clock rate by the CPI.",
                "Expected Label": "C",
                "Predicted Label": "C) Instruction execution rate \n\n**Explanation:**\n\nThe instruction execution rate (also known as Instructions Per Cycle or IPC) is a key performance metric for processors. It represents the average number of instructions executed by a processor per clock cycle.  Here's why:\n\n* **Clock Rate:**  The frequency at which a processor operates.\n* **CPI (Cycles Per Instruction):** The average number of clock cycles it takes to execute one instruction.\n\n**Calculation:**\n\nInstruction Execution Rate = Clock Rate / CPI \n\n**In simpler terms:** If a processor has a clock rate of 2 GHz (2 billion cycles per second) and a CPI of 1, it can theoretically execute 2 billion instructions per second. \n"
            }
        ]
    },
    {
        "doc_id": 1148,
        "doc": {
            "question_id": 123121213,
            "question": "The primary limitation of speculating on multiple paths in a processor is ____ .",
            "options": [
                "increased power consumption",
                "reduced branch prediction accuracy",
                "higher cache miss rates",
                "exponential growth in required hardware resources"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary limitation of speculating on multiple paths in a processor is ____ .",
                "Expected Label": "D",
                "Predicted Label": "D) exponential growth in required hardware resources \n"
            }
        ]
    },
    {
        "doc_id": 1149,
        "doc": {
            "question_id": 123121214,
            "question": "____ is a key capability for extensive speculation that is challenging to achieve at compile time for integer programs with pointers.",
            "options": [
                "Memory disambiguation",
                "Branch prediction",
                "Instruction scheduling",
                "Register allocation"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key capability for extensive speculation that is challenging to achieve at compile time for integer programs with pointers.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1150,
        "doc": {
            "question_id": 123121215,
            "question": "The main disadvantage of supporting speculation in hardware is the ____ required.",
            "options": [
                "compiler complexity",
                "additional hardware resources",
                "software optimization",
                "instruction set modifications"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The main disadvantage of supporting speculation in hardware is the ____ required.",
                "Expected Label": "B",
                "Predicted Label": "B) additional hardware resources \n"
            }
        ]
    },
    {
        "doc_id": 1151,
        "doc": {
            "question_id": 123121216,
            "question": "_____ allows multiple threads to share the functional units of a single processor in an overlapping fashion.",
            "options": [
                "Superscalar execution",
                "Pipelining",
                "Multithreading",
                "Branch prediction"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ allows multiple threads to share the functional units of a single processor in an overlapping fashion.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1152,
        "doc": {
            "question_id": 123121218,
            "question": "____ uses thread-level parallelism to hide long-latency events in a processor, thereby increasing the usage of the functional units.",
            "options": [
                "Simultaneous multithreading",
                "Superscalar processing",
                "Pipelining",
                "Branch prediction"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ uses thread-level parallelism to hide long-latency events in a processor, thereby increasing the usage of the functional units.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1153,
        "doc": {
            "question_id": 123121219,
            "question": "____ multithreading switches threads only when there is a stall in the processor pipeline.",
            "options": [
                "Fine-grained",
                "Coarse-grained",
                "Simultaneous",
                "Interleaved"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ multithreading switches threads only when there is a stall in the processor pipeline.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1154,
        "doc": {
            "question_id": 123121220,
            "question": "In multithreaded processors, the ____ state indicates that a thread is waiting for the completion of an event such as a cache miss or pipeline delay.",
            "options": [
                "Executing",
                "Ready",
                "Chosen",
                "Not ready"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In multithreaded processors, the ____ state indicates that a thread is waiting for the completion of an event such as a cache miss or pipeline delay.",
                "Expected Label": "D",
                "Predicted Label": "D) Not ready \n\n**Explanation:**\n\n* **Not ready** is the correct term for a thread waiting on an external event like a cache miss or pipeline stall. \n\nHere's why the other options are incorrect:\n\n* **Executing:** The thread is actively using processor resources.\n* **Ready:** The thread is prepared to execute and is waiting for its turn on the processor.\n* **Chosen:** This term isn't commonly used in the context of thread states within multithreaded processors. \n"
            }
        ]
    },
    {
        "doc_id": 1155,
        "doc": {
            "question_id": 123121223,
            "question": "In a simple scoreboard architecture, operands for instructions are always retrieved from ____.",
            "options": [
                "cache",
                "registers",
                "main memory",
                "instruction buffer"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a simple scoreboard architecture, operands for instructions are always retrieved from ____.",
                "Expected Label": "B",
                "Predicted Label": "B) registers \n"
            }
        ]
    },
    {
        "doc_id": 1156,
        "doc": {
            "question_id": 123121224,
            "question": "The ____ is typically used to achieve high instruction throughput by combining multiple issue and high clock rates in modern processors.",
            "options": [
                "Instruction cache",
                "Branch predictor",
                "L1 data cache",
                "Out-of-order speculative microarchitecture"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is typically used to achieve high instruction throughput by combining multiple issue and high clock rates in modern processors.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1157,
        "doc": {
            "question_id": 123121225,
            "question": "____ is used to speed up function return in instruction fetch.",
            "options": [
                "Return address stack",
                "Branch target buffer",
                "Instruction cache",
                "Reorder buffer"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is used to speed up function return in instruction fetch.",
                "Expected Label": "A",
                "Predicted Label": "A) Return address stack \n"
            }
        ]
    },
    {
        "doc_id": 1158,
        "doc": {
            "question_id": 123121226,
            "question": "____ is a technique that combines certain instruction pairs to issue them to a single reservation station while still allowing them to issue independently.",
            "options": [
                "Macrofusion",
                "Microfusion",
                "Loop stream detection",
                "Register renaming"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique that combines certain instruction pairs to issue them to a single reservation station while still allowing them to issue independently.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1159,
        "doc": {
            "question_id": 123121229,
            "question": "In register renaming, the ____ register designation is used to index the rename hardware.",
            "options": [
                "destination",
                "source",
                "temporary",
                "accumulator"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In register renaming, the ____ register designation is used to index the rename hardware.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **(A) destination**. Here's why:\n\n* **Register Renaming:**  This technique eliminates false dependencies in a processor's pipeline. False dependencies occur when instructions appear to rely on each other because they use the same register names, even if they operate on different data.\n* **How Renaming Works:** When an instruction is decoded, the destination register is used as a key to look up a free physical register from a pool.  This physical register is then associated with the instruction's destination register name. Subsequent instructions that depend on this result are redirected to use the newly assigned physical register. \n\n**In essence, the destination register acts as the identifier for renaming, ensuring that the correct data dependencies are maintained.** \n"
            }
        ]
    },
    {
        "doc_id": 1160,
        "doc": {
            "question_id": 123121232,
            "question": "In pipelined processors, ____ is typically used to eliminate stalls caused by data hazards.",
            "options": [
                "Branch prediction",
                "Out-of-order execution",
                "Register renaming",
                "Operand forwarding"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In pipelined processors, ____ is typically used to eliminate stalls caused by data hazards.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1161,
        "doc": {
            "question_id": 123121233,
            "question": "____ is typically used to communicate results between functional units in Tomasulo's algorithm.",
            "options": [
                "Common data bus",
                "Register file",
                "Reservation stations",
                "Reorder buffer"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to communicate results between functional units in Tomasulo's algorithm.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1162,
        "doc": {
            "question_id": 123121234,
            "question": "In a Tomasulo algorithm implementation, the ____ can become a bottleneck, limiting the number of results that can be computed per clock cycle.",
            "options": [
                "Reservation Station",
                "Common Data Bus",
                "Reorder Buffer",
                "Load/Store Queue"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a Tomasulo algorithm implementation, the ____ can become a bottleneck, limiting the number of results that can be computed per clock cycle.",
                "Expected Label": "B",
                "Predicted Label": "B) Common Data Bus \n\n**Explanation:**\n\nThe Common Data Bus (CDB) acts as a central highway in the Tomasulo algorithm. All execution units use it to broadcast their results to reservation stations and load/store queues waiting for those values. If the CDB has limited bandwidth (e.g., can only broadcast one result per cycle), it creates a bottleneck.  Multiple units might finish computations simultaneously, but only one can broadcast its result per cycle, delaying dependent instructions. \n"
            }
        ]
    },
    {
        "doc_id": 1163,
        "doc": {
            "question_id": 123121236,
            "question": "The primary focus of ____ is to accelerate graphics and image processing workloads.",
            "options": [
                "Vector processors",
                "SIMD extensions",
                "Loop-level parallelism",
                "Graphics Processing Units"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary focus of ____ is to accelerate graphics and image processing workloads.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1164,
        "doc": {
            "question_id": 123121238,
            "question": "In vector architectures, _____ act as compiler-controlled buffers to hide memory latency and leverage memory bandwidth.",
            "options": [
                "Cache memories",
                "Branch predictors",
                "Translation lookaside buffers",
                "Large register files"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In vector architectures, _____ act as compiler-controlled buffers to hide memory latency and leverage memory bandwidth.",
                "Expected Label": "D",
                "Predicted Label": "D) Large register files \n\n**Explanation:**\n\nLarge register files are a key characteristic of vector architectures. They act as a staging area between the processor's functional units and the main memory. By using large register files, vector processors can:\n\n* **Hide Memory Latency:** Load data into registers before it's needed, overlapping memory access time with computation.\n* **Exploit Memory Bandwidth:** Fetch data in large chunks and keep it readily available for vector operations.\n\nLet me know if you would like more details on vector architectures! \n"
            }
        ]
    },
    {
        "doc_id": 1165,
        "doc": {
            "question_id": 123121244,
            "question": "In vector processors, ____ is divided across lanes, with each lane holding every nth element of each vector register, where n is the number of lanes.",
            "options": [
                "Scalar register storage",
                "Cache memory",
                "Vector register storage",
                "Instruction pipeline"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In vector processors, ____ is divided across lanes, with each lane holding every nth element of each vector register, where n is the number of lanes.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1166,
        "doc": {
            "question_id": 123121245,
            "question": "The technique used to handle vector operations longer than the maximum vector length is called ____.",
            "options": [
                "loop unrolling",
                "vectorization",
                "pipelining",
                "strip mining"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The technique used to handle vector operations longer than the maximum vector length is called ____.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1167,
        "doc": {
            "question_id": 123121246,
            "question": "In vector processors, ____ are part of the architectural state and are explicitly manipulated by compilers.",
            "options": [
                "arithmetic units",
                "mask registers",
                "load/store units",
                "memory banks"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In vector processors, ____ are part of the architectural state and are explicitly manipulated by compilers.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1168,
        "doc": {
            "question_id": 123121248,
            "question": "In multidimensional arrays, the distance between elements to be gathered into a single register is called the ____.",
            "options": [
                "offset",
                "spacing",
                "gap",
                "stride"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In multidimensional arrays, the distance between elements to be gathered into a single register is called the ____.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1169,
        "doc": {
            "question_id": 123121249,
            "question": "____ is a technique used in vector architectures to handle non-contiguous memory accesses for sparse matrices.",
            "options": [
                "Gather-scatter",
                "Strided access",
                "Bank interleaving",
                "Prefetching"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in vector architectures to handle non-contiguous memory accesses for sparse matrices.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1170,
        "doc": {
            "question_id": 123121250,
            "question": "____ is a hardware feature that supports efficient handling of sparse vector operations in modern vector processors.",
            "options": [
                "Cache coherence",
                "Gather-scatter",
                "Branch prediction",
                "Superscalar execution"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a hardware feature that supports efficient handling of sparse vector operations in modern vector processors.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Gather-scatter** operations are specifically designed to handle non-contiguous memory accesses, which are characteristic of operations on sparse vectors. \n\nLet's look at why the other options are incorrect:\n\n* **Cache coherence** deals with maintaining data consistency across multiple caches, not directly with sparse vectors.\n* **Branch prediction** improves performance by predicting the outcome of conditional branches, not relevant to vector operations.\n* **Superscalar execution** involves executing multiple instructions concurrently, which can be beneficial for vector processing in general, but isn't a feature specifically targeting sparse vectors. \n"
            }
        ]
    },
    {
        "doc_id": 1171,
        "doc": {
            "question_id": 123121255,
            "question": "To enable compilers to generate SIMD instructions automatically, programmers must ensure that data in memory is ____ to the width of the SIMD unit.",
            "options": [
                "randomized",
                "hashed",
                "aligned",
                "compressed"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To enable compilers to generate SIMD instructions automatically, programmers must ensure that data in memory is ____ to the width of the SIMD unit.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1172,
        "doc": {
            "question_id": 123121258,
            "question": "____ is the programming model classification NVIDIA uses for CUDA.",
            "options": [
                "SIMD",
                "SIMT",
                "MIMD",
                "SPMD"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is the programming model classification NVIDIA uses for CUDA.",
                "Expected Label": "B",
                "Predicted Label": "B\n \n\n**SIMT (Single Instruction, Multiple Threads)** is the programming model classification NVIDIA uses for CUDA. \n\nHere's why:\n\n* **SIMT** allows a single instruction to be executed across multiple threads simultaneously, but each thread has its own program counter and register state. This allows for efficient execution of data-parallel workloads on GPUs.\n\nLet's look at the other options:\n\n* **SIMD (Single Instruction, Multiple Data)** executes a single instruction on multiple data points within a single thread. While GPUs utilize SIMD units, it's not the overarching programming model.\n* **MIMD (Multiple Instruction, Multiple Data)** allows multiple threads to execute different instructions on different data. While GPUs are capable of MIMD, CUDA primarily focuses on the SIMT model. \n* **SPMD (Single Program, Multiple Data)** means all threads execute the same program but on different parts of the data.  SIMT is a specialized form of SPMD optimized for GPU architectures.\n\n**Therefore, SIMT (B) is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 1173,
        "doc": {
            "question_id": 123121259,
            "question": "In GPU architecture, a ____ is composed of Thread Blocks and represents the entire code that runs on the GPU.",
            "options": [
                "SIMD instruction",
                "Processor",
                "Vector",
                "Grid"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In GPU architecture, a ____ is composed of Thread Blocks and represents the entire code that runs on the GPU.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1174,
        "doc": {
            "question_id": 123121261,
            "question": "In a multithreaded SIMD Processor, the ____ is responsible for scheduling multiple independent threads of SIMD instructions.",
            "options": [
                "Thread Block Scheduler",
                "Instruction cache",
                "SIMD Thread Scheduler",
                "Address coalescing unit"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a multithreaded SIMD Processor, the ____ is responsible for scheduling multiple independent threads of SIMD instructions.",
                "Expected Label": "C",
                "Predicted Label": "**C) SIMD Thread Scheduler** \n\n**Explanation:**\n\n* **SIMD Thread Scheduler** is specifically designed to manage and schedule multiple independent threads composed of SIMD (Single Instruction, Multiple Data) instructions. It ensures efficient utilization of SIMD processing units by switching between threads to maximize parallelism.\n\nLet's look at why the other options are incorrect:\n\n* **Thread Block Scheduler:**  This term is more common in the context of GPUs, where a thread block is a group of threads scheduled together. While it deals with thread scheduling, it's not specific to SIMD instructions.\n* **Instruction cache:** This is a memory component that stores recently fetched instructions, speeding up instruction access. It's not involved in thread scheduling.\n* **Address coalescing unit:** This unit optimizes memory access patterns in GPUs, particularly for threads accessing adjacent memory locations. It's not directly related to scheduling SIMD threads. \n"
            }
        ]
    },
    {
        "doc_id": 1175,
        "doc": {
            "question_id": 123121263,
            "question": "In GPU architectures, ____ are used to perform parallel operations on SIMD instructions.",
            "options": [
                "SIMD Lanes",
                "Thread Blocks",
                "Dispatch Units",
                "Scoreboards"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In GPU architectures, ____ are used to perform parallel operations on SIMD instructions.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1176,
        "doc": {
            "question_id": 123121265,
            "question": "In NVIDIA GPUs, the translation of PTX instructions to hardware instructions occurs in ____ at load time.",
            "options": [
                "hardware",
                "firmware",
                "software",
                "microcode"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In NVIDIA GPUs, the translation of PTX instructions to hardware instructions occurs in ____ at load time.",
                "Expected Label": "C",
                "Predicted Label": "A) hardware \n\n**Explanation:**\n\nNVIDIA GPUs translate PTX (Parallel Thread Execution) instructions directly into hardware instructions within the GPU's hardware units. This translation process happens at load time, meaning when the PTX code is submitted to the GPU for execution.\n\nLet's eliminate the other options:\n\n* **Firmware:** While firmware plays a role in the overall GPU operation, it doesn't handle the dynamic translation of PTX instructions.\n* **Software:** Software running on the CPU might compile higher-level languages to PTX, but the actual PTX-to-hardware translation is a hardware function within the GPU. \n* **Microcode:**  Microcode is a lower-level instruction set used in some processors, but modern GPUs like NVIDIA's rely on hardware translation for PTX.\n\n**Therefore, the most accurate answer is (A).** \n"
            }
        ]
    },
    {
        "doc_id": 1177,
        "doc": {
            "question_id": 123121266,
            "question": "____ is typically used in GPUs for thread synchronization within a block.",
            "options": [
                "exit",
                "ret",
                "call",
                "bar.sync"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is typically used in GPUs for thread synchronization within a block.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1178,
        "doc": {
            "question_id": 123121268,
            "question": "In GPU architecture, ____ is used to enable or disable each SIMD Lane for predicated instructions.",
            "options": [
                "A stack entry",
                "A per-lane predicate register",
                "A thread-active mask",
                "An identifier token"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In GPU architecture, ____ is used to enable or disable each SIMD Lane for predicated instructions.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **C) A thread-active mask**. Here's why:\n\n* **Thread-active masks** are specifically designed in GPU architectures to manage which SIMD lanes execute a predicated instruction. Each bit in the mask corresponds to a lane, and a set bit enables the lane, while a cleared bit disables it.\n\nLet's look at why the other options are incorrect:\n\n* **A) A stack entry:** Stacks are used for function calls and data storage, not for controlling individual SIMD lanes.\n* **B) A per-lane predicate register:** While conceptually similar, per-lane predicate registers are less common than a unified thread-active mask for managing lane execution.\n* **D) An identifier token:**  Identifier tokens are a generic concept and don't have a specific role in SIMD lane control within GPU architecture. \n\n**Therefore, C is the most accurate answer.** \n"
            }
        ]
    },
    {
        "doc_id": 1179,
        "doc": {
            "question_id": 123121269,
            "question": "In SIMD architectures, ____ are used to control which lanes execute specific instructions in conditional branches.",
            "options": [
                "Branch predictors",
                "Cache coherence protocols",
                "Active masks",
                "Scoreboarding techniques"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In SIMD architectures, ____ are used to control which lanes execute specific instructions in conditional branches.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1180,
        "doc": {
            "question_id": 123121271,
            "question": "In GPU architectures, ____ is used to hide memory latency instead of relying on large caches.",
            "options": [
                "Prefetching",
                "Multithreading",
                "Branch prediction",
                "Speculative execution"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In GPU architectures, ____ is used to hide memory latency instead of relying on large caches.",
                "Expected Label": "B",
                "Predicted Label": "B) Multithreading \n"
            }
        ]
    },
    {
        "doc_id": 1181,
        "doc": {
            "question_id": 123121272,
            "question": "In GPU architectures, ____ is typically used to hide DRAM latency.",
            "options": [
                "Larger cache sizes",
                "Higher clock speeds",
                "Multiple threads",
                "Wider memory buses"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In GPU architectures, ____ is typically used to hide DRAM latency.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1182,
        "doc": {
            "question_id": 123121273,
            "question": "In GPU architectures, ____ is typically used to improve the dependability of long-running applications across thousands of servers.",
            "options": [
                "Branch prediction",
                "Speculative execution",
                "Out-of-order execution",
                "Error Correcting Codes"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In GPU architectures, ____ is typically used to improve the dependability of long-running applications across thousands of servers.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Error Correcting Codes (ECC)** are specifically designed to detect and correct errors in data storage and transmission, crucial for maintaining data integrity in large-scale, fault-tolerant systems like those using GPUs across many servers.\n\nLet's look at why the other options are less suitable:\n\n* **A) Branch prediction** and **B) Speculative execution** are performance optimization techniques and don't directly address dependability or error handling.\n* **C) Out-of-order execution**, while also performance-oriented, can complicate error recovery if a fault occurs. \n\nTherefore, **D) Error Correcting Codes** is the most relevant and accurate answer. \n"
            }
        ]
    },
    {
        "doc_id": 1183,
        "doc": {
            "question_id": 123121274,
            "question": "____ is typically responsible for handling atomic instructions in GPU architectures.",
            "options": [
                "A special hardware unit associated with the L2 cache",
                "The SIMD Processors",
                "The register file",
                "The instruction decoder"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically responsible for handling atomic instructions in GPU architectures.",
                "Expected Label": "A",
                "Predicted Label": "**B) The SIMD Processors** \n\nHere's why:\n\n* **SIMD Processors (Single Instruction, Multiple Data):** GPUs heavily rely on SIMD architectures. Atomic operations, which need to be performed without interference, are often handled directly within the SIMD execution units to ensure atomicity across multiple threads operating on potentially shared data.\n\nLet me know if you'd like more explanation on any of the other options! \n"
            }
        ]
    },
    {
        "doc_id": 1184,
        "doc": {
            "question_id": 123121279,
            "question": "____ dependences arise when an operand is written at some point and read at a later point.",
            "options": [
                "Control",
                "Name",
                "Data",
                "Loop"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ dependences arise when an operand is written at some point and read at a later point.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1185,
        "doc": {
            "question_id": 123121280,
            "question": "In loop-level parallelism analysis, ____ are typically easier for compilers to recognize and eliminate compared to other types of dependences.",
            "options": [
                "Loop-carried dependences",
                "Data hazards",
                "Control dependences",
                "Induction variable dependences"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In loop-level parallelism analysis, ____ are typically easier for compilers to recognize and eliminate compared to other types of dependences.",
                "Expected Label": "D",
                "Predicted Label": "D) Induction variable dependences \n\n**Explanation:**\n\nInduction variable dependences are a specific type of data dependence that occur within loops, where a variable (the induction variable) is incremented or decremented by a constant value in each iteration. Compilers are good at recognizing these patterns and can often eliminate them through techniques like strength reduction or induction variable elimination, thus enabling loop parallelization.\n\nHere's why the other options are less likely:\n\n* **Loop-carried dependences:** These dependences exist between iterations of a loop, making parallelization difficult.\n* **Data hazards:** A broader term encompassing various dependences, including loop-carried ones.\n* **Control dependences:** These arise from control flow statements (like if-else) and can hinder parallelization. \n\nLet me know if you would like more details on induction variable elimination or other loop optimization techniques! \n"
            }
        ]
    },
    {
        "doc_id": 1186,
        "doc": {
            "question_id": 123121283,
            "question": "____ is typically the limiting factor for computational performance when the arithmetic intensity is low.",
            "options": [
                "Cache size",
                "Memory bandwidth",
                "Clock speed",
                "Register file size"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically the limiting factor for computational performance when the arithmetic intensity is low.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1187,
        "doc": {
            "question_id": 123121284,
            "question": "In throughput computing kernels, ____ is typically bandwidth bound for large matrices.",
            "options": [
                "SGEMM",
                "Monte Carlo",
                "SpMV",
                "GJK"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In throughput computing kernels, ____ is typically bandwidth bound for large matrices.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1188,
        "doc": {
            "question_id": 123122166,
            "question": "____ parallelism became a key focus for improving computer performance when increasing single-thread speed reached diminishing returns.",
            "options": [
                "Instruction-level",
                "Thread-level",
                "Data-level",
                "Task-level"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ parallelism became a key focus for improving computer performance when increasing single-thread speed reached diminishing returns.",
                "Expected Label": "B",
                "Predicted Label": "B) Thread-level \n\n**Explanation:**\n\nWhile all the options represent forms of parallelism used in computer architecture, the shift to **thread-level parallelism** was a direct response to the plateauing of single-thread performance gains.  Here's why:\n\n* **Instruction-level parallelism (ILP)** focuses on extracting parallel operations within a single instruction stream. While important, ILP techniques were already heavily exploited, and their benefits were reaching a limit.\n* **Data-level parallelism (DLP)** operates on large datasets simultaneously (think SIMD operations). While powerful, it's not the primary focus when single-thread performance is the bottleneck.\n* **Task-level parallelism** is broader and often relates to distributing work across different processing units, which is helpful but not the core solution to single-thread limitations.\n\n**Thread-level parallelism**, by allowing multiple independent threads of execution within a process, provided a way to leverage the increasing number of processor cores becoming available. This approach directly addressed the limitations of relying solely on improving the performance of a single instruction stream. \n"
            }
        ]
    },
    {
        "doc_id": 1189,
        "doc": {
            "question_id": 123122167,
            "question": "Thread-level parallelism implies the existence of multiple ____ in a system.",
            "options": [
                "cache hierarchies",
                "memory controllers",
                "program counters",
                "branch predictors"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Thread-level parallelism implies the existence of multiple ____ in a system.",
                "Expected Label": "C",
                "Predicted Label": "C) program counters \n\n**Explanation:**\n\nThread-level parallelism means running multiple threads (independent units of execution) concurrently. Each thread requires its own program counter (PC) to track the current instruction being executed. \n\nLet's look at why the other options are incorrect:\n\n* **A) Cache hierarchies:** While caches improve performance, they don't directly imply thread-level parallelism. A single-threaded program can still benefit from caches.\n* **B) Memory controllers:** Similar to caches, memory controllers manage memory access but don't necessitate multiple threads.\n* **D) Branch predictors:** Branch predictors improve pipeline performance and are not directly related to having multiple threads. \n"
            }
        ]
    },
    {
        "doc_id": 1190,
        "doc": {
            "question_id": 123122169,
            "question": "In ____, communication among threads occurs through a shared address space.",
            "options": [
                "message-passing systems",
                "distributed memory systems",
                "SMP and DSM architectures",
                "cluster computing"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In ____, communication among threads occurs through a shared address space.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1191,
        "doc": {
            "question_id": 123122172,
            "question": "In a coherent memory system, a read by a processor to a location following its own write to that location, with no intervening writes by other processors, must return the value ____.",
            "options": [
                "from main memory",
                "from the most recent write by any processor",
                "written by that processor",
                "from the shared cache"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a coherent memory system, a read by a processor to a location following its own write to that location, with no intervening writes by other processors, must return the value ____.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1192,
        "doc": {
            "question_id": 123122173,
            "question": "____ defines the behavior of reads and writes to the same memory location.",
            "options": [
                "Consistency",
                "Serialization",
                "Instantaneity",
                "Coherence"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ defines the behavior of reads and writes to the same memory location.",
                "Expected Label": "D",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1193,
        "doc": {
            "question_id": 123122174,
            "question": "____ is a cache coherence protocol where every cache that has a copy of data from a block of physical memory tracks the sharing status of the block.",
            "options": [
                "Snooping",
                "Directory-based",
                "Invalidation",
                "Write-through"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a cache coherence protocol where every cache that has a copy of data from a block of physical memory tracks the sharing status of the block.",
                "Expected Label": "A",
                "Predicted Label": "A) Snooping \n\n**Explanation:**\n\n* **Snooping protocols** rely on caches constantly \"listening\" (or snooping) on the bus for memory transactions related to the data they hold. When a cache detects a write to a shared block, it can update or invalidate its own copy, ensuring coherence.\n\nLet's look at why the other options aren't the best fit:\n\n* **Directory-based (B):** These protocols use a centralized directory to track which caches have copies of specific data blocks. \n* **Invalidation (C):**  This is a mechanism *used* in coherence protocols (including snooping) where stale copies of data are marked invalid. It's not a protocol itself.\n* **Write-through (D):** This is a write policy where data is written to both the cache and the main memory simultaneously. While relevant to caching, it's not a cache coherence protocol. \n"
            }
        ]
    },
    {
        "doc_id": 1194,
        "doc": {
            "question_id": 123122176,
            "question": "In a snooping-based cache coherence protocol, processors continuously monitor the ____ to check for invalidation requests.",
            "options": [
                "memory bus",
                "network interface",
                "shared bus",
                "cache controller"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a snooping-based cache coherence protocol, processors continuously monitor the ____ to check for invalidation requests.",
                "Expected Label": "C",
                "Predicted Label": "C) shared bus \n\n**Explanation:**\n\nSnooping protocols rely on processors having access to a shared communication medium (like a shared bus) to observe memory transactions from other processors. When a processor intends to modify data in its cache, it broadcasts this intention across the shared bus. Other processors \"snoop\" on this bus, and if they have a copy of that data, they invalidate their copy to maintain coherence. \n"
            }
        ]
    },
    {
        "doc_id": 1195,
        "doc": {
            "question_id": 123122178,
            "question": "____ is a state in a simple cache coherence protocol that indicates the block has been updated in the private cache.",
            "options": [
                "Modified",
                "Shared",
                "Invalid",
                "Exclusive"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a state in a simple cache coherence protocol that indicates the block has been updated in the private cache.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1196,
        "doc": {
            "question_id": 123122179,
            "question": "In multicore processors, cache coherence between cores is typically implemented using ____ protocols.",
            "options": [
                "Distributed",
                "Asynchronous",
                "Hierarchical",
                "Snooping or simple central directory"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In multicore processors, cache coherence between cores is typically implemented using ____ protocols.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1197,
        "doc": {
            "question_id": 123122180,
            "question": "In a ____ protocol, a block can be changed from the Modified to Owned state in the original cache without writing it to memory.",
            "options": [
                "MOESI",
                "MSI",
                "MESI",
                "MESIF"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a ____ protocol, a block can be changed from the Modified to Owned state in the original cache without writing it to memory.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1198,
        "doc": {
            "question_id": 123122181,
            "question": "In a snooping cache coherence system without a bus, ensuring that a race between two processors attempting to write the same block has only one winner is accomplished by using ____ for all misses.",
            "options": [
                "unicast",
                "multicast",
                "anycast",
                "broadcast"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a snooping cache coherence system without a bus, ensuring that a race between two processors attempting to write the same block has only one winner is accomplished by using ____ for all misses.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **(B) multicast**. Here's why:\n\n* **Snooping Cache Coherence:** In this system, caches \"snoop\" on the communication channel (not necessarily a bus) to monitor memory accesses from other processors.  \n* **Race Condition (Write-Write):**  When two processors try to write to the same memory block simultaneously, a race condition occurs. Only one write should succeed to maintain data consistency.\n* **Multicast for Misses:** When a processor misses in its cache and needs to write, it sends a multicast message to all other caches. This message serves two purposes:\n    * **Invalidation:**  It invalidates any copies of that block in other caches, ensuring they don't hold stale data.\n    * **Arbitration:** The multicast mechanism inherently helps determine a single \"winner\" for the write operation. This could be based on factors like network proximity or a pre-defined priority scheme within the multicast protocol.\n\n**Other Options:**\n\n* **Unicast:** Point-to-point communication, not suitable for informing all caches about a potential write.\n* **Anycast:** Used to send data to any single member of a group; doesn't guarantee all caches are informed.\n* **Broadcast:**  While similar to multicast, it implies sending to *every* device on the network, which might be overkill in a targeted cache coherence scenario.\n\nLet me know if you'd like more detail on cache coherence protocols! \n"
            }
        ]
    },
    {
        "doc_id": 1199,
        "doc": {
            "question_id": 123122183,
            "question": "In shared-memory multiprocessors, ____ occurs when a cache block is invalidated due to a write to a different word in the same block.",
            "options": [
                "True sharing",
                "False sharing",
                "Cache thrashing",
                "Cache pollution"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In shared-memory multiprocessors, ____ occurs when a cache block is invalidated due to a write to a different word in the same block.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1200,
        "doc": {
            "question_id": 123122187,
            "question": "The ____ component of cache misses is typically higher for the operating system kernel compared to user processes in multiprocessor systems.",
            "options": [
                "capacity",
                "coherence",
                "conflict",
                "compulsory"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ component of cache misses is typically higher for the operating system kernel compared to user processes in multiprocessor systems.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1201,
        "doc": {
            "question_id": 123122189,
            "question": "In multiprocessor systems, as cache size increases, the ____ component of kernel data miss rate typically increases.",
            "options": [
                "compulsory",
                "capacity",
                "conflict",
                "coherence"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In multiprocessor systems, as cache size increases, the ____ component of kernel data miss rate typically increases.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1202,
        "doc": {
            "question_id": 123122191,
            "question": "In a directory-based coherence protocol, ____ is sent from a remote cache to the home directory to update the data value for a specific address.",
            "options": [
                "Data value reply",
                "Fetch",
                "Invalidate",
                "Data write-back"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a directory-based coherence protocol, ____ is sent from a remote cache to the home directory to update the data value for a specific address.",
                "Expected Label": "D",
                "Predicted Label": "D) Data write-back \n"
            }
        ]
    },
    {
        "doc_id": 1203,
        "doc": {
            "question_id": 123122192,
            "question": "In a directory-based cache coherence protocol, the ____ node is the one that has a copy of a cache block, whether exclusive or shared.",
            "options": [
                "remote",
                "local",
                "home",
                "central"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a directory-based cache coherence protocol, the ____ node is the one that has a copy of a cache block, whether exclusive or shared.",
                "Expected Label": "A",
                "Predicted Label": "Answer: C\n\n**Explanation:**\n\nIn directory-based cache coherence protocols, the **home node** is responsible for tracking which other nodes (if any) have cached a copy of a specific block of memory. \n\n* **Home Node:**  Acts as a central authority for a specific memory block, maintaining information about which nodes have a copy (shared or exclusive).\n* **Remote Node:** Any node other than the home node that might request or hold a copy of the data.\n* **Local Node:** Typically refers to the node currently making a memory request. It can be the home node for some data and a remote node for other data.\n* **Central Node:** This term is not generally used in the context of directory-based cache coherence.\n\nTherefore, the **home node** is the one that always knows which nodes possess a copy of the cache block, making \"C\" the correct answer. \n"
            }
        ]
    },
    {
        "doc_id": 1204,
        "doc": {
            "question_id": 123122194,
            "question": "In multiprocessors with fewer than 64 nodes, the set of nodes that have a copy of a block is typically kept as a ____.",
            "options": [
                "linked list",
                "hash table",
                "bit vector",
                "balanced tree"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In multiprocessors with fewer than 64 nodes, the set of nodes that have a copy of a block is typically kept as a ____.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) bit vector**. Here's why:\n\n* **Bit Vectors for Cache Coherence:** In systems with a relatively small number of nodes (like those under 64), a bit vector provides a simple and efficient way to track cache coherence. Each bit in the vector represents a specific node. If the bit is set (1), that node has a copy of the block. If it's clear (0), the node doesn't. This allows for quick lookups and updates.\n\nLet's look at why the other options are less suitable:\n\n* **A) Linked List:**  Linked lists introduce overhead for traversal, making them inefficient for checking which nodes have a block.\n* **B) Hash Table:** Hash tables are powerful for larger datasets, but the overhead for smaller node counts makes them less efficient than bit vectors.\n* **D) Balanced Tree:** Similar to hash tables, balanced trees are more complex than necessary for this scale and introduce unnecessary overhead.\n\n**In summary, a bit vector provides a compact and fast way to manage cache coherence information in multiprocessors with a limited number of nodes.** \n"
            }
        ]
    },
    {
        "doc_id": 1205,
        "doc": {
            "question_id": 123122197,
            "question": "The ____ instruction is used in conjunction with a load linked instruction to implement atomic operations.",
            "options": [
                "load locked",
                "store conditional",
                "test-and-set",
                "fetch-and-increment"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ instruction is used in conjunction with a load linked instruction to implement atomic operations.",
                "Expected Label": "B",
                "Predicted Label": "B) store conditional \n\n**Explanation:**\n\n* **Load Linked (LL)** and **Store Conditional (SC)** are a pair of instructions designed specifically for atomic operations. \n    * LL reads a value from memory and marks the cache line containing that address.\n    * SC attempts to store a value to the same address **only if** the cache line hasn't been modified by another core since the LL. If successful, it signals completion (usually by setting a register flag). If unsuccessful, it signals failure, indicating another core interfered.\n\nLet me know if you'd like more details on atomic operations or other computer architecture concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1206,
        "doc": {
            "question_id": 123122198,
            "question": "In a spin lock implementation, the processor _____ while waiting to acquire the lock.",
            "options": [
                "sleeps",
                "context switches",
                "loops continuously",
                "performs other tasks"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a spin lock implementation, the processor _____ while waiting to acquire the lock.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1207,
        "doc": {
            "question_id": 123122200,
            "question": "The ____ consistency model retains ordering among writes while relaxing the W\u2192R ordering.",
            "options": [
                "weak",
                "release",
                "partial store",
                "total store"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ consistency model retains ordering among writes while relaxing the W\u2192R ordering.",
                "Expected Label": "D",
                "Predicted Label": "C) partial store \n\nHere's why:\n\n* **Partial Store Ordering (PSO)** specifically focuses on relaxing the Write-to-Read (W\u2192R) ordering constraint. It maintains the order of writes (so a processor sees its own writes in order) but allows reads to be reordered even against writes to different addresses.\n\nLet's look at why the other options aren't the best fit:\n\n* **Weak Consistency:**  Provides very relaxed consistency with minimal guarantees. It doesn't inherently focus on relaxing W\u2192R ordering specifically.\n* **Release Consistency:**  Relies on synchronization operations (acquire/release) to define ordering constraints. It's not directly focused on relaxing W\u2192R ordering in a general sense.\n* **Total Store Ordering (TSO):**  A stronger model than PSO. It maintains global order for both writes and reads, making it stricter than what the question asks for. \n\n**In summary, Partial Store Ordering is the model explicitly designed to relax W\u2192R ordering while preserving write-to-write ordering.** \n"
            }
        ]
    },
    {
        "doc_id": 1208,
        "doc": {
            "question_id": 123122202,
            "question": "In a multi-level cache hierarchy, ____ is a technique used to ensure that data in lower-level caches is also present in higher-level caches.",
            "options": [
                "Snooping",
                "Invalidation",
                "Inclusion",
                "Directory"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a multi-level cache hierarchy, ____ is a technique used to ensure that data in lower-level caches is also present in higher-level caches.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1209,
        "doc": {
            "question_id": 123122203,
            "question": "____ is a primary limitation factor in determining the clock rate of multicore processors designed for server applications.",
            "options": [
                "Power",
                "Cache size",
                "Memory bandwidth",
                "Thread count"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a primary limitation factor in determining the clock rate of multicore processors designed for server applications.",
                "Expected Label": "A",
                "Predicted Label": "A) Power \n\n**Explanation:**\n\nWhile all the options can influence performance, **power consumption** is the most critical limiting factor for clock rates in server-grade multicore processors. Here's why:\n\n* **Heat Dissipation:** Higher clock rates lead to significantly increased power consumption, which translates into heat.  Servers, especially in data centers, need efficient cooling solutions. Excessive heat can damage components and limit clock speeds.\n* **Power Budgets:** Data centers have power limitations. Higher clock rates on many cores can push power consumption beyond acceptable levels.\n* **Energy Efficiency:**  Lower clock speeds with optimized architecture often provide better performance per watt, which is crucial for server economics.\n\n**Let's look at the other options:**\n\n* **Cache size:** While important, cache size primarily affects execution efficiency, not the maximum achievable clock rate.\n* **Memory bandwidth:** A bottleneck, but not the primary limitation on clock rate itself.\n* **Thread count:**  High thread counts utilize available resources but don't directly limit the clock rate of each core. \n"
            }
        ]
    },
    {
        "doc_id": 1210,
        "doc": {
            "question_id": 123122210,
            "question": "The successful exploitation of thread-level parallelism will depend as much on the development of ____ as it will on the contributions of computer architects.",
            "options": [
                "faster processors",
                "larger caches",
                "suitable software systems",
                "advanced cooling systems"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The successful exploitation of thread-level parallelism will depend as much on the development of ____ as it will on the contributions of computer architects.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile all the options have some relevance to performance,  **thread-level parallelism** hinges on the ability of **software** to effectively break down tasks into threads and manage them. Here's why:\n\n* **Faster processors, larger caches, and advanced cooling systems** improve overall execution speed, but they don't inherently guarantee better utilization of multiple threads.\n* **Suitable software systems** are crucial. They need to:\n    * Identify parallelizable portions of code.\n    * Create and manage threads efficiently.\n    * Handle synchronization and communication between threads.\n\nWithout software designed to exploit thread-level parallelism, even the most powerful hardware won't reach its full potential. \n"
            }
        ]
    },
    {
        "doc_id": 1211,
        "doc": {
            "question_id": 123122212,
            "question": "____ is typically used to manage coherency in multicore processors with on-chip interconnects.",
            "options": [
                "Coherency manager",
                "Memory controller",
                "Cache controller",
                "Bus arbiter"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to manage coherency in multicore processors with on-chip interconnects.",
                "Expected Label": "A",
                "Predicted Label": "C) Cache controller \n\n**Explanation:**\n\nWhile other components play roles in memory management, **cache controllers** are specifically designed to maintain coherency in multicore systems. They track which cores have copies of data and enforce protocols (like snooping or directory-based) to ensure all cores have a consistent view of memory. \n"
            }
        ]
    },
    {
        "doc_id": 1212,
        "doc": {
            "question_id": 123122213,
            "question": "In the MOSI cache coherence protocol, the ____ state behaves like Shared for reads but like Modified for supplying data on misses.",
            "options": [
                "Exclusive",
                "Owned",
                "Dirty",
                "Victimized"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the MOSI cache coherence protocol, the ____ state behaves like Shared for reads but like Modified for supplying data on misses.",
                "Expected Label": "B",
                "Predicted Label": "B) Owned \n"
            }
        ]
    },
    {
        "doc_id": 1213,
        "doc": {
            "question_id": 123122214,
            "question": "In a cache coherence protocol, the ____ state indicates that no other node has a copy of the block, but it has not yet been modified.",
            "options": [
                "Modified",
                "Shared",
                "Exclusive",
                "Invalid"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a cache coherence protocol, the ____ state indicates that no other node has a copy of the block, but it has not yet been modified.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) Exclusive**. Here's why:\n\n* **Exclusive:** This state means the cache block holds the latest data, and no other cache has a copy. Crucially, it also means the data hasn't been modified since being fetched from memory. This allows for optimization, as the cache can write back to memory without needing to check for coherence with other caches.\n\nLet's look at why the other options are incorrect:\n\n* **Modified:** This state indicates the cache block has been modified and holds the only valid copy of the data.\n* **Shared:** This state signifies that multiple caches may hold a copy of the block, all of which are consistent with the main memory. \n* **Invalid:** This state means the cache block doesn't hold valid data for the corresponding memory location. \n\n**In summary:** The \"Exclusive\" state is the only one that implies both a unique copy of the data and the fact that it hasn't been modified. \n"
            }
        ]
    },
    {
        "doc_id": 1214,
        "doc": {
            "question_id": 123122215,
            "question": "In the ____ consistency model, processors can implement write buffers that hold committed writes that have not yet been ordered with respect to other processors' writes.",
            "options": [
                "Sequential Consistency",
                "Processor Consistency",
                "Release Consistency",
                "Total Store Order"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In the ____ consistency model, processors can implement write buffers that hold committed writes that have not yet been ordered with respect to other processors' writes.",
                "Expected Label": "D",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1215,
        "doc": {
            "question_id": 123122220,
            "question": "____ is a synchronization primitive that allows atomic read-modify-write operations on memory.",
            "options": [
                "Spin lock",
                "Semaphore",
                "Load-linked/store-conditional",
                "Mutex"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a synchronization primitive that allows atomic read-modify-write operations on memory.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1216,
        "doc": {
            "question_id": 123122222,
            "question": "____ is a key consideration in the design and operation of warehouse-scale computers.",
            "options": [
                "Physical infrastructure",
                "Quantum computing",
                "Neuromorphic computing",
                "Optical computing"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key consideration in the design and operation of warehouse-scale computers.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nWhile the other options represent exciting areas of computing, **physical infrastructure** is the most critical aspect of warehouse-scale computers.  Here's why:\n\n* **Scale:** These systems involve massive data centers with thousands of servers. Managing power, cooling, networking, and physical space is paramount for efficient operation.\n* **Reliability:**  Downtime is extremely costly.  The physical infrastructure (redundant power, cooling systems, etc.) ensures continuous operation.\n* **Cost:**  Building and maintaining these facilities represents a significant investment. Optimizing the physical infrastructure directly impacts overall costs. \n\nWhile other options might play a role in the future of warehouse-scale computing, physical infrastructure remains the defining and most immediate concern. \n"
            }
        ]
    },
    {
        "doc_id": 1217,
        "doc": {
            "question_id": 123122224,
            "question": "The ________ is considered the modern descendant of the supercomputer.",
            "options": [
                "Personal computer",
                "Mainframe",
                "Warehouse-scale computer",
                "Cloud server"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ________ is considered the modern descendant of the supercomputer.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nWhile cloud servers and mainframes are powerful, warehouse-scale computers (WSCs) are specifically designed for massive data processing and complex computations, similar to the original purpose of supercomputers. They are essentially giant data centers optimized for high-performance computing. \n"
            }
        ]
    },
    {
        "doc_id": 1218,
        "doc": {
            "question_id": 123122225,
            "question": "In warehouse-scale computers, ____ is managed by software to achieve high availability.",
            "options": [
                "Hardware redundancy",
                "Power distribution",
                "Network latency",
                "Server redundancy"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In warehouse-scale computers, ____ is managed by software to achieve high availability.",
                "Expected Label": "D",
                "Predicted Label": "D) Server redundancy \n\n**Explanation:**\n\nWarehouse-scale computers rely heavily on software to manage vast numbers of servers.  A key aspect of this is handling **server redundancy**. This means having duplicate servers and systems in place so that if one fails, the software automatically redirects tasks and data to the functioning ones, ensuring continuous operation. \n\nLet's look at why the other options aren't the best fit:\n\n* **Hardware redundancy (A):** While hardware redundancy exists, it's not the primary element managed by software for high availability. The software leverages the redundant hardware.\n* **Power distribution (B):**  Power is crucial, but it's more of a physical infrastructure concern, not the core aspect managed by software for high availability.\n* **Network latency (C):**  Software tries to minimize network latency, but it's a performance factor, not the primary target of high availability management. \n"
            }
        ]
    },
    {
        "doc_id": 1219,
        "doc": {
            "question_id": 123122227,
            "question": "____ were the predecessors of modern warehouse-scale computers (WSCs).",
            "options": [
                "Mainframes",
                "Clusters",
                "Supercomputers",
                "Grid systems"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ were the predecessors of modern warehouse-scale computers (WSCs).",
                "Expected Label": "B",
                "Predicted Label": "C) Supercomputers \n"
            }
        ]
    },
    {
        "doc_id": 1220,
        "doc": {
            "question_id": 123122229,
            "question": "In a MapReduce program, the ____ function collects the output of distributed tasks and collapses them using a programmer-defined function.",
            "options": [
                "Map",
                "Emit",
                "Parse",
                "Reduce"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a MapReduce program, the ____ function collects the output of distributed tasks and collapses them using a programmer-defined function.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1221,
        "doc": {
            "question_id": 123122230,
            "question": "____ is a programming framework used for batch processing in warehouse-scale computers.",
            "options": [
                "MapReduce",
                "SIMD",
                "AVX",
                "GFS"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a programming framework used for batch processing in warehouse-scale computers.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1222,
        "doc": {
            "question_id": 123122234,
            "question": "In warehouse-scale computers, ____ is typically used to distribute datasets across multiple arrays when an application needs more than one array.",
            "options": [
                "Replication",
                "Caching",
                "Virtualization",
                "Sharding"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In warehouse-scale computers, ____ is typically used to distribute datasets across multiple arrays when an application needs more than one array.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1223,
        "doc": {
            "question_id": 123122237,
            "question": "In data center cooling systems, ____ are used to move warm air past cold water coils.",
            "options": [
                "Pumps",
                "Chillers",
                "Fans",
                "Compressors"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In data center cooling systems, ____ are used to move warm air past cold water coils.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1224,
        "doc": {
            "question_id": 123122241,
            "question": "In a typical warehouse-scale computer, the ____ represents the largest portion of the total facility cost.",
            "options": [
                "Server equipment",
                "Networking gear",
                "Property acquisition",
                "Power and cooling infrastructure"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a typical warehouse-scale computer, the ____ represents the largest portion of the total facility cost.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1225,
        "doc": {
            "question_id": 123122245,
            "question": "In cloud computing, ____ is often used to control resource usage and provide multiple pricing tiers.",
            "options": [
                "Hardware partitioning",
                "Time-sharing",
                "Process isolation",
                "Virtualization"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In cloud computing, ____ is often used to control resource usage and provide multiple pricing tiers.",
                "Expected Label": "D",
                "Predicted Label": "D) Virtualization \n\n**Explanation:**\n\nVirtualization is the core technology enabling cloud computing's flexibility and resource management. It allows for:\n\n* **Resource Pooling:**  Creating a large pool of physical resources (servers, storage, networking) that can be divided and allocated to multiple virtual machines.\n* **Isolation:**  Each virtual machine operates independently, preventing conflicts and security issues.\n* **Multiple Pricing Tiers:**  Cloud providers can offer different levels of performance and features by adjusting the allocated virtual resources, leading to various pricing models.\n\nLet me know if you have more questions about computer architecture or cloud computing! \n"
            }
        ]
    },
    {
        "doc_id": 1226,
        "doc": {
            "question_id": 123122246,
            "question": "In cloud computing, ____ typically offer the highest potential cost savings but may not guarantee continuous availability of resources.",
            "options": [
                "On-Demand Instances",
                "Spot Instances",
                "Reserved Instances",
                "Dedicated Hosts"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In cloud computing, ____ typically offer the highest potential cost savings but may not guarantee continuous availability of resources.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1227,
        "doc": {
            "question_id": 123122247,
            "question": "_____ is a key benefit of cloud computing that allows companies to scale their computing resources based on demand without additional costs.",
            "options": [
                "Risk avoidance",
                "Data security",
                "Cost associativity",
                "Energy efficiency"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ is a key benefit of cloud computing that allows companies to scale their computing resources based on demand without additional costs.",
                "Expected Label": "C",
                "Predicted Label": "**C) Cost associativity** \n\n**Explanation:**\n\nWhile cloud computing offers benefits like risk mitigation and potential energy efficiency, **cost associativity** is the *key* benefit described. Here's why:\n\n* **Cost associativity** means you only pay for the computing resources you use. This directly addresses the ability to \"scale their computing resources based on demand without additional costs.\" \n* **Risk avoidance** is a general benefit but not directly tied to scaling costs.\n* **Data security** can be enhanced but is not inherently guaranteed by cloud computing.\n* **Energy efficiency** is a potential benefit but not the defining one for cost-effective scaling. \n"
            }
        ]
    },
    {
        "doc_id": 1228,
        "doc": {
            "question_id": 123122248,
            "question": "The ____ in a warehouse-scale computer significantly impacts software design and data placement.",
            "options": [
                "CPU architecture",
                "Storage system",
                "Cooling infrastructure",
                "Network topology"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ in a warehouse-scale computer significantly impacts software design and data placement.",
                "Expected Label": "D",
                "Predicted Label": "D) Network topology \n\n**Explanation:**\n\nWhile all options are relevant to warehouse-scale computing, **network topology** has the most direct and significant impact on software design and data placement. Here's why:\n\n* **Data Locality is Crucial:** In massive data centers, transferring data is often the bottleneck, not processing it. Network topology dictates how fast and efficiently data can move between nodes. Software needs to be designed to minimize data movement across distant nodes, taking advantage of data locality offered by the topology.\n* **Latency and Bandwidth:** Different network topologies (e.g., tree, mesh, torus) have varying latency and bandwidth characteristics. These directly influence how data should be distributed and how parallel algorithms are designed to minimize communication overhead.\n* **Fault Tolerance:**  Network topology plays a vital role in redundancy and fault tolerance.  Software needs to be resilient to network failures, and the topology influences how data replication and task distribution happen to ensure continued operation.\n\n**Let's look at why the other options are less directly impactful:**\n\n* **CPU Architecture:** While important, CPU architecture influences instruction sets and processing power within a single node. It has less impact on the overall data placement and software design strategy at a warehouse scale.\n* **Storage System:** Similar to CPU architecture, storage system choices affect data access speeds within a node, but network topology dictates how data is distributed and accessed across the entire warehouse.\n* **Cooling Infrastructure:**  Crucial for operation but less directly tied to software design and data placement decisions compared to network topology. \n"
            }
        ]
    },
    {
        "doc_id": 1229,
        "doc": {
            "question_id": 123122249,
            "question": "______ is typically the most inefficient operating range for power supplies in warehouse-scale computers.",
            "options": [
                "25% load or less",
                "50% load",
                "75% load",
                "100% load"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is typically the most inefficient operating range for power supplies in warehouse-scale computers.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1230,
        "doc": {
            "question_id": 123122251,
            "question": "____ is a primary consideration when designing the airflow in a data center container.",
            "options": [
                "Thermal isolation",
                "Humidity control",
                "Noise reduction",
                "Dust filtration"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a primary consideration when designing the airflow in a data center container.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Thermal isolation** is the most critical factor in data center container airflow design.  Here's why:\n\n* **Heat Dissipation:** Data center equipment generates significant heat. Proper airflow is crucial to move this heat away from servers and other components, preventing overheating and ensuring optimal performance.\n* **Hot/Cold Aisles:** Effective airflow design often incorporates hot and cold aisle containment to maximize cooling efficiency.\n\nLet's consider the other options:\n\n* **Humidity Control:** While important, humidity is typically managed at the overall data center level rather than being a primary design focus for individual container airflow.\n* **Noise Reduction:**  Noise is a concern but less critical than thermal management for the operation of the equipment within the container.\n* **Dust Filtration:**  Dust filtration is important for equipment longevity but is usually handled by dedicated filtration systems at the intake points of the airflow system. \n\n**Therefore, thermal isolation (A) is the most critical consideration when designing airflow in a data center container.** \n"
            }
        ]
    },
    {
        "doc_id": 1231,
        "doc": {
            "question_id": 123122252,
            "question": "In modern warehouse-scale computers, the ____ can be implemented using standard batteries on each server shelf for improved efficiency.",
            "options": [
                "CPU",
                "UPS",
                "RAM",
                "SSD"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In modern warehouse-scale computers, the ____ can be implemented using standard batteries on each server shelf for improved efficiency.",
                "Expected Label": "B",
                "Predicted Label": "B\n \n\n**Explanation:**\n\n* **UPS (Uninterruptible Power Supply)** is the component responsible for providing temporary power during outages. In warehouse-scale computers, using batteries on each server shelf to implement a distributed UPS system is more efficient than a single, large, centralized UPS.\n\nLet's look at why the other options are incorrect:\n\n* **CPU (Central Processing Unit):** The CPU is the brain of the computer and requires constant power to function. Batteries are not a suitable replacement for the continuous power supply the CPU needs.\n* **RAM (Random Access Memory):** RAM is volatile memory, meaning it loses its data when power is lost. While some systems use batteries to maintain RAM data briefly during outages, it's not the primary application in warehouse-scale computers.\n* **SSD (Solid State Drive):** SSDs are storage devices and, while they benefit from power for data integrity during write operations, they don't typically rely on batteries as a primary power source. \n"
            }
        ]
    },
    {
        "doc_id": 1232,
        "doc": {
            "question_id": 123122253,
            "question": "In a warehouse-scale computer, ____ is typically the largest contributor to power inefficiency after IT equipment power.",
            "options": [
                "Power distribution",
                "Lighting",
                "Cooling",
                "Security systems"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a warehouse-scale computer, ____ is typically the largest contributor to power inefficiency after IT equipment power.",
                "Expected Label": "C",
                "Predicted Label": "C) Cooling \n\n**Explanation:**\n\nCooling is a major power consumer in warehouse-scale computers (data centers).  Here's why:\n\n* **Massive Heat Generation:** The sheer density of servers and networking equipment generates immense heat.\n* **Constant Cooling Required:**  To prevent overheating and ensure reliable operation, powerful cooling systems run continuously.\n* **Inefficiencies:** Traditional air cooling methods can be inefficient, especially at such a large scale. \n\nWhile other options consume power, cooling typically dwarfs them in a well-designed data center. \n"
            }
        ]
    },
    {
        "doc_id": 1233,
        "doc": {
            "question_id": 123122255,
            "question": "In warehouse-scale computers, ____ low power modes are generally more practical and effective than inactive low power modes for conserving energy.",
            "options": [
                "Partial",
                "Hybrid",
                "Active",
                "Intermittent"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In warehouse-scale computers, ____ low power modes are generally more practical and effective than inactive low power modes for conserving energy.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) Active**. Here's why:\n\n* **Active Low Power Modes:** These modes involve reducing the voltage and frequency of components while they are still actively processing tasks. This approach strikes a balance between performance and energy savings, which is crucial in warehouse-scale computing where workloads are often dynamic.\n\nLet's analyze why the other options are less suitable:\n\n* **Partial Low Power Modes:** While this might sound plausible, it lacks a clear definition in the context of computer architecture.  Partial power reduction is usually achieved *within* active power management techniques.\n* **Hybrid Low Power Modes:** This concept is too broad. Most modern power management strategies are inherently hybrid, combining different techniques. \n* **Intermittent Low Power Modes:** This refers to completely powering down components when idle. While effective for saving energy, it introduces latency when bringing components back online \u2013 a significant drawback in warehouse-scale environments that demand continuous operation.\n\n**In summary, active low power modes provide the most practical and effective means of energy conservation in warehouse-scale computers because they allow for dynamic adaptation to workload demands without significant performance degradation.** \n"
            }
        ]
    },
    {
        "doc_id": 1234,
        "doc": {
            "question_id": 123122260,
            "question": "In warehouse-scale computers, ____ is often used to compare different server options that have multiple dimensions.",
            "options": [
                "CAPEX",
                "OPEX",
                "Performance per watt",
                "Total Cost of Ownership"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Benchmarking and Measurement"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In warehouse-scale computers, ____ is often used to compare different server options that have multiple dimensions.",
                "Expected Label": "D",
                "Predicted Label": "D) Total Cost of Ownership \n\n**Explanation:**\n\nWhile all the options are relevant factors in evaluating server options, **Total Cost of Ownership (TCO)** offers the most comprehensive approach for warehouse-scale computers. Here's why:\n\n* **TCO encompasses CAPEX and OPEX:** It includes both the initial capital expenditure (CAPEX) for purchasing servers and the ongoing operational expenses (OPEX) like power consumption, cooling, and maintenance.\n* **TCO considers performance:** It factors in performance aspects to ensure the chosen servers meet the required workload demands.\n* **TCO accounts for the lifespan:**  It evaluates costs over the entire lifespan of the servers, including potential upgrades or replacements.\n\nIn a warehouse-scale environment with thousands of servers, a holistic view provided by TCO is crucial for making informed decisions and optimizing the overall cost-effectiveness of the infrastructure. \n"
            }
        ]
    },
    {
        "doc_id": 1235,
        "doc": {
            "question_id": 123122261,
            "question": "____ is a key factor in determining the total cost of ownership (TCO) for servers in a warehouse-scale computer.",
            "options": [
                "Power consumption",
                "CPU clock speed",
                "Cache size",
                "Number of cores"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key factor in determining the total cost of ownership (TCO) for servers in a warehouse-scale computer.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1236,
        "doc": {
            "question_id": 123122262,
            "question": "____ is a key factor in enabling warehouse-scale computing.",
            "options": [
                "Instruction-level parallelism",
                "Request-level parallelism",
                "Thread-level parallelism",
                "Data-level parallelism"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a key factor in enabling warehouse-scale computing.",
                "Expected Label": "B",
                "Predicted Label": "B) Request-level parallelism \n\n**Explanation:**\n\nWarehouse-scale computing deals with massive data centers handling enormous numbers of user requests. **Request-level parallelism** is the key here because it allows the system to process many independent user requests (e.g., search queries, website accesses) concurrently.  \n\nHere's why the other options are less fitting:\n\n* **Instruction-level parallelism (ILP):**  Focuses on executing multiple instructions within a single program simultaneously. While important for general CPU performance, it's less impactful at the scale of thousands of servers.\n* **Thread-level parallelism (TLP):** Involves parallel execution of different threads within a single process. This is useful for multithreaded applications but doesn't directly address the scale of user requests in a data center.\n* **Data-level parallelism (DLP):**  Focuses on performing the same operation on multiple data elements simultaneously. It's important for data-intensive tasks but doesn't define the core ability to handle massive, independent user requests. \n"
            }
        ]
    },
    {
        "doc_id": 1237,
        "doc": {
            "question_id": 123122267,
            "question": "____ is a technique used in blade server enclosures to dynamically match power supply units to actual server power consumption.",
            "options": [
                "Shared PSU pooling",
                "2N redundancy",
                "Power stranding",
                "Load balancing"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in blade server enclosures to dynamically match power supply units to actual server power consumption.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1238,
        "doc": {
            "question_id": 123122268,
            "question": "The ____ of a CRAC unit is defined as the ratio of heat removed to the amount of work necessary to remove that heat.",
            "options": [
                "Cooling Factor",
                "Coefficient of Performance",
                "Energy Efficiency Ratio",
                "Thermal Effectiveness"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ of a CRAC unit is defined as the ratio of heat removed to the amount of work necessary to remove that heat.",
                "Expected Label": "B",
                "Predicted Label": "B) Coefficient of Performance \n\n**Explanation:**\n\nThe **Coefficient of Performance (COP)** is the key metric for evaluating the efficiency of cooling systems like CRAC (Computer Room Air Conditioning) units. It directly measures how much heat energy is removed (cooling capacity) versus the amount of electrical energy consumed to achieve that cooling. \n"
            }
        ]
    },
    {
        "doc_id": 1239,
        "doc": {
            "question_id": 123122270,
            "question": "The power consumption of a server cooling fan is a ____ function of its speed.",
            "options": [
                "linear",
                "cubic",
                "quadratic",
                "logarithmic"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The power consumption of a server cooling fan is a ____ function of its speed.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1240,
        "doc": {
            "question_id": 123122272,
            "question": "The ____ is a key component in many early computer architectures for performing arithmetic and logical operations.",
            "options": [
                "Accumulator",
                "Cache",
                "Pipeline",
                "Register file"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "The ____ is a key component in many early computer architectures for performing arithmetic and logical operations.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1241,
        "doc": {
            "question_id": 123122273,
            "question": "____ is typically emphasized in desktop computing applications.",
            "options": [
                "Program size",
                "Integer and floating-point performance",
                "Energy efficiency",
                "Character string processing"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically emphasized in desktop computing applications.",
                "Expected Label": "B",
                "Predicted Label": "B) Integer and floating-point performance \n"
            }
        ]
    },
    {
        "doc_id": 1242,
        "doc": {
            "question_id": 123122274,
            "question": "The ____ architecture keeps all operands in memory.",
            "options": [
                "register-memory",
                "load-store",
                "memory-memory",
                "extended accumulator"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ architecture keeps all operands in memory.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1243,
        "doc": {
            "question_id": 123122276,
            "question": "In register-register architecture, ____ is typically fixed-length.",
            "options": [
                "Memory access",
                "Instruction encoding",
                "Cache size",
                "Pipeline depth"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In register-register architecture, ____ is typically fixed-length.",
                "Expected Label": "B",
                "Predicted Label": "B) Instruction encoding \n\n**Explanation:**\n\nRegister-register architectures emphasize operations primarily on data held in registers.  To make these operations efficient and predictable, instruction encoding (how the instruction is represented in binary) is typically fixed-length. This allows for easier decoding and execution.\n\n**Why other options are less likely:**\n\n* **A) Memory access:**  Memory access time can vary due to caching and virtual memory translations.\n* **C) Cache size:** Cache size is a design decision and can vary significantly.\n* **D) Pipeline depth:**  Pipeline depth is influenced by factors like instruction complexity and can vary between architectures. \n"
            }
        ]
    },
    {
        "doc_id": 1244,
        "doc": {
            "question_id": 123122277,
            "question": "In ____ byte order, the byte with address 'x...x000' is placed at the most-significant position in a double word.",
            "options": [
                "Little Endian",
                "Mixed",
                "Big Endian",
                "Reversed"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In ____ byte order, the byte with address 'x...x000' is placed at the most-significant position in a double word.",
                "Expected Label": "C",
                "Predicted Label": "C) Big Endian \n\n**Explanation:**\n\n* **Big Endian:** The most significant byte (MSB) is stored at the lowest memory address. This means the byte at address 'x...x000' would be the most significant in a double word.\n* **Little Endian:** The least significant byte (LSB) is stored at the lowest memory address.\n* **Mixed and Reversed** are not standard byte ordering schemes. \n"
            }
        ]
    },
    {
        "doc_id": 1245,
        "doc": {
            "question_id": 123122278,
            "question": "____ addressing mode is particularly useful for stepping through arrays within a loop.",
            "options": [
                "Autoincrement",
                "Displacement",
                "Register indirect",
                "Scaled"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ addressing mode is particularly useful for stepping through arrays within a loop.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 1246,
        "doc": {
            "question_id": 123122283,
            "question": "The ____ specifies the basic rules for which registers should be caller saved and which should be callee saved.",
            "options": [
                "Compiler",
                "Operating System",
                "Instruction Set Architecture",
                "Application Binary Interface"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ specifies the basic rules for which registers should be caller saved and which should be callee saved.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nThe **Application Binary Interface (ABI)** acts as the agreement between different software components, like an application and the operating system or between different parts of an application.  A crucial aspect of the ABI is defining how function calls work, including which registers are responsible for preserving values across function calls (callee-saved) and which are expected to be saved by the caller if needed (caller-saved). \n\nLet's look at why the other options are less suitable:\n\n* **Compiler (A):** The compiler works within the constraints of the ABI. It can optimize register usage but must ultimately adhere to the ABI's calling conventions.\n* **Operating System (B):** The OS provides a platform and services but usually doesn't dictate the fine-grained details of register saving conventions within programs.\n* **Instruction Set Architecture (ISA) (C):** The ISA defines the processor's instructions and registers but typically doesn't specify high-level conventions like caller/callee saved registers. \n"
            }
        ]
    },
    {
        "doc_id": 1247,
        "doc": {
            "question_id": 123122284,
            "question": "____ addressing is commonly used to support procedure returns in instruction set architectures.",
            "options": [
                "Register indirect",
                "Immediate",
                "Direct",
                "Indexed"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ addressing is commonly used to support procedure returns in instruction set architectures.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1248,
        "doc": {
            "question_id": 123122287,
            "question": "____ is used to allocate local variables in high-level languages.",
            "options": [
                "The heap",
                "The global data area",
                "The stack",
                "The register file"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is used to allocate local variables in high-level languages.",
                "Expected Label": "C",
                "Predicted Label": "Answer: **C** \n\n**Explanation:**\n\n* **The stack** is the primary data structure used for allocating space for local variables within functions. When a function is called, a new stack frame is created to hold its local variables. When the function returns, the stack frame is popped, effectively deallocating the memory.\n\nLet's break down why the other options are incorrect:\n\n* **The heap** is used for dynamic memory allocation, where the program requests memory as needed during runtime (using commands like `malloc` in C or `new` in C++).\n* **The global data area** stores global variables, which are accessible from any part of the program throughout its execution.\n* **The register file** holds a small number of frequently used data values for very fast access by the CPU. It's not typically used for general-purpose local variable storage. \n"
            }
        ]
    },
    {
        "doc_id": 1249,
        "doc": {
            "question_id": 123122288,
            "question": "____ is an optimization technique that simplifies or eliminates array addressing calculations within loops.",
            "options": [
                "Code motion",
                "Constant propagation",
                "Strength reduction",
                "Induction variable elimination"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is an optimization technique that simplifies or eliminates array addressing calculations within loops.",
                "Expected Label": "D",
                "Predicted Label": "D) Induction variable elimination \n\n**Explanation:**\n\nWhile some of the other options play a role in optimizing code, **Induction Variable Elimination** directly targets the simplification or elimination of array addressing calculations in loops. Here's why:\n\n* **Induction Variable Elimination:**  This technique analyzes loop structures and identifies variables (induction variables) that change predictably within the loop. Often, these variables are used to calculate array indices. By simplifying or eliminating these variables and their associated calculations, the compiler can optimize array access, leading to performance gains.\n\nLet's look at the other options:\n\n* **Code Motion:** This technique moves code blocks outside of loops if the calculation's result remains the same within each iteration. While helpful for optimization, it doesn't specifically target array addressing.\n* **Constant Propagation:** This optimization replaces variables with known constant values. It can simplify some calculations but doesn't directly address the complexity of array addressing within loops. \n* **Strength Reduction:** This technique replaces expensive operations with computationally cheaper ones (e.g., replacing multiplication with addition in certain loop scenarios). While it can optimize some array-related calculations, it's not as directly focused as induction variable elimination. \n"
            }
        ]
    },
    {
        "doc_id": 1250,
        "doc": {
            "question_id": 123122292,
            "question": "In instruction set design, it is generally better to provide ____ instead of solutions.",
            "options": [
                "complexities",
                "alternatives",
                "constants",
                "primitives"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In instruction set design, it is generally better to provide ____ instead of solutions.",
                "Expected Label": "D",
                "Predicted Label": "D) primitives \n\n**Explanation:**\n\nProviding **primitives** in instruction set design allows for greater flexibility and efficiency. \n\n* **Primitives** are basic, fundamental instructions that can be combined in various ways to create more complex operations. This empowers programmers to tailor solutions to specific needs and optimize for performance. \n* **Complexities** would make the instruction set harder to use and understand.\n* **Alternatives** might lead to redundancy and bloat in the design.\n* **Constants** are too specific and limit flexibility. \n"
            }
        ]
    },
    {
        "doc_id": 1251,
        "doc": {
            "question_id": 123122296,
            "question": "In MIPS architecture, ____ instructions are used to convert a simple branch into a conditional arithmetic instruction.",
            "options": [
                "Jump",
                "Conditional move",
                "Branch",
                "Link"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, ____ instructions are used to convert a simple branch into a conditional arithmetic instruction.",
                "Expected Label": "B",
                "Predicted Label": "B) Conditional move \n\n**Explanation:**\n\nConditional move instructions in MIPS allow you to perform arithmetic operations *only if* a specific condition is met. This effectively combines a branch and an arithmetic instruction into a single operation, improving efficiency. \n\nHere's why the other options are incorrect:\n\n* **A) Jump:** Jump instructions unconditionally transfer control to a different part of the program. They don't involve arithmetic operations.\n* **C) Branch:** Branch instructions are conditional jumps. They decide whether to jump based on a condition but don't perform arithmetic.\n* **D) Link:** Link instructions are used for function calls, saving the return address in a register. They are not directly related to conditional arithmetic. \n"
            }
        ]
    },
    {
        "doc_id": 1252,
        "doc": {
            "question_id": 123122297,
            "question": "The ____ instruction set includes both integer and floating-point multiply-add instructions to support multimedia applications.",
            "options": [
                "x86",
                "ARM",
                "MIPS64",
                "RISC-V"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ instruction set includes both integer and floating-point multiply-add instructions to support multimedia applications.",
                "Expected Label": "C",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1253,
        "doc": {
            "question_id": 123122298,
            "question": "In MIPS64, the ____ instruction is used to copy data between general-purpose registers and special registers.",
            "options": [
                "MOV",
                "LW",
                "JAL",
                "MTC0"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In MIPS64, the ____ instruction is used to copy data between general-purpose registers and special registers.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1254,
        "doc": {
            "question_id": 123122299,
            "question": "The ________ of a program can significantly impact instruction set usage patterns.",
            "options": [
                "execution time",
                "memory footprint",
                "application domain",
                "compiler version"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ________ of a program can significantly impact instruction set usage patterns.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Application domain** heavily influences what a program does and, consequently, the instructions it uses. For example:\n    * Scientific applications rely heavily on floating-point arithmetic.\n    * Database programs use a lot of load/store and comparison instructions.\n    * Multimedia applications utilize specialized multimedia instructions if available. \n\nLet's look at why the other options aren't the best fit:\n\n* **Execution time (A):** While execution time is influenced by instruction usage, it's a result of the program's behavior and the architecture itself, not a direct influencer of instruction set usage patterns.\n* **Memory Footprint (B):** Memory footprint is more related to data structures and program size, not directly to instruction usage patterns. \n* **Compiler version (D):** Different compiler versions might optimize code differently, potentially affecting instruction choices, but the core instruction usage patterns are dictated by the application's purpose (its domain). \n"
            }
        ]
    },
    {
        "doc_id": 1255,
        "doc": {
            "question_id": 123122300,
            "question": "The _____ uses a stack for floating-point data, unlike other processor architectures.",
            "options": [
                "MIPS",
                "ARM",
                "SPARC",
                "80x86"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The _____ uses a stack for floating-point data, unlike other processor architectures.",
                "Expected Label": "D",
                "Predicted Label": "D) 80x86 \n"
            }
        ]
    },
    {
        "doc_id": 1256,
        "doc": {
            "question_id": 123122302,
            "question": "Floating point ____ operations typically have the highest CPI among basic arithmetic operations.",
            "options": [
                "addition",
                "subtraction",
                "division",
                "multiplication"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "Floating point ____ operations typically have the highest CPI among basic arithmetic operations.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1257,
        "doc": {
            "question_id": 123122305,
            "question": "In a load-store architecture, ____ are typically used to hold operands for arithmetic and logical operations.",
            "options": [
                "Cache lines",
                "Memory banks",
                "Instruction pipelines",
                "General-purpose registers"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a load-store architecture, ____ are typically used to hold operands for arithmetic and logical operations.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1258,
        "doc": {
            "question_id": 123122306,
            "question": "____ addressing is commonly used for branch instructions in RISC architectures.",
            "options": [
                "Absolute",
                "PC-relative",
                "Indirect",
                "Register-indexed"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ addressing is commonly used for branch instructions in RISC architectures.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **PC-relative addressing** is heavily favored in RISC architectures for branch instructions. It calculates the target address by adding a signed offset embedded within the instruction to the current value of the Program Counter (PC). This approach offers several advantages for RISC:\n\n    * **Code Position Independence:** The code can be loaded at different memory locations without needing to modify the branch instructions.\n    * **Smaller Instruction Size:**  PC-relative offsets can often be encoded using fewer bits than full absolute addresses, leading to more compact instructions.\n\nLet's look at why the other options aren't as suitable:\n\n* **Absolute Addressing:**  Specifies the full memory address of the target instruction. This is less efficient in RISC due to larger instruction sizes and issues with code relocation.\n* **Indirect Addressing:** Uses a register or memory location to hold the address of the actual operand. While flexible, it adds complexity and is not the primary method for branch instructions in RISC.\n* **Register-Indexed Addressing:** Calculates the operand address by adding the contents of a register to a constant offset. It's more common for data manipulation than branch instructions. \n"
            }
        ]
    },
    {
        "doc_id": 1259,
        "doc": {
            "question_id": 123122308,
            "question": "____ is the name given to the highest or first level of the memory hierarchy encountered once the address leaves the processor.",
            "options": [
                "Cache",
                "Main memory",
                "Register file",
                "Virtual memory"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the name given to the highest or first level of the memory hierarchy encountered once the address leaves the processor.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Cache** is the fastest memory level directly connected to the processor. It stores frequently used data and instructions to speed up access times. \n\nLet's analyze the other options:\n\n* **Main memory (RAM)** is slower than cache and serves as the primary storage for running programs and data. It's accessed after the cache.\n* **Register file** exists within the CPU itself and is even faster than cache, but it's not considered part of the memory hierarchy in the same way as cache, RAM, and secondary storage.\n* **Virtual memory** is a memory management technique that uses disk space to extend the apparent size of RAM. It's not a distinct physical memory level. \n"
            }
        ]
    },
    {
        "doc_id": 1260,
        "doc": {
            "question_id": 123122312,
            "question": "______ is used to determine which set a block is mapped to in a set associative cache.",
            "options": [
                "Bit selection",
                "Tag comparison",
                "Full address mapping",
                "Block offset"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is used to determine which set a block is mapped to in a set associative cache.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1261,
        "doc": {
            "question_id": 123122316,
            "question": "____ is a technique used to reduce the frequency of writing back blocks on replacement in write-back caches.",
            "options": [
                "Dirty bit",
                "Write buffer",
                "Write allocate",
                "Write stall"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used to reduce the frequency of writing back blocks on replacement in write-back caches.",
                "Expected Label": "A",
                "Predicted Label": "**B) Write buffer** \n\n**Explanation:**\n\n* **Write buffers** act as a temporary storage area for data that needs to be written back to the main memory. Instead of writing back an entire block immediately upon modification, the modified data is placed in the write buffer.  This allows the processor to continue with other tasks. The buffer's contents are written to memory later, often when the buffer becomes full or during idle processor cycles. \n\nLet's examine why the other options are not the best answer:\n\n* **A) Dirty bit:**  The dirty bit indicates *if* a block has been modified, but it doesn't directly reduce the frequency of write-backs.\n* **C) Write allocate:** Write allocate is a policy related to how data is handled on a cache miss, not specifically about reducing write-backs.\n* **D) Write stall:** Write stalls are delays caused by waiting for write operations to complete, the opposite of reducing write-back frequency. \n"
            }
        ]
    },
    {
        "doc_id": 1262,
        "doc": {
            "question_id": 123122317,
            "question": "In a write-back cache, ____ is typically used on a write miss.",
            "options": [
                "no-write allocate",
                "write allocate",
                "write-through",
                "write-around"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a write-back cache, ____ is typically used on a write miss.",
                "Expected Label": "B",
                "Predicted Label": "B. \n"
            }
        ]
    },
    {
        "doc_id": 1263,
        "doc": {
            "question_id": 123122318,
            "question": "In a set associative cache, the ____ selects the tag to be tested to determine if the desired block is in the cache.",
            "options": [
                "Block offset",
                "Address tag",
                "Cache index",
                "Valid bit"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a set associative cache, the ____ selects the tag to be tested to determine if the desired block is in the cache.",
                "Expected Label": "C",
                "Predicted Label": "C) Cache index \n\n**Explanation:**\n\nHere's why:\n\n* **Cache Index:** The cache index bits within the memory address determine which *set* the data might be located in.  Each set in a set-associative cache can hold multiple blocks. \n* **Address Tag:** Once the set is determined, the address tag is compared against the tags of all blocks within that set to see if there's a match, indicating a cache hit.\n* **Block Offset:** The block offset is used to locate the specific data word within the retrieved cache block.\n* **Valid Bit:** The valid bit indicates whether the data in a specific cache block is valid or not. It's not used for selecting the tag to be tested. \n"
            }
        ]
    },
    {
        "doc_id": 1264,
        "doc": {
            "question_id": 123122319,
            "question": "Average memory access time is calculated as Hit time + Miss rate \u00d7 ____.",
            "options": [
                "Cache size",
                "Miss penalty",
                "Hit rate",
                "Clock cycles"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "Average memory access time is calculated as Hit time + Miss rate \u00d7 ____.",
                "Expected Label": "B",
                "Predicted Label": "B) Miss penalty \n"
            }
        ]
    },
    {
        "doc_id": 1265,
        "doc": {
            "question_id": 123122320,
            "question": "In an in-order execution processor, the memory stall time is strongly correlated to ____.",
            "options": [
                "cache size",
                "instruction count",
                "average memory access time",
                "branch prediction accuracy"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In an in-order execution processor, the memory stall time is strongly correlated to ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Memory stall time** refers to the delay a processor experiences when it needs to access data from memory, but the data is not readily available in the cache.  \n\n* **Average memory access time** directly impacts stall time.  Slower memory (higher access time) means longer delays when cache misses occur.\n\n**Why other options are less relevant:**\n\n* **A) Cache size:** While a larger cache can *reduce* the frequency of cache misses, it doesn't directly determine the duration of the stall *when* a miss occurs.\n* **B) Instruction count:** The number of instructions doesn't directly correlate to memory stall time.  Stalls are about memory access, not instruction processing itself.\n* **D) Branch prediction accuracy:** Branch prediction affects the pipeline's efficiency, but its impact on memory stall time is indirect and less significant compared to the average memory access time. \n"
            }
        ]
    },
    {
        "doc_id": 1266,
        "doc": {
            "question_id": 123122321,
            "question": "The impact of cache misses on processor performance is ____ for processors with lower CPI execution.",
            "options": [
                "decreased",
                "unchanged",
                "variable",
                "increased"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The impact of cache misses on processor performance is ____ for processors with lower CPI execution.",
                "Expected Label": "D",
                "Predicted Label": "D) increased \n\n**Explanation:**\n\nCPI (Cycles Per Instruction) represents the average number of clock cycles a processor takes to execute a single instruction. A lower CPI indicates faster instruction execution. \n\nCache misses stall the processor as it waits for data to be fetched from the main memory. This stall introduces idle cycles, significantly impacting performance. When the CPI is already low (meaning instructions execute quickly), the relative impact of these idle cycles due to cache misses becomes more pronounced. \n\nTherefore, the impact of cache misses on processor performance is **increased** for processors with lower CPI execution. \n"
            }
        ]
    },
    {
        "doc_id": 1267,
        "doc": {
            "question_id": 123122322,
            "question": "In out-of-order execution processors, memory stalls are defined as the ____ latency of a miss.",
            "options": [
                "total",
                "nonoverlapped",
                "overlapped",
                "exposed"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In out-of-order execution processors, memory stalls are defined as the ____ latency of a miss.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **D) exposed**. Here's why:\n\n* **Exposed Latency:** In out-of-order processors, many instructions are processed concurrently. A memory stall's *exposed* latency refers to the time the processor remains stalled *despite* out-of-order execution.  It's the delay that couldn't be hidden by executing other instructions.\n\nLet's clarify why the other options are less accurate:\n\n* **Total Latency:** This refers to the entire duration of a cache miss, including the time spent handling it. Out-of-order execution aims to hide a portion of this total latency.\n* **Nonoverlapped Latency:** This implies a portion of the latency that cannot be hidden, which is the core idea of exposed latency, making \"exposed\" a more precise term.\n* **Overlapped Latency:**  This refers to the portion of the latency that *can* be hidden by doing other work while waiting, which is the opposite of what the question asks for. \n\n**In essence, exposed latency is the portion of memory stall delay that out-of-order execution cannot compensate for, directly impacting performance.** \n"
            }
        ]
    },
    {
        "doc_id": 1268,
        "doc": {
            "question_id": 123122325,
            "question": "____ misses are unavoidable and occur on the first access to a memory block.",
            "options": [
                "Compulsory",
                "Capacity",
                "Conflict",
                "Coherency"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ misses are unavoidable and occur on the first access to a memory block.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1269,
        "doc": {
            "question_id": 123122327,
            "question": "When a memory hierarchy is _____, the computer runs close to the speed of the lower-level memory, or maybe even slower due to miss overhead.",
            "options": [
                "optimized",
                "balanced",
                "thrashing",
                "caching"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "When a memory hierarchy is _____, the computer runs close to the speed of the lower-level memory, or maybe even slower due to miss overhead.",
                "Expected Label": "C",
                "Predicted Label": "C) thrashing \n\n**Explanation:**\n\nThrashing occurs when a memory hierarchy spends excessive time transferring data between levels (like constantly swapping data between main memory and a much slower secondary storage) instead of actually processing it. This leads to a significant performance degradation, often making the system slower than the slowest memory level due to the overhead of these transfers. \n\nLet's look at why the other options are incorrect:\n\n* **A) optimized:** An optimized memory hierarchy aims for the opposite effect \u2013 minimizing data transfers and running close to the speed of the fastest memory level.\n* **B) balanced:** A balanced hierarchy aims for cost-effectiveness by matching memory speeds and sizes to the program's needs. While not as fast as a perfectly optimized system, it doesn't inherently imply the severe performance degradation of thrashing.\n* **D) caching:** Caching is a technique used *within* a memory hierarchy to improve performance. While cache misses exist, they are a normal part of operation and don't automatically lead to the extreme slowdowns seen in thrashing. \n"
            }
        ]
    },
    {
        "doc_id": 1270,
        "doc": {
            "question_id": 123122329,
            "question": "The ____ rule of thumb states that a direct-mapped cache of size N has about the same miss rate as a two-way set associative cache of size N/2.",
            "options": [
                "1:1 cache",
                "3:1 cache",
                "2:1 cache",
                "4:1 cache"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ rule of thumb states that a direct-mapped cache of size N has about the same miss rate as a two-way set associative cache of size N/2.",
                "Expected Label": "C",
                "Predicted Label": "C) 2:1 cache \n"
            }
        ]
    },
    {
        "doc_id": 1271,
        "doc": {
            "question_id": 123122333,
            "question": "The primary benefit of ____ is that cache consistency can be determined by checking only the second-level cache.",
            "options": [
                "Write-through caching",
                "Write-back caching",
                "Cache coherence protocols",
                "Multilevel inclusion"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary benefit of ____ is that cache consistency can be determined by checking only the second-level cache.",
                "Expected Label": "D",
                "Predicted Label": "D) Multilevel inclusion \n"
            }
        ]
    },
    {
        "doc_id": 1272,
        "doc": {
            "question_id": 123122335,
            "question": "In most modern desktop and server processors, ____ are given priority over writes when accessing memory.",
            "options": [
                "Writes",
                "Reads",
                "Flushes",
                "Prefetches"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In most modern desktop and server processors, ____ are given priority over writes when accessing memory.",
                "Expected Label": "B",
                "Predicted Label": "B) Reads \n"
            }
        ]
    },
    {
        "doc_id": 1273,
        "doc": {
            "question_id": 123122336,
            "question": "A ____ can be used to reduce the need for cache flushing during process switches in virtually addressed caches.",
            "options": [
                "translation lookaside buffer",
                "page table",
                "process-identifier tag",
                "protection bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ can be used to reduce the need for cache flushing during process switches in virtually addressed caches.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1274,
        "doc": {
            "question_id": 123122337,
            "question": "The ____ technique allows cache read to begin immediately while still performing tag comparison with physical addresses.",
            "options": [
                "Fully associative mapping",
                "Direct-mapped caching",
                "Page coloring",
                "Virtually indexed, physically tagged"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ technique allows cache read to begin immediately while still performing tag comparison with physical addresses.",
                "Expected Label": "D",
                "Predicted Label": "D) Virtually indexed, physically tagged \n"
            }
        ]
    },
    {
        "doc_id": 1275,
        "doc": {
            "question_id": 123122339,
            "question": "____ is typically used as the lower-level backing store for main memory in the hierarchy.",
            "options": [
                "Secondary storage",
                "Primary cache",
                "Register file",
                "L2 cache"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used as the lower-level backing store for main memory in the hierarchy.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Secondary storage** (like hard drives or SSDs) is slower but larger than main memory (RAM). It's used to hold data and programs not actively being used, providing a place for the operating system to swap data in and out of main memory.\n\nLet's analyze the other options:\n\n* **Primary cache** and **L2 cache** are faster memory levels closer to the CPU, used to speed up access to frequently used data from main memory.\n* **Register file** is the fastest memory level, residing within the CPU itself and holding data immediately used by the processor. \n\nTherefore, **secondary storage (A)** is the most fitting answer as the lower-level backing store for main memory. \n"
            }
        ]
    },
    {
        "doc_id": 1276,
        "doc": {
            "question_id": 123122340,
            "question": "In virtual memory systems, ____ allows blocks to be placed anywhere in main memory.",
            "options": [
                "Paging",
                "Full associativity",
                "Direct mapping",
                "Set associativity"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In virtual memory systems, ____ allows blocks to be placed anywhere in main memory.",
                "Expected Label": "B",
                "Predicted Label": "B) Full associativity \n"
            }
        ]
    },
    {
        "doc_id": 1277,
        "doc": {
            "question_id": 123122341,
            "question": "In virtual memory systems, the write strategy for managing updates to main memory and disk is typically ____.",
            "options": [
                "write-through",
                "write-allocate",
                "write-back",
                "write-around"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In virtual memory systems, the write strategy for managing updates to main memory and disk is typically ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Write-back (C):** This is the most common write strategy for virtual memory. Updates are made to the page in main memory (RAM) and marked as \"dirty.\" The dirty page is only written back to disk when it's evicted from memory. This strategy minimizes disk writes and improves performance.\n\nLet's look at why the other options are incorrect:\n\n* **Write-through (A):**  Every write to memory also updates the disk immediately. This ensures data consistency but significantly slows down write operations.\n* **Write-allocate (B):**  This strategy is used in conjunction with write-back and dictates that when a write miss occurs, the page is first loaded into memory before the write operation is performed.\n* **Write-around (D):** Writes directly update the secondary storage (disk) bypassing the main memory. While this avoids cache inconsistencies, it leads to poor performance for frequently updated data. \n"
            }
        ]
    },
    {
        "doc_id": 1278,
        "doc": {
            "question_id": 123122343,
            "question": "____ refers to the unused memory within a page when a contiguous region of virtual memory is not equal in size to a multiple of the page size.",
            "options": [
                "Internal fragmentation",
                "External fragmentation",
                "Memory wastage",
                "Page overhead"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ refers to the unused memory within a page when a contiguous region of virtual memory is not equal in size to a multiple of the page size.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1279,
        "doc": {
            "question_id": 123122345,
            "question": "In computer architecture, ____ is a protection structure that expands memory access protection from two levels to multiple concentric security levels.",
            "options": [
                "Paging",
                "Segmentation",
                "Virtual memory",
                "Rings"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ is a protection structure that expands memory access protection from two levels to multiple concentric security levels.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1280,
        "doc": {
            "question_id": 123122347,
            "question": "In the IA-32 architecture, the ____ contains fields similar to those found in page table entries.",
            "options": [
                "global descriptor table",
                "segment descriptor",
                "local descriptor table",
                "attribute field"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the IA-32 architecture, the ____ contains fields similar to those found in page table entries.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) segment descriptor**. Here's why:\n\n* **Segment Descriptors:** In IA-32 (x86), segment descriptors are the core of segmentation. They are stored in descriptor tables and hold crucial information about a memory segment, including its base address, limit (size), access rights, and more. These fields are analogous to those found in page table entries, which manage memory in paged systems.\n\nLet's analyze why the other options are incorrect:\n\n* **A) Global Descriptor Table (GDT):** The GDT is a table containing segment descriptors. It doesn't contain the fields *within* a segment descriptor itself.\n* **C) Local Descriptor Table (LDT):** Similar to the GDT, the LDT is a table holding segment descriptors. It's not a descriptor itself.\n* **D) Attribute Field:**  The attribute field is a part of a segment descriptor. While it contains important information, it doesn't represent all the fields found in a page table entry.\n\n**Therefore, the segment descriptor (B) is the most accurate answer as it directly contains fields comparable to those found in page table entries.** \n"
            }
        ]
    },
    {
        "doc_id": 1281,
        "doc": {
            "question_id": 123122348,
            "question": "The ____ field in segment descriptors is used to specify whether a segment grows towards higher or lower memory addresses.",
            "options": [
                "G",
                "D",
                "Expand-down",
                "DPL"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ field in segment descriptors is used to specify whether a segment grows towards higher or lower memory addresses.",
                "Expected Label": "C",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **G (Granularity Flag):**  Determines if the segment size is interpreted in bytes or 4KB pages.\n* **D (Default Operand Size):**  Specifies the default operand and addressing mode (16-bit or 32-bit).\n* **Expand-down:**  Not a field within a segment descriptor. This concept relates to stack growth direction.\n* **DPL (Descriptor Privilege Level):** Defines the privilege level required to access the segment. \n\nThe **D flag (Default Operand Size)** is used to indicate the growth direction. If D=0, the segment grows upward (from low to high addresses). If D=1, the segment grows downward (from high to low addresses), typical for stacks. \n"
            }
        ]
    },
    {
        "doc_id": 1282,
        "doc": {
            "question_id": 123122351,
            "question": "The ____ bit in a page table entry indicates whether the page has been modified.",
            "options": [
                "Presence",
                "Dirty",
                "Accessed",
                "No execute"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ bit in a page table entry indicates whether the page has been modified.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1283,
        "doc": {
            "question_id": 123122352,
            "question": "The ____ protection model is generally considered more complex to implement than simple paging protection used in UNIX-like systems.",
            "options": [
                "flat",
                "hierarchical",
                "segmented",
                "virtual"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ protection model is generally considered more complex to implement than simple paging protection used in UNIX-like systems.",
                "Expected Label": "C",
                "Predicted Label": "C. \n"
            }
        ]
    },
    {
        "doc_id": 1284,
        "doc": {
            "question_id": 123122353,
            "question": "The ____ is typically the most difficult aspect of computer architecture to change after initial design.",
            "options": [
                "Clock speed",
                "Cache size",
                "Instruction set",
                "Address size"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is typically the most difficult aspect of computer architecture to change after initial design.",
                "Expected Label": "D",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Instruction Set (C):**  The instruction set architecture (ISA) defines the fundamental operations a processor can perform. Changing it after the initial design would break compatibility with existing software and hardware, requiring massive rewrites and redesigns. This makes it the most difficult aspect to change.\n\nHere's why the other options are less difficult to change:\n\n* **Clock Speed (A):** Clock speed can be adjusted relatively easily, even after manufacture (think overclocking).\n* **Cache Size (B):** While not trivial, cache sizes are often modular and can be modified in later revisions or generations of a processor. \n* **Address Size (D):**  Similar to cache size, address size can be expanded in later generations, though it can be a complex process. \n"
            }
        ]
    },
    {
        "doc_id": 1285,
        "doc": {
            "question_id": 123122358,
            "question": "The ____ of a process can significantly impact TLB miss rates and overhead in a multitasking environment.",
            "options": [
                "CPU speed",
                "Memory size",
                "Cache size",
                "Context switch frequency"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ of a process can significantly impact TLB miss rates and overhead in a multitasking environment.",
                "Expected Label": "D",
                "Predicted Label": "D) Context switch frequency \n\nHere's why:\n\n* **Context Switch Frequency:** Every context switch requires loading the new process's page table information into the TLB.  Frequent context switches mean the TLB is constantly being flushed and reloaded, leading to more TLB misses and higher overhead.\n\nLet's look at why the other options are less relevant:\n\n* **CPU Speed:**  While CPU speed affects overall performance, it doesn't directly influence TLB miss rates in relation to multitasking.\n* **Memory Size:** A larger memory might reduce page faults, but not necessarily TLB misses specifically related to context switching.\n* **Cache Size:** Cache size primarily impacts memory access times and is less directly related to the TLB, which handles virtual-to-physical address translation. \n"
            }
        ]
    },
    {
        "doc_id": 1286,
        "doc": {
            "question_id": 123122359,
            "question": "____ is a key feature that allows different protection levels for read and write/execute operations on memory pages.",
            "options": [
                "Multi-level protection IDs",
                "Page table encryption",
                "Virtual memory segmentation",
                "Cache coherence protocols"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key feature that allows different protection levels for read and write/execute operations on memory pages.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1287,
        "doc": {
            "question_id": 123122360,
            "question": "____ is a key challenge in implementing pipelined processors.",
            "options": [
                "Power consumption",
                "Hazard resolution",
                "Cache coherence",
                "Branch prediction"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a key challenge in implementing pipelined processors.",
                "Expected Label": "B",
                "Predicted Label": "B) Hazard resolution \n"
            }
        ]
    },
    {
        "doc_id": 1288,
        "doc": {
            "question_id": 123122363,
            "question": "In RISC architectures, ____ are typically used to move data between memory and registers.",
            "options": [
                "ALU instructions",
                "load and store instructions",
                "branch instructions",
                "floating-point instructions"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In RISC architectures, ____ are typically used to move data between memory and registers.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1289,
        "doc": {
            "question_id": 123122366,
            "question": "____ is typically used in both the instruction decode and write-back stages of a RISC pipeline.",
            "options": [
                "The register file",
                "The ALU",
                "The data cache",
                "The program counter"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used in both the instruction decode and write-back stages of a RISC pipeline.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1290,
        "doc": {
            "question_id": 123451241,
            "question": "____ hazards arise when an instruction depends on the results of a previous instruction in a way that is exposed by the overlapping of instructions in the pipeline.",
            "options": [
                "Data",
                "Control",
                "Structural",
                "Resource"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ hazards arise when an instruction depends on the results of a previous instruction in a way that is exposed by the overlapping of instructions in the pipeline.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1291,
        "doc": {
            "question_id": 123451242,
            "question": "The ideal CPI on a pipelined processor is almost always ____.",
            "options": [
                "0",
                "1",
                "2",
                "4"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ideal CPI on a pipelined processor is almost always ____.",
                "Expected Label": "B",
                "Predicted Label": "B) 1 \n\n**Explanation:**\n\nCPI (Cycles Per Instruction) is a measure of processor performance.  An ideal pipelined processor aims to complete one instruction per clock cycle. \n\n* **CPI of 1** signifies that, on average, it takes one clock cycle to execute each instruction. This is the ideal scenario for a well-pipelined processor.\n\nLet me know if you'd like a deeper dive into pipelining concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1292,
        "doc": {
            "question_id": 123451243,
            "question": "A ____ is commonly used to describe a stall in a pipelined processor that takes up space but performs no useful work.",
            "options": [
                "cache miss",
                "branch mispredict",
                "pipeline bubble",
                "structural hazard"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ is commonly used to describe a stall in a pipelined processor that takes up space but performs no useful work.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1293,
        "doc": {
            "question_id": 123451244,
            "question": "The primary effect of a pipeline bubble is to ____ for that instruction slot as it travels through the pipeline.",
            "options": [
                "reduce power consumption",
                "increase throughput",
                "decrease latency",
                "occupy the resources"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary effect of a pipeline bubble is to ____ for that instruction slot as it travels through the pipeline.",
                "Expected Label": "D",
                "Predicted Label": "D) occupy the resources \n\n**Explanation:**\n\nPipeline bubbles represent stalls in the instruction pipeline.  While a bubble occupies a stage, no useful work for the stalled instruction is being done in that stage. This does *not* mean the resources are actively being *used* \u2013 they are simply reserved and sitting idle.\n\nLet's look at why the other options are incorrect:\n\n* **A) reduce power consumption:** Bubbles generally *increase* power consumption. Even though no work is being done, the pipeline stage still consumes power.\n* **B) increase throughput:** Bubbles directly *reduce* throughput. Throughput is about instructions completed per unit of time, and bubbles introduce delays.\n* **C) decrease latency:** Bubbles *increase* latency. Latency is the time taken for a single instruction to complete, and bubbles add idle time to that process. \n"
            }
        ]
    },
    {
        "doc_id": 1294,
        "doc": {
            "question_id": 123451247,
            "question": "____ is a technique used to resolve data hazards by bypassing values from pipeline registers to execution units.",
            "options": [
                "Data forwarding",
                "Branch prediction",
                "Out-of-order execution",
                "Register renaming"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used to resolve data hazards by bypassing values from pipeline registers to execution units.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1295,
        "doc": {
            "question_id": 123451251,
            "question": "A ____ branch includes the direction that the branch was predicted.",
            "options": [
                "speculative",
                "canceling",
                "delayed",
                "conditional"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ branch includes the direction that the branch was predicted.",
                "Expected Label": "B",
                "Predicted Label": "**B) canceling** \n\n**Explanation:**\n\n* **Canceling branches** are a technique used in pipelined processors. The branch prediction is included in the instruction itself. If the prediction is incorrect, the instruction and its effects are \"canceled\" or flushed from the pipeline. \n\nLet's look at why the other options are incorrect:\n\n* **Speculative branches:** These branches involve making a prediction (either taken or not taken) and speculatively executing instructions based on that prediction. However, they don't inherently include the prediction information within the branch instruction itself.\n* **Delayed branches:** These branches delay the execution of the branch instruction to allow time for instructions in the pipeline to complete. This technique doesn't directly relate to including the prediction direction.\n* **Conditional branches:**  These are branches that occur based on a condition (like \"branch if equal\"). While all branches in the other options are technically conditional branches, the term itself doesn't specifically imply the inclusion of prediction information. \n"
            }
        ]
    },
    {
        "doc_id": 1296,
        "doc": {
            "question_id": 123451253,
            "question": "A ____ is a simple dynamic branch prediction scheme that uses a small memory indexed by part of the branch instruction address.",
            "options": [
                "branch-prediction buffer",
                "branch target buffer",
                "return address stack",
                "correlation predictor"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ is a simple dynamic branch prediction scheme that uses a small memory indexed by part of the branch instruction address.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Branch-prediction buffer (A):** This is the correct answer. A branch-prediction buffer is a small, specialized cache that stores the recent history of branches. It's indexed by a portion of the branch instruction address, allowing it to quickly predict whether a branch will be taken or not based on past behavior.\n\nLet's look at why the other options are incorrect:\n\n* **Branch target buffer (B):** A branch target buffer caches the target addresses of recently taken branches. It helps reduce the penalty of a taken branch by providing the target address quickly but doesn't directly predict whether the branch will be taken.\n* **Return address stack (C):** This structure is used to store the return addresses of function calls, ensuring that the program can return to the correct location after a function completes. It's not involved in branch prediction.\n* **Correlation predictor (D):** This is a more sophisticated branch prediction technique that uses the history of multiple branches to predict the outcome of a current branch. While effective, it's not a simple scheme. \n"
            }
        ]
    },
    {
        "doc_id": 1297,
        "doc": {
            "question_id": 123451257,
            "question": "In a single-cycle CPU implementation where every instruction takes 1 long clock cycle, ____ would be eliminated.",
            "options": [
                "the ALU",
                "the register file",
                "temporary registers",
                "the program counter"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a single-cycle CPU implementation where every instruction takes 1 long clock cycle, ____ would be eliminated.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) temporary registers**. Here's why:\n\n* **Single-cycle CPUs** execute instructions in a single clock cycle. To achieve this, they employ a long clock period to accommodate the slowest instruction.\n* **Temporary registers** are primarily used to hold intermediate results within a clock cycle, especially in multi-cycle instruction execution (fetch, decode, execute, etc.). \n* Since single-cycle CPUs don't break down instructions into multiple stages within a clock cycle, they eliminate the need for storing intermediate results in temporary registers.\n\n**The other options are incorrect because:**\n\n* **A) the ALU (Arithmetic Logic Unit)** is essential for performing calculations and is required in both single and multi-cycle CPUs.\n* **B) the register file** stores frequently used data and instructions, crucial for CPU operation regardless of the cycle design.\n* **D) the program counter** keeps track of the next instruction to be executed and is indispensable for program flow. \n"
            }
        ]
    },
    {
        "doc_id": 1298,
        "doc": {
            "question_id": 123451259,
            "question": "____ are added between pipeline stages to convey values and control information from one stage to the next.",
            "options": [
                "Pipeline registers",
                "Multiplexers",
                "Adders",
                "Instruction decoders"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are added between pipeline stages to convey values and control information from one stage to the next.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1299,
        "doc": {
            "question_id": 123451260,
            "question": "The ____ stage of a typical RISC pipeline is where data hazards are typically checked.",
            "options": [
                "IF",
                "ID",
                "EX",
                "MEM"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ stage of a typical RISC pipeline is where data hazards are typically checked.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1300,
        "doc": {
            "question_id": 123451261,
            "question": "The ____ stage in a typical MIPS pipeline is responsible for fetching operands from registers.",
            "options": [
                "IF",
                "EX",
                "ID",
                "MEM"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ stage in a typical MIPS pipeline is responsible for fetching operands from registers.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **IF (Instruction Fetch):** Fetches instructions from memory.\n* **ID (Instruction Decode):** Decodes the instruction and fetches operands from registers.\n* **EX (Execute):** Performs the arithmetic or logical operation.\n* **MEM (Memory Access):** Accesses memory for load and store instructions.\n\nTherefore, the **ID (Instruction Decode)** stage is responsible for fetching operands from registers. \n"
            }
        ]
    },
    {
        "doc_id": 1301,
        "doc": {
            "question_id": 123451263,
            "question": "____ is typically used to detect the need for data forwarding in a pipelined processor.",
            "options": [
                "Comparison of register fields",
                "Branch prediction",
                "Cache coherence protocols",
                "Memory prefetching"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to detect the need for data forwarding in a pipelined processor.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Data forwarding** (also called data bypassing) is a technique where a pipeline stage can directly fetch data from a later stage that has already produced it, rather than waiting for the data to be written back to the register file. \n\n* **Comparison of register fields** in the pipeline stages (specifically, comparing the destination register of an instruction with the source registers of subsequent instructions) is the mechanism used to detect when an instruction depends on the result of a previous instruction that hasn't yet completed. This dependency triggers the need for data forwarding.\n\nLet's look at why the other options are incorrect:\n\n* **Branch prediction** deals with predicting the outcome of branch instructions to keep the pipeline filled, but it's not directly related to detecting the need for data forwarding.\n* **Cache coherence protocols** ensure that data shared among multiple processors (in a multiprocessor system) remains consistent. This is not directly related to data forwarding within a single pipeline.\n* **Memory prefetching** aims to predict and fetch data from memory into the cache before it's needed, improving memory access performance. While this can benefit a pipelined processor, it's not the mechanism for detecting data forwarding needs. \n"
            }
        ]
    },
    {
        "doc_id": 1302,
        "doc": {
            "question_id": 123451266,
            "question": "The primary purpose of using a separate adder in the ID stage of a pipelined processor is to ____.",
            "options": [
                "perform arithmetic operations",
                "calculate memory addresses",
                "execute logical operations",
                "compute the branch-target address"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary purpose of using a separate adder in the ID stage of a pipelined processor is to ____.",
                "Expected Label": "D",
                "Predicted Label": "D) compute the branch-target address \n\n**Explanation:**\n\nWhile adders in processors handle arithmetic and can be involved in memory addressing, the *primary* reason for a dedicated adder in the ID (Instruction Decode) stage of a pipelined processor is to quickly calculate branch target addresses.  This is essential for:\n\n* **Conditional Branching:** The adder calculates the target address based on the branch condition and offset, allowing the pipeline to fetch the correct instruction even before the condition is fully evaluated.\n* **Pipeline Efficiency:**  Delays from branch mispredictions are minimized because the target address is ready early in the pipeline. \n\nLet me know if you'd like more details on pipelining and branch prediction! \n"
            }
        ]
    },
    {
        "doc_id": 1303,
        "doc": {
            "question_id": 123451270,
            "question": "In a pipelined processor, exceptions are typically handled by ____ associated with each instruction as it moves through the pipeline.",
            "options": [
                "a program counter",
                "an exception status vector",
                "a branch predictor",
                "a reorder buffer"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a pipelined processor, exceptions are typically handled by ____ associated with each instruction as it moves through the pipeline.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Exception status vectors** are used to track the potential for exceptions as instructions progress through the pipeline. If an exception occurs, this vector helps identify the offending instruction and its stage in the pipeline, allowing for proper handling.\n\nLet's look at why the other options are less suitable:\n\n* **Program counter (A):** The program counter keeps track of the next instruction to be fetched, but doesn't directly handle exceptions. \n* **Branch predictor (C):**  A branch predictor speculates on branch outcomes to optimize instruction flow. It's not directly involved in exception handling.\n* **Reorder buffer (D):** A reorder buffer helps commit instructions in the correct order despite out-of-order execution. While it plays a role in exception recovery, it's not the primary mechanism for handling exceptions *during* pipeline execution. \n"
            }
        ]
    },
    {
        "doc_id": 1304,
        "doc": {
            "question_id": 123451272,
            "question": "In pipelined processors, ____ are typically treated as operands that require hazard detection for RAW hazards with branches.",
            "options": [
                "Registers",
                "Cache lines",
                "Program counters",
                "Condition codes"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In pipelined processors, ____ are typically treated as operands that require hazard detection for RAW hazards with branches.",
                "Expected Label": "D",
                "Predicted Label": "D) Condition codes \n\n**Explanation:**\n\n* **Condition codes** (or flags) are set by instructions based on the result of operations (e.g., zero, negative, overflow). Branches often rely on these flags to determine the next instruction to execute. If an instruction modifying a condition code is in the pipeline while a branch instruction that depends on that code is fetched, a RAW (Read-After-Write) hazard occurs.\n\nLet's analyze why the other options are less suitable:\n\n* **Registers:** While registers are involved in RAW hazards, they are not *specifically* treated as operands requiring hazard detection with branches. The hazard detection mechanism considers the dependency between instructions, regardless of whether they use registers.\n* **Cache lines:** Cache lines are related to memory access and don't directly influence branch prediction or hazard detection in the context of RAW hazards.\n* **Program counters:**  The program counter is updated for branches, but it doesn't represent an operand that causes a RAW hazard in the same way as condition codes.\n\n**In summary:** Condition codes are directly involved in the decision-making process of branches, making them crucial for detecting and resolving potential RAW hazards. \n"
            }
        ]
    },
    {
        "doc_id": 1305,
        "doc": {
            "question_id": 123451275,
            "question": "The ____ of a pipelined processor is typically one cycle less than the number of pipeline stages from EX to the result-producing stage.",
            "options": [
                "Initiation interval",
                "Clock frequency",
                "Pipeline latency",
                "Throughput"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ of a pipelined processor is typically one cycle less than the number of pipeline stages from EX to the result-producing stage.",
                "Expected Label": "C",
                "Predicted Label": "A) Initiation interval \n"
            }
        ]
    },
    {
        "doc_id": 1306,
        "doc": {
            "question_id": 123451282,
            "question": "____ is typically the stage in a pipeline where effective address calculation, ALU operations, and branch-target computation occur.",
            "options": [
                "Execution",
                "Instruction Fetch",
                "Write-back",
                "Memory Access"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically the stage in a pipeline where effective address calculation, ALU operations, and branch-target computation occur.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1307,
        "doc": {
            "question_id": 123451285,
            "question": "The floating-point operation with the longest latency in a typical pipelined processor is ____.",
            "options": [
                "Addition",
                "Multiplication",
                "Division",
                "Square root"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The floating-point operation with the longest latency in a typical pipelined processor is ____.",
                "Expected Label": "D",
                "Predicted Label": "D) Square root \n"
            }
        ]
    },
    {
        "doc_id": 1308,
        "doc": {
            "question_id": 123451287,
            "question": "The primary source of pipeline stalls in ____ programs is typically branch delays.",
            "options": [
                "floating-point",
                "memory-intensive",
                "integer",
                "I/O-bound"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The primary source of pipeline stalls in ____ programs is typically branch delays.",
                "Expected Label": "C",
                "Predicted Label": "C. \n"
            }
        ]
    },
    {
        "doc_id": 1309,
        "doc": {
            "question_id": 123451288,
            "question": "____ is a technique for allowing instructions to execute out of order when there are sufficient resources and no data dependences.",
            "options": [
                "Scoreboarding",
                "Pipelining",
                "Superscalar",
                "Branch prediction"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique for allowing instructions to execute out of order when there are sufficient resources and no data dependences.",
                "Expected Label": "A",
                "Predicted Label": "C) Superscalar \n\n**Explanation:**\n\n* **Superscalar** architectures are specifically designed to execute instructions out of order. They achieve this by having multiple execution units, allowing them to process independent instructions simultaneously, even if they are not in sequential order.\n\nLet's look at why the other options are not the best fit:\n\n* **Scoreboarding** is a technique used to track data dependencies and hazards in a pipelined processor to avoid data hazards caused by out-of-order execution. While it enables out-of-order execution, it's a mechanism to manage it, not the technique itself.\n* **Pipelining** improves performance by overlapping the execution of multiple instructions. However, it doesn't inherently imply out-of-order execution. \n* **Branch prediction** is used to predict the outcome of conditional branch instructions and speculatively execute instructions along the predicted path. While it contributes to instruction level parallelism, it's not the core technique for enabling out-of-order execution. \n"
            }
        ]
    },
    {
        "doc_id": 1310,
        "doc": {
            "question_id": 123451292,
            "question": "____ is a key component of a scoreboard that indicates which of the four steps an instruction is in.",
            "options": [
                "Instruction status",
                "Functional unit status",
                "Register status",
                "Pipeline status"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key component of a scoreboard that indicates which of the four steps an instruction is in.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1311,
        "doc": {
            "question_id": 123451305,
            "question": "____ faults are caused by mistakes made by operations and maintenance personnel.",
            "options": [
                "Hardware",
                "Operation",
                "Environmental",
                "Design"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ faults are caused by mistakes made by operations and maintenance personnel.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1312,
        "doc": {
            "question_id": 123451306,
            "question": "In fault-tolerant computing systems, transient faults often precede ____ faults.",
            "options": [
                "intermittent",
                "soft",
                "hard",
                "permanent"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In fault-tolerant computing systems, transient faults often precede ____ faults.",
                "Expected Label": "C",
                "Predicted Label": "D) permanent \n\n**Explanation:** Transient faults are temporary errors that correct themselves or can be easily fixed.  If left unaddressed, they can sometimes escalate into permanent (or hard) faults, which are persistent and require component replacement. \n"
            }
        ]
    },
    {
        "doc_id": 1313,
        "doc": {
            "question_id": 123451310,
            "question": "In TPC benchmarks, ____ is included with the benchmark results to enable evaluations based on price-performance.",
            "options": [
                "Energy consumption",
                "Price",
                "Market share",
                "Development time"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In TPC benchmarks, ____ is included with the benchmark results to enable evaluations based on price-performance.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) Price**. \n\nHere's why:\n\n* **TPC (Transaction Processing Performance Council)** benchmarks are designed to measure the performance of transaction processing systems. \n* A key goal of these benchmarks is to provide a fair comparison of different systems, and that includes considering the **price/performance ratio**.  \n* Including price information in the benchmark results allows potential users to evaluate how much performance they get for their investment.\n\nLet me know if you'd like more details on TPC benchmarks! \n"
            }
        ]
    },
    {
        "doc_id": 1314,
        "doc": {
            "question_id": 123451313,
            "question": "In queueing theory, the area where tasks accumulate while waiting to be serviced is called the ____.",
            "options": [
                "server",
                "buffer",
                "queue",
                "accumulator"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Computing Domains and Workloads"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In queueing theory, the area where tasks accumulate while waiting to be serviced is called the ____.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1315,
        "doc": {
            "question_id": 123451315,
            "question": "____ is the simplest and most common queue discipline in I/O systems.",
            "options": [
                "First in, first out (FIFO)",
                "Last in, first out (LIFO)",
                "Round robin",
                "Priority-based"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the simplest and most common queue discipline in I/O systems.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1316,
        "doc": {
            "question_id": 123451318,
            "question": "In queuing theory, ____ is used to characterize both the request arrival rate and service time in the M/M/1 model.",
            "options": [
                "Exponential distribution",
                "Poisson distribution",
                "Normal distribution",
                "Uniform distribution"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In queuing theory, ____ is used to characterize both the request arrival rate and service time in the M/M/1 model.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1317,
        "doc": {
            "question_id": 123451321,
            "question": "____ is the element of storage exported from a disk array, usually constructed from a subset of the array's disks.",
            "options": [
                "Logical unit",
                "Physical volume",
                "Logical volume",
                "Block server"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the element of storage exported from a disk array, usually constructed from a subset of the array's disks.",
                "Expected Label": "A",
                "Predicted Label": "**C) Logical volume** \n\n**Explanation:**\n\n* **Logical volume** is a flexible storage unit created from a pool of physical storage (like disks in a disk array). It can be sized and managed independently of the underlying physical disks. \n\nLet's break down why the other options are incorrect:\n\n* **Logical unit (LU):**  A term often used in the context of SCSI storage protocols, referring to an addressable entity on a storage device. Not specific to disk arrays or constructed from their subsets.\n* **Physical volume:** Represents a single physical disk or a portion of a disk directly managed by the operating system. It's the building block *before* a logical volume is created.\n* **Block server:** A server that provides block-level storage access over a network (like Fibre Channel or iSCSI). While it uses disk arrays, it's not an element *exported* from them. \n"
            }
        ]
    },
    {
        "doc_id": 1318,
        "doc": {
            "question_id": 123451322,
            "question": "_____ I/O allows a process to continue making requests so that many I/O requests can be operating simultaneously.",
            "options": [
                "Synchronous",
                "Asynchronous",
                "Sequential",
                "Random"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "_____ I/O allows a process to continue making requests so that many I/O requests can be operating simultaneously.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1319,
        "doc": {
            "question_id": 123451331,
            "question": "____ is a RAID configuration that can tolerate two disk failures.",
            "options": [
                "RAID-DP",
                "RAID-0",
                "RAID-1",
                "RAID-3"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a RAID configuration that can tolerate two disk failures.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1320,
        "doc": {
            "question_id": 123451332,
            "question": "In RAID systems, ____ is the time it takes to rebuild data on a replacement disk after a disk failure.",
            "options": [
                "MTDL",
                "MTTR",
                "MTTF",
                "MTBF"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In RAID systems, ____ is the time it takes to rebuild data on a replacement disk after a disk failure.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1321,
        "doc": {
            "question_id": 123451335,
            "question": "In storage systems, ____ is a proactive approach to detect and correct errors by periodically scanning all data.",
            "options": [
                "Lazy checking",
                "Error correction coding",
                "Parity protection",
                "Disk scrubbing"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Storage Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In storage systems, ____ is a proactive approach to detect and correct errors by periodically scanning all data.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1322,
        "doc": {
            "question_id": 123451338,
            "question": "In a system where I/O operations involve extra memory copies, ____ can become a bottleneck during I/O-intensive tasks like sorting.",
            "options": [
                "CPU speed",
                "Disk capacity",
                "Memory bandwidth",
                "Cache size"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a system where I/O operations involve extra memory copies, ____ can become a bottleneck during I/O-intensive tasks like sorting.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1323,
        "doc": {
            "question_id": 123451341,
            "question": "____ is a specialized processor commonly used in embedded systems for efficient signal processing tasks.",
            "options": [
                "GPU",
                "DSP",
                "FPGA",
                "TPU"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a specialized processor commonly used in embedded systems for efficient signal processing tasks.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1324,
        "doc": {
            "question_id": 123451342,
            "question": "____ is a common approach for solving embedded problems where custom hardware is integrated with a standardized processor core.",
            "options": [
                "System on a chip (SOC)",
                "Application-specific integrated circuit (ASIC)",
                "Field-programmable gate array (FPGA)",
                "Microcontroller unit (MCU)"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a common approach for solving embedded problems where custom hardware is integrated with a standardized processor core.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1325,
        "doc": {
            "question_id": 123451353,
            "question": "In embedded systems design, ____ is often used to ensure real-time data flow between memory and display.",
            "options": [
                "Cache coherence",
                "DMA channels",
                "Virtual memory",
                "Pipelining"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In embedded systems design, ____ is often used to ensure real-time data flow between memory and display.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1326,
        "doc": {
            "question_id": 123451358,
            "question": "In cellular communication, the ____ is typically used for transmitting at a higher frequency range than the mobile device.",
            "options": [
                "Mobile device",
                "Satellite",
                "Repeater",
                "Base station"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In cellular communication, the ____ is typically used for transmitting at a higher frequency range than the mobile device.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1327,
        "doc": {
            "question_id": 123451359,
            "question": "____ is a technique used in CDMA to preserve bandwidth and allow for more calls per cell.",
            "options": [
                "Speech compression",
                "Frequency hopping",
                "Time division",
                "Analog modulation"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in CDMA to preserve bandwidth and allow for more calls per cell.",
                "Expected Label": "A",
                "Predicted Label": "A) Speech compression \n"
            }
        ]
    },
    {
        "doc_id": 1328,
        "doc": {
            "question_id": 123451360,
            "question": "____ are attempts to combine DSP capabilities with more general-purpose processing abilities for signal processing applications.",
            "options": [
                "VLIW processors",
                "Media extensions",
                "Embedded systems",
                "Real-time constraints"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ are attempts to combine DSP capabilities with more general-purpose processing abilities for signal processing applications.",
                "Expected Label": "B",
                "Predicted Label": "B) Media extensions \n\n**Explanation:**\n\n* **Media extensions** are specific instructions added to general-purpose processor architectures (like x86 or ARM) to accelerate common signal processing tasks. This offers a balance between the flexibility of general-purpose CPUs and the efficiency of dedicated DSPs.\n\nLet's break down why the other options are incorrect:\n\n* **VLIW processors** (Very Long Instruction Word) are designed for parallel processing and can be used in DSP, but they don't represent the core concept of combining DSP with general processing.\n* **Embedded systems** are application-specific computer systems, often using microcontrollers. While they might employ DSP techniques, they are not a specific architectural attempt to combine processing types.\n* **Real-time constraints** are requirements for systems to respond within specific time limits. These are relevant to DSP applications but not a solution for combining processing capabilities. \n"
            }
        ]
    },
    {
        "doc_id": 1329,
        "doc": {
            "question_id": 123451361,
            "question": "____ are replacing buses as the normal means of communication between computers, between I/O devices, between boards, between chips, and even between modules inside chips.",
            "options": [
                "Switched networks",
                "Shared memory systems",
                "Direct memory access",
                "Cache coherence protocols"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are replacing buses as the normal means of communication between computers, between I/O devices, between boards, between chips, and even between modules inside chips.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1330,
        "doc": {
            "question_id": 123451366,
            "question": "The ____ is typically used to distinguish messages intended for different processes on the same device.",
            "options": [
                "Checksum",
                "Port number",
                "Sequence number",
                "Message ID"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ is typically used to distinguish messages intended for different processes on the same device.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1331,
        "doc": {
            "question_id": 123451369,
            "question": "____ networks typically implement flow control mechanisms.",
            "options": [
                "WAN",
                "SAN",
                "LAN",
                "Internet"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ networks typically implement flow control mechanisms.",
                "Expected Label": "B",
                "Predicted Label": "C) LAN \n\n**Explanation:**\n\nWhile flow control can exist in other network types, it's most crucial and commonly implemented in LANs. Here's why:\n\n* **Shared Medium:** LANs often involve shared mediums (like Ethernet cables), increasing the likelihood of collisions if data transmission isn't managed.\n* **Performance Sensitivity:** LANs are performance-sensitive due to their localized nature. Uncontrolled data flows can lead to congestion and slowdowns.\n\nLet me know if you'd like more details on flow control mechanisms! \n"
            }
        ]
    },
    {
        "doc_id": 1332,
        "doc": {
            "question_id": 123451370,
            "question": "In ____, the sending and receiving overheads can practically be ignored compared to the propagation delay component.",
            "options": [
                "OCNs",
                "WANs",
                "SANs",
                "All network types"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In ____, the sending and receiving overheads can practically be ignored compared to the propagation delay component.",
                "Expected Label": "B",
                "Predicted Label": "**A) OCNs** \n\n**Explanation:**\n\n* **OCNs (Optical Carrier Networks)** utilize fiber optic cables for high-speed data transmission over long distances. The propagation delay (time for a signal to travel) in OCNs is significant due to the vast distances involved. In contrast, the sending and receiving overheads associated with processing data packets are relatively minuscule in comparison. \n\nLet me know if you'd like more details on any of the other network types! \n"
            }
        ]
    },
    {
        "doc_id": 1333,
        "doc": {
            "question_id": 123451376,
            "question": "____ is the simplest way to connect multiple devices in a network.",
            "options": [
                "Shared media",
                "Switched media",
                "Full-duplex",
                "Half-duplex"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the simplest way to connect multiple devices in a network.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1334,
        "doc": {
            "question_id": 123451377,
            "question": "____ is a mechanism where network nodes wait a random amount of time before retransmitting after a collision to avoid repeated conflicts.",
            "options": [
                "Token passing",
                "Backoff",
                "Carrier sensing",
                "Collision detection"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a mechanism where network nodes wait a random amount of time before retransmitting after a collision to avoid repeated conflicts.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1335,
        "doc": {
            "question_id": 123451378,
            "question": "In _____ networks, multiple nodes can transmit packets simultaneously over disjoint portions of the network media.",
            "options": [
                "shared-media",
                "unicast",
                "switched-media",
                "broadcast"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In _____ networks, multiple nodes can transmit packets simultaneously over disjoint portions of the network media.",
                "Expected Label": "C",
                "Predicted Label": "C) switched-media \n"
            }
        ]
    },
    {
        "doc_id": 1336,
        "doc": {
            "question_id": 123451384,
            "question": "______ is a common topology used in commercial products for interconnecting a set of devices when the number of switch ports is equal to or larger than the number of devices.",
            "options": [
                "Crossbar",
                "Mesh",
                "Torus",
                "Hypercube"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is a common topology used in commercial products for interconnecting a set of devices when the number of switch ports is equal to or larger than the number of devices.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1337,
        "doc": {
            "question_id": 123451385,
            "question": "In a Multistage Interconnection Network (MIN) built with k \u00d7 k switches, the minimum number of stages required to interconnect N input-output ports is ____.",
            "options": [
                "N/k",
                "logk N",
                "k log N",
                "N/k log N"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a Multistage Interconnection Network (MIN) built with k \u00d7 k switches, the minimum number of stages required to interconnect N input-output ports is ____.",
                "Expected Label": "B",
                "Predicted Label": "Answer: B) logk N \n\n**Explanation:**\n\nThe key to understanding this is the concept of how MINs use stages to scale connectivity. Here's a breakdown:\n\n* **Base:** A single k x k switch connects 'k' inputs to 'k' outputs.\n* **Scaling:** To connect more than 'k' inputs/outputs, you cascade these switches into stages.  Each stage expands the network's connectivity.\n* **Logarithmic Growth:**  With each stage, you effectively multiply the number of ports you can connect by 'k'.  This leads to a logarithmic relationship.\n\nTo connect N ports, you need enough stages (each multiplying connectivity by 'k') to reach at least N. This is represented by logk N. \n"
            }
        ]
    },
    {
        "doc_id": 1338,
        "doc": {
            "question_id": 123451387,
            "question": "In a folded Bene\u0161 network, end nodes are typically connected to the ____ switch stage of the original Bene\u0161 network.",
            "options": [
                "outermost",
                "middle",
                "second",
                "innermost"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a folded Bene\u0161 network, end nodes are typically connected to the ____ switch stage of the original Bene\u0161 network.",
                "Expected Label": "D",
                "Predicted Label": "A) outermost \n"
            }
        ]
    },
    {
        "doc_id": 1339,
        "doc": {
            "question_id": 123451400,
            "question": "The ____ can maximize the number of switch output ports that accept packets, which increases the utilization of network bandwidth.",
            "options": [
                "routing algorithm",
                "arbitration algorithm",
                "switching technique",
                "transmission delay"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ can maximize the number of switch output ports that accept packets, which increases the utilization of network bandwidth.",
                "Expected Label": "B",
                "Predicted Label": "B) arbitration algorithm \n\nHere's why:\n\n* **Arbitration algorithms** determine how multiple inputs share access to a limited number of output ports in a network switch.  An efficient arbitration algorithm minimizes contention and ensures that the maximum number of ports can send data simultaneously, thus maximizing bandwidth utilization.\n\nLet's look at the other options:\n\n* **Routing algorithms** determine the path a packet takes through a network, but don't directly impact switch port utilization.\n* **Switching techniques** (like store-and-forward, cut-through) affect how a switch handles a packet internally, but their impact on port utilization is indirect.\n* **Transmission delay** is the time it takes to send a packet, which isn't directly related to maximizing switch output port usage. \n"
            }
        ]
    },
    {
        "doc_id": 1340,
        "doc": {
            "question_id": 123451401,
            "question": "In network simulations, ____ traffic is often used to allow the network to stabilize at a certain working point and for behavior to be analyzed in detail.",
            "options": [
                "real-world",
                "trace-driven",
                "synthetic",
                "randomized"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In network simulations, ____ traffic is often used to allow the network to stabilize at a certain working point and for behavior to be analyzed in detail.",
                "Expected Label": "C",
                "Predicted Label": "C\n \n\n**Explanation:**\n\n* **Synthetic traffic** is artificially generated and highly controllable, making it ideal for setting up specific network conditions in simulations. This control allows researchers to analyze how a network performs at a specific load or under particular stress conditions. \n\nLet's look at why the other options are less suitable:\n\n* **Real-world traffic** is unpredictable and difficult to reproduce accurately in a simulation.\n* **Trace-driven traffic** relies on recorded data, limiting the ability to modify parameters and explore different scenarios.\n* **Randomized traffic**, while offering some control, might not be representative of real-world patterns and might not lead to consistent, analyzable results. \n"
            }
        ]
    },
    {
        "doc_id": 1341,
        "doc": {
            "question_id": 123451402,
            "question": "In interconnection networks, ____ typically occurs when the network reaches its saturation point, causing a decrease in throughput.",
            "options": [
                "Packet reordering",
                "Load balancing",
                "Deadlock",
                "Head-of-line blocking"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In interconnection networks, ____ typically occurs when the network reaches its saturation point, causing a decrease in throughput.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1342,
        "doc": {
            "question_id": 123451404,
            "question": "In switch microarchitecture, ____ can be used to reduce the size of forwarding tables while maintaining routing flexibility.",
            "options": [
                "Centralized routing",
                "Input buffering",
                "Hybrid approaches",
                "Output buffering"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In switch microarchitecture, ____ can be used to reduce the size of forwarding tables while maintaining routing flexibility.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1343,
        "doc": {
            "question_id": 123451405,
            "question": "In an interconnection network switch, ____ prevents input queue overflow at the neighboring switch on the other end of the link.",
            "options": [
                "Output buffering",
                "Virtual channel allocation",
                "Centralized arbitration",
                "Link-level flow control"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In an interconnection network switch, ____ prevents input queue overflow at the neighboring switch on the other end of the link.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1344,
        "doc": {
            "question_id": 123451410,
            "question": "The number of crosspoint memories in a buffered crossbar switch increases ____ with the number of switch ports.",
            "options": [
                "linearly",
                "logarithmically",
                "quadratically",
                "exponentially"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The number of crosspoint memories in a buffered crossbar switch increases ____ with the number of switch ports.",
                "Expected Label": "C",
                "Predicted Label": "C) quadratically \n\n**Explanation:**\n\nA crossbar switch connects *n* inputs to *m* outputs.  A buffered crossbar adds memory at each intersection point (crosspoint) to store data during the switching process. The total number of crosspoints in such a switch is *n x m*. If the switch has the same number of inputs and outputs (*n = m*), the number of crosspoints grows with the square of the number of ports (n\u00b2), which is a quadratic relationship. \n"
            }
        ]
    },
    {
        "doc_id": 1345,
        "doc": {
            "question_id": 123451414,
            "question": "_____ failures in interconnection networks are typically produced by electromagnetic interference and can often be dealt with by retransmitting the packet.",
            "options": [
                "Permanent",
                "Hardware",
                "Transient",
                "Software"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ failures in interconnection networks are typically produced by electromagnetic interference and can often be dealt with by retransmitting the packet.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C\n"
            }
        ]
    },
    {
        "doc_id": 1346,
        "doc": {
            "question_id": 123451415,
            "question": "____ is a technique that allows for the addition or removal of nodes from a network without disabling it.",
            "options": [
                "Fault-tolerant routing",
                "Network reconfiguration",
                "Bypass switching",
                "Hot swapping"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "____ is a technique that allows for the addition or removal of nodes from a network without disabling it.",
                "Expected Label": "D",
                "Predicted Label": "D) Hot swapping \n"
            }
        ]
    },
    {
        "doc_id": 1347,
        "doc": {
            "question_id": 123451419,
            "question": "In large-scale interconnection networks, ____ is a key feature to handle component failures.",
            "options": [
                "Load balancing",
                "Fault tolerance",
                "Traffic shaping",
                "Quality of Service"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In large-scale interconnection networks, ____ is a key feature to handle component failures.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1348,
        "doc": {
            "question_id": 123451420,
            "question": "In InfiniBand, the ____ mechanism allows the sender to directly write data into the receiver's memory without the receiver explicitly posting a receive buffer.",
            "options": [
                "send/receive",
                "cut-through switching",
                "remote DMA (RDMA)",
                "credit-based flow control"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In InfiniBand, the ____ mechanism allows the sender to directly write data into the receiver's memory without the receiver explicitly posting a receive buffer.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1349,
        "doc": {
            "question_id": 123451423,
            "question": "____ is a technique used to reduce network communication overhead by bypassing the operating system for protocol processing.",
            "options": [
                "Packet switching",
                "OS bypass",
                "Ethernet",
                "InfiniBand"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique used to reduce network communication overhead by bypassing the operating system for protocol processing.",
                "Expected Label": "B",
                "Predicted Label": "**B) OS bypass** \n\n**Explanation:**\n\nOS bypass techniques aim to reduce the overhead associated with traditional network communication where the operating system (OS) handles protocol processing. By bypassing the OS, data can be transferred more directly between applications and network interfaces, reducing latency and improving performance.\n\nLet's analyze why the other options are incorrect:\n\n* **A) Packet switching:** A fundamental networking paradigm, not specifically a technique for reducing overhead by bypassing the OS.\n* **C) Ethernet:** A widely used networking standard, not a technique for OS bypass.\n* **D) InfiniBand:** A high-speed interconnect technology, not inherently focused on OS bypass, though it can be used in conjunction with such techniques. \n"
            }
        ]
    },
    {
        "doc_id": 1350,
        "doc": {
            "question_id": 123451425,
            "question": "In network architecture, _____ is used to organize communication tasks into distinct functional groups.",
            "options": [
                "Multiplexing",
                "Pipelining",
                "Virtualization",
                "Layering"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In network architecture, _____ is used to organize communication tasks into distinct functional groups.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1351,
        "doc": {
            "question_id": 123451427,
            "question": "The ____ layer in the TCP/IP protocol stack is responsible for routing datagrams to their destination.",
            "options": [
                "Ethernet",
                "TCP",
                "IP",
                "Application"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ layer in the TCP/IP protocol stack is responsible for routing datagrams to their destination.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1352,
        "doc": {
            "question_id": 123451429,
            "question": "In InfiniBand networks, ____ implement routing tables.",
            "options": [
                "Network interface cards",
                "Switches",
                "Host channel adapters",
                "Target channel adapters"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In InfiniBand networks, ____ implement routing tables.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1353,
        "doc": {
            "question_id": 123451436,
            "question": "The ____ topology was first used in the Connection Machine CM-5 supercomputer and later in IBM ASCI White and ASC Purple supercomputers.",
            "options": [
                "torus",
                "hypercube",
                "mesh",
                "fat tree"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ topology was first used in the Connection Machine CM-5 supercomputer and later in IBM ASCI White and ASC Purple supercomputers.",
                "Expected Label": "D",
                "Predicted Label": "D) fat tree \n"
            }
        ]
    },
    {
        "doc_id": 1354,
        "doc": {
            "question_id": 123451439,
            "question": "The primary reason for ATM's fixed transfer size is to ____.",
            "options": [
                "maximize bandwidth",
                "simplify routing",
                "reduce packet collisions",
                "minimize transmission delays for time-sensitive applications"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary reason for ATM's fixed transfer size is to ____.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) minimize transmission delays for time-sensitive applications**. \n\nHere's why:\n\n* **ATM (Asynchronous Transfer Mode)** was designed for networks carrying various traffic types, including voice and video, which are highly sensitive to delays. \n* Fixed-size cells (53 bytes) in ATM allow for **predictable transmission times**. This predictability is crucial for real-time applications where even small delays can impact quality.\n\nLet's look at the other options:\n\n* **A) maximize bandwidth:** While fixed cell size contributes to efficient bandwidth usage, it's not the primary reason.\n* **B) simplify routing:** Fixed cell size simplifies some aspects of switching, but its primary benefit is not in routing simplification.\n* **C) reduce packet collisions:** ATM uses a connection-oriented approach and virtual circuits, minimizing the chances of collisions. Fixed cell size is not the primary mechanism for this. \n"
            }
        ]
    },
    {
        "doc_id": 1355,
        "doc": {
            "question_id": 123451440,
            "question": "In an interconnection network, a ____ permutation is when the node with binary coordinates an\u20131, an\u20132, ..., a1, a0 communicates with the node a0, a1, ..., an\u20132, an\u20131.",
            "options": [
                "bit-reversal",
                "perfect shuffle",
                "bit-complement",
                "butterfly"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In an interconnection network, a ____ permutation is when the node with binary coordinates an\u20131, an\u20132, ..., a1, a0 communicates with the node a0, a1, ..., an\u20132, an\u20131.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1356,
        "doc": {
            "question_id": 123451442,
            "question": "In an M/M/1 queuing model, the probability of finding ____ or more tasks in the system is equal to Utilization raised to the power of n.",
            "options": [
                "n-1",
                "n+1",
                "n",
                "2n"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In an M/M/1 queuing model, the probability of finding ____ or more tasks in the system is equal to Utilization raised to the power of n.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1357,
        "doc": {
            "question_id": 123451445,
            "question": "The ____ determines how many vector operations can be executed in parallel within a single vector functional unit.",
            "options": [
                "Number of vector registers",
                "Vector clock rate",
                "Number of vector load-store units",
                "Number of lanes"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ determines how many vector operations can be executed in parallel within a single vector functional unit.",
                "Expected Label": "D",
                "Predicted Label": "D) Number of lanes \n"
            }
        ]
    },
    {
        "doc_id": 1358,
        "doc": {
            "question_id": 123451452,
            "question": "To prevent memory bank conflicts within a single vector memory instruction, the stride and number of banks should be ____.",
            "options": [
                "equal",
                "multiples of each other",
                "relatively prime",
                "powers of two"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "To prevent memory bank conflicts within a single vector memory instruction, the stride and number of banks should be ____.",
                "Expected Label": "C",
                "Predicted Label": "C) relatively prime \n"
            }
        ]
    },
    {
        "doc_id": 1359,
        "doc": {
            "question_id": 123451453,
            "question": "____ is a technique used in vector processors to allow dependent instructions to be executed in the same convoy, reducing execution time.",
            "options": [
                "Chaining",
                "Pipelining",
                "Superscalar",
                "Out-of-order execution"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in vector processors to allow dependent instructions to be executed in the same convoy, reducing execution time.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1360,
        "doc": {
            "question_id": 123451454,
            "question": "The ____ is often used to report vector processor performance on a loop instead of execution time.",
            "options": [
                "MIPS rating",
                "IPC",
                "CPI",
                "MFLOPS rating"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Benchmarking and Measurement"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is often used to report vector processor performance on a loop instead of execution time.",
                "Expected Label": "D",
                "Predicted Label": "D) MFLOPS rating \n\n**Explanation:**\n\n* **MFLOPS (Millions of Floating-Point Operations Per Second)** is a direct measure of a vector processor's ability to perform floating-point calculations, which are common in scientific and multimedia workloads.  Vector processors excel at these operations.\n\nHere's why the other options are less suitable:\n\n* **MIPS (Millions of Instructions Per Second)** is a general measure of processor speed, but it doesn't specifically focus on the floating-point operations that vector processors are optimized for.\n* **IPC (Instructions Per Cycle)** measures how many instructions a processor can execute per clock cycle. While relevant to performance, it doesn't isolate vector processing capabilities.\n* **CPI (Cycles Per Instruction)** is the inverse of IPC and similarly doesn't directly represent vector processing performance. \n"
            }
        ]
    },
    {
        "doc_id": 1361,
        "doc": {
            "question_id": 123451456,
            "question": "In vector processors, the ____ is typically the main factor contributing to the difference between peak and sustained performance.",
            "options": [
                "clock rate",
                "start-up overhead",
                "vector length",
                "chime time"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In vector processors, the ____ is typically the main factor contributing to the difference between peak and sustained performance.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1362,
        "doc": {
            "question_id": 123451463,
            "question": "The technique of adding sequences of progressively shorter vectors to handle recurrences in vector processing is called ____.",
            "options": [
                "recursive halving",
                "recursive doubling",
                "partial sums",
                "vector chaining"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The technique of adding sequences of progressively shorter vectors to handle recurrences in vector processing is called ____.",
                "Expected Label": "B",
                "Predicted Label": "A. Recursive Halving \n"
            }
        ]
    },
    {
        "doc_id": 1363,
        "doc": {
            "question_id": 123451473,
            "question": "_____ is a compiler technique that reorganizes loops by selecting instructions from different iterations of the original loop for each iteration of the reorganized code.",
            "options": [
                "Loop unrolling",
                "Trace scheduling",
                "Software pipelining",
                "Algebraic optimization"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "_____ is a compiler technique that reorganizes loops by selecting instructions from different iterations of the original loop for each iteration of the reorganized code.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1364,
        "doc": {
            "question_id": 123451478,
            "question": "____ is a key consideration when deciding whether to move code above a conditional branch in global code scheduling.",
            "options": [
                "Relative execution frequency of branch cases",
                "Number of registers available",
                "Cache size",
                "Pipeline depth"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key consideration when deciding whether to move code above a conditional branch in global code scheduling.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1365,
        "doc": {
            "question_id": 123451480,
            "question": "The ____ technique involves unwinding a loop multiple times to expose instruction-level parallelism.",
            "options": [
                "Loop fusion",
                "Software pipelining",
                "Loop unrolling",
                "Branch prediction"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ technique involves unwinding a loop multiple times to expose instruction-level parallelism.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Loop unrolling** is the technique that explicitly replicates loop body multiple times to expose more instructions for parallel execution.\n\nHere's why the other options are incorrect:\n\n* **Loop fusion:** Combines adjacent loops with similar iterations into a single loop to reduce overhead.\n* **Software pipelining:** A compiler technique that restructures loop iterations to overlap instructions from different iterations.\n* **Branch prediction:** A hardware technique that predicts the outcome of conditional branches to speculatively execute instructions. \n"
            }
        ]
    },
    {
        "doc_id": 1366,
        "doc": {
            "question_id": 123451482,
            "question": "____ is a technique that involves replicating the body of a loop multiple times to create a larger code block for optimization.",
            "options": [
                "Loop unrolling",
                "Software pipelining",
                "Trace scheduling",
                "Branch prediction"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique that involves replicating the body of a loop multiple times to create a larger code block for optimization.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1367,
        "doc": {
            "question_id": 123451483,
            "question": "____ instructions execute normally if a condition is true, and as a no-op if the condition is false.",
            "options": [
                "Branching",
                "Conditional",
                "Pipelined",
                "Trace"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ instructions execute normally if a condition is true, and as a no-op if the condition is false.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1368,
        "doc": {
            "question_id": 123451490,
            "question": "____ is a key mechanism used in VLIW architectures to allow loads to be moved across stores when the compiler cannot be absolutely certain the addresses do not conflict.",
            "options": [
                "Address conflict checking",
                "Register renaming",
                "Branch prediction",
                "Cache coherence"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key mechanism used in VLIW architectures to allow loads to be moved across stores when the compiler cannot be absolutely certain the addresses do not conflict.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1369,
        "doc": {
            "question_id": 123451492,
            "question": "In the IA-64 architecture, a ____ is a sequence of consecutive instructions with no register data dependences among them.",
            "options": [
                "bundle",
                "procedure",
                "instruction group",
                "register stack"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the IA-64 architecture, a ____ is a sequence of consecutive instructions with no register data dependences among them.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1370,
        "doc": {
            "question_id": 123451493,
            "question": "In modern VLIW architectures, ____ typically specifies the execution unit types required for each instruction in a bundle.",
            "options": [
                "The instruction opcode",
                "A separate control register",
                "The program counter",
                "A template field"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In modern VLIW architectures, ____ typically specifies the execution unit types required for each instruction in a bundle.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1371,
        "doc": {
            "question_id": 123451494,
            "question": "In VLIW architectures, ____ is used to group multiple instructions that can potentially be executed in parallel.",
            "options": [
                "A thread",
                "A bundle",
                "A core",
                "A pipeline"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In VLIW architectures, ____ is used to group multiple instructions that can potentially be executed in parallel.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1372,
        "doc": {
            "question_id": 123451498,
            "question": "____ is typically the most critical performance issue in multiprocessor systems.",
            "options": [
                "Branch prediction",
                "Interprocessor communication",
                "Cache size",
                "Clock frequency"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically the most critical performance issue in multiprocessor systems.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1373,
        "doc": {
            "question_id": 123451505,
            "question": "In parallel FFT algorithms, ____ communication is typically required during the transpose phases.",
            "options": [
                "all-to-all",
                "one-to-all",
                "all-to-one",
                "one-to-one"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In parallel FFT algorithms, ____ communication is typically required during the transpose phases.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1374,
        "doc": {
            "question_id": 123451508,
            "question": "In parallel processing, the ____ ratio is a key characteristic in determining the performance of parallel programs.",
            "options": [
                "processor-to-memory",
                "cache-to-main memory",
                "instruction-to-data",
                "computation-to-communication"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Performance Analysis and Optimization"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In parallel processing, the ____ ratio is a key characteristic in determining the performance of parallel programs.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1375,
        "doc": {
            "question_id": 123451510,
            "question": "In multiprocessor systems with high contention, the primary concern for synchronization operations is ____.",
            "options": [
                "Latency",
                "Throughput",
                "Power consumption",
                "Serialization"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In multiprocessor systems with high contention, the primary concern for synchronization operations is ____.",
                "Expected Label": "D",
                "Predicted Label": "B) Throughput \n\n**Explanation:**\n\nWhile latency is a factor, **throughput** is the primary concern in high-contention multiprocessor scenarios.  Here's why:\n\n* **High Contention:**  This implies many processors frequently need to access shared resources (data, memory, etc.) simultaneously. \n* **Throughput Focus:** The main goal is to maximize the overall rate at which these processors can complete their operations.  \n* **Synchronization Bottleneck:**  Synchronization mechanisms (like locks) are essential to prevent data corruption, but they can become bottlenecks if not designed for high contention. Inefficient synchronization significantly reduces the rate at which processors can acquire access and proceed, impacting overall throughput.\n\n**Let's look at why the other options are less important in this context:**\n\n* **Latency (A):**  While minimizing the time each processor waits is good, it's secondary to ensuring the system as a whole processes data efficiently.\n* **Power Consumption (C):**  Power is a general concern but not directly tied to the core issue of synchronization in high-contention scenarios.\n* **Serialization (D):**  Serialization is a consequence of synchronization to ensure correctness.  The challenge is minimizing the performance hit serialization introduces. \n"
            }
        ]
    },
    {
        "doc_id": 1376,
        "doc": {
            "question_id": 123451511,
            "question": "A ____ is a structure where multiple requests are locally combined in tree fashion, which can be used to reduce contention in barrier synchronization.",
            "options": [
                "queuing lock",
                "combining tree",
                "spin lock",
                "hardware primitive"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ is a structure where multiple requests are locally combined in tree fashion, which can be used to reduce contention in barrier synchronization.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1377,
        "doc": {
            "question_id": 123451513,
            "question": "The ____ primitive atomically fetches a variable and increments its value, which can be used to improve barrier implementations.",
            "options": [
                "fetch-and-add",
                "test-and-set",
                "compare-and-swap",
                "fetch-and-increment"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ primitive atomically fetches a variable and increments its value, which can be used to improve barrier implementations.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1378,
        "doc": {
            "question_id": 123451518,
            "question": "____ is a phenomenon in multiprocessor caches where increasing block size can lead to an increase in coherence misses.",
            "options": [
                "False sharing",
                "True sharing",
                "Cache thrashing",
                "Cache pollution"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a phenomenon in multiprocessor caches where increasing block size can lead to an increase in coherence misses.",
                "Expected Label": "A",
                "Predicted Label": "A. False Sharing \n"
            }
        ]
    },
    {
        "doc_id": 1379,
        "doc": {
            "question_id": 123451529,
            "question": "____ is a key technique used in distributed directory-based cache coherence protocols to handle resource limitations.",
            "options": [
                "NAKing requests",
                "Forwarding data",
                "Atomic operations",
                "Infinite buffering"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a key technique used in distributed directory-based cache coherence protocols to handle resource limitations.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1380,
        "doc": {
            "question_id": 123451531,
            "question": "In supercomputer architecture, placing all logic except ____ into a single chip can lead to higher density, lower power, and lower cost.",
            "options": [
                "CPUs",
                "FPUs",
                "DRAMs",
                "Caches"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In supercomputer architecture, placing all logic except ____ into a single chip can lead to higher density, lower power, and lower cost.",
                "Expected Label": "C",
                "Predicted Label": "C) DRAMs \n\n**Explanation:**\n\nWhile integrating CPUs, FPUs, and caches into a single chip contributes to density and performance, DRAMs (Dynamic Random-Access Memory) are typically kept separate in supercomputer architectures. Here's why:\n\n* **Capacity Demands:** Supercomputers require massive amounts of memory (often terabytes) to handle their complex datasets. Integrating DRAM onto the same die as processing units would severely limit memory capacity.\n* **Cost-Effectiveness:** DRAM technology differs from processor technology. Keeping them separate allows for optimized manufacturing processes and cost-effective scaling of memory capacity independently.\n* **Performance Trade-offs:**  Integrating DRAM might introduce signal integrity and latency issues due to the close proximity of high-speed processing units. \n\nTherefore, keeping DRAMs separate allows for a more scalable, cost-effective, and potentially higher-performing design in supercomputers. \n"
            }
        ]
    },
    {
        "doc_id": 1381,
        "doc": {
            "question_id": 123451533,
            "question": "In a basic hardware divider, the ____ register typically contains the remainder after the division operation is complete.",
            "options": [
                "A",
                "P",
                "B",
                "Q"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a basic hardware divider, the ____ register typically contains the remainder after the division operation is complete.",
                "Expected Label": "B",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1382,
        "doc": {
            "question_id": 123451536,
            "question": "In Booth recoding, when the current bit is 1 and the previous bit is 0, the algorithm ____ the multiplier from the partial product.",
            "options": [
                "adds",
                "subtracts",
                "ignores",
                "doubles"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In Booth recoding, when the current bit is 1 and the previous bit is 0, the algorithm ____ the multiplier from the partial product.",
                "Expected Label": "B",
                "Predicted Label": "B) subtracts \n\n**Explanation:**\n\nBooth recoding simplifies multiplication within computers by efficiently handling strings of 1s in the multiplier. Here's how the logic works for the scenario presented:\n\n* **Current bit is 1, previous bit is 0:** This represents the *beginning* of a string of 1s in the multiplier. Booth's algorithm treats this as subtracting the multiplicand at this position, effectively preparing for the string of additions that a series of 1s would normally require.\n\n**Let me know if you'd like a more detailed explanation of Booth's algorithm!** \n"
            }
        ]
    },
    {
        "doc_id": 1383,
        "doc": {
            "question_id": 123451539,
            "question": "In IEEE floating-point arithmetic, ____ is used to represent the result of computations whose value is less than the minimum normalized number.",
            "options": [
                "Infinity",
                "Denormal numbers",
                "NaN",
                "Zero"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In IEEE floating-point arithmetic, ____ is used to represent the result of computations whose value is less than the minimum normalized number.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1384,
        "doc": {
            "question_id": 123451542,
            "question": "In floating-point multiplication rounding, the ____ bit is used to record if any digits past the round digit are nonzero.",
            "options": [
                "guard",
                "sticky",
                "round",
                "significand"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In floating-point multiplication rounding, the ____ bit is used to record if any digits past the round digit are nonzero.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1385,
        "doc": {
            "question_id": 123451543,
            "question": "In floating-point multiplication, the ____ is the logical OR of certain bits in the multiplication result.",
            "options": [
                "guard bit",
                "round bit",
                "sticky bit",
                "sign bit"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In floating-point multiplication, the ____ is the logical OR of certain bits in the multiplication result.",
                "Expected Label": "C",
                "Predicted Label": "C \n\nThe **sticky bit** is used in floating-point multiplication (and division) to indicate if, during the alignment and normalization process, any bits were shifted out that were to the right of the guard and round bits.  It helps determine if rounding should be up or down for the most accurate result. \n"
            }
        ]
    },
    {
        "doc_id": 1386,
        "doc": {
            "question_id": 123451550,
            "question": "In floating-point division algorithms, the ____ can be used to compute the remainder.",
            "options": [
                "quotient register",
                "accumulator",
                "P register",
                "mantissa"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In floating-point division algorithms, the ____ can be used to compute the remainder.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **D) mantissa**. Here's why:\n\n* **Mantissa:** In floating-point representation, the mantissa (or significand) holds the significant digits of a number. After dividing the mantissas of two floating-point numbers, any leftover portion represents the remainder.\n\nLet's look at why the other options are incorrect:\n\n* **Quotient register:** This register specifically stores the result of the division operation, not the remainder.\n* **Accumulator:** An accumulator is a general-purpose register used for arithmetic and logic operations. While it might temporarily hold intermediate values during division, it's not specifically used for storing the remainder.\n* **P register:** This option is not a standard register found in typical computer architectures. It's not directly related to floating-point division or remainder calculation. \n"
            }
        ]
    },
    {
        "doc_id": 1387,
        "doc": {
            "question_id": 123451553,
            "question": "In IEEE floating-point arithmetic, the ____ exception occurs when the result of an operation must be rounded or when it overflows.",
            "options": [
                "underflow",
                "overflow",
                "inexact",
                "invalid"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In IEEE floating-point arithmetic, the ____ exception occurs when the result of an operation must be rounded or when it overflows.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1388,
        "doc": {
            "question_id": 123451555,
            "question": "The number of logic levels in an optimized carry-lookahead adder is approximately ____ for n-bit addition.",
            "options": [
                "n",
                "log2n",
                "2n",
                "n/2"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The number of logic levels in an optimized carry-lookahead adder is approximately ____ for n-bit addition.",
                "Expected Label": "B",
                "Predicted Label": "B) log2n \n\n**Explanation:**\n\nCarry-lookahead adders are designed to reduce the propagation delay of the carry bits, which is the primary bottleneck in ripple-carry adders. They achieve this by using a tree-like structure to compute carry bits in parallel for groups of bits. \n\nThe number of logic levels in this tree structure is logarithmic to the number of bits (n) being added. Hence, the answer is approximately **log2n**. \n"
            }
        ]
    },
    {
        "doc_id": 1389,
        "doc": {
            "question_id": 123451559,
            "question": "In a carry-skip adder, making the ____ blocks larger will speed up the adder.",
            "options": [
                "exterior",
                "leaf",
                "interior",
                "input"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a carry-skip adder, making the ____ blocks larger will speed up the adder.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) interior**. Here's why:\n\n* **Carry-skip adders** are designed to accelerate addition by dividing the adder into blocks.  The key idea is to quickly detect if a carry generated in one block will propagate through (\"skip\") other blocks.\n\n* **Interior blocks** are the ones in the middle of the adder. If these blocks are larger:\n    * A carry has a higher chance of being absorbed *within* a block, reducing the need for carry propagation across multiple blocks.\n    * The critical path (longest delay) is often determined by carry propagation through these interior blocks.\n\nLet me know if you'd like a more detailed explanation of carry-skip adders! \n"
            }
        ]
    },
    {
        "doc_id": 1390,
        "doc": {
            "question_id": 123451561,
            "question": "In SRT division, the ____ register pair holds the remainder and is a two's complement number.",
            "options": [
                "A,B",
                "P,A",
                "R,S",
                "Q,R"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In SRT division, the ____ register pair holds the remainder and is a two's complement number.",
                "Expected Label": "B",
                "Predicted Label": "B) P,A \n\n**Explanation:**\n\nIn SRT (Sweeney, Robertson, and Toeplitz) division, the **P register** holds the **partial remainder**, and the **A register** is combined with it to form a double-word register pair. This combined register pair (P, A) stores the remainder as a two's complement number. \n\nLet me know if you'd like more details on SRT division! \n"
            }
        ]
    },
    {
        "doc_id": 1391,
        "doc": {
            "question_id": 123451565,
            "question": "The primary advantage of array multipliers over simpler designs is their ____ latency.",
            "options": [
                "higher",
                "equivalent",
                "lower",
                "variable"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The primary advantage of array multipliers over simpler designs is their ____ latency.",
                "Expected Label": "C",
                "Predicted Label": "C) lower \n"
            }
        ]
    },
    {
        "doc_id": 1392,
        "doc": {
            "question_id": 123451567,
            "question": "In SRT division, the number of bits examined from the ____ and divisor determines the speed of the divider.",
            "options": [
                "quotient",
                "carry",
                "partial remainder",
                "multiplier"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In SRT division, the number of bits examined from the ____ and divisor determines the speed of the divider.",
                "Expected Label": "C",
                "Predicted Label": "C) partial remainder \n\n**Explanation:**\n\nSRT (Sweeney, Robertson, and Tocher) division is a fast division algorithm that uses a lookup table based on the **partial remainder** and the divisor to predict quotient digits. The more bits of the partial remainder and divisor are considered during the table lookup, the more accurate the prediction, and the faster the division operation can be completed. \n"
            }
        ]
    },
    {
        "doc_id": 1393,
        "doc": {
            "question_id": 123451571,
            "question": "The primary purpose of floating-point representation in early computers was to ____ compared to fixed-point systems.",
            "options": [
                "increase computation speed",
                "retain more significant digits",
                "reduce power consumption",
                "simplify hardware design"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The primary purpose of floating-point representation in early computers was to ____ compared to fixed-point systems.",
                "Expected Label": "B",
                "Predicted Label": "B) retain more significant digits \n"
            }
        ]
    },
    {
        "doc_id": 1394,
        "doc": {
            "question_id": 123451578,
            "question": "____ is considered one of the earliest and most influential CISC architectures.",
            "options": [
                "The IBM 360/370",
                "The Intel 80x86",
                "The ARM",
                "The MIPS"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered one of the earliest and most influential CISC architectures.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1395,
        "doc": {
            "question_id": 123451583,
            "question": "In RISC architectures, the ____ field typically contains the main opcode of an instruction.",
            "options": [
                "Rs1",
                "Rd",
                "Const",
                "Op"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In RISC architectures, the ____ field typically contains the main opcode of an instruction.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1396,
        "doc": {
            "question_id": 123451590,
            "question": "____ is a technique used in some multimedia instruction set extensions where arithmetic results are clamped to the maximum or minimum representable value instead of wrapping around.",
            "options": [
                "Modulo arithmetic",
                "Saturation",
                "Vectorization",
                "Permutation"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is a technique used in some multimedia instruction set extensions where arithmetic results are clamped to the maximum or minimum representable value instead of wrapping around.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1397,
        "doc": {
            "question_id": 123451596,
            "question": "____ is a technique used in some architectures to support tagged data types for languages like LISP and Smalltalk.",
            "options": [
                "Tagged addition",
                "Register renaming",
                "Branch prediction",
                "Speculative execution"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used in some architectures to support tagged data types for languages like LISP and Smalltalk.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1398,
        "doc": {
            "question_id": 123451597,
            "question": "____ is a technique in some ISAs that allows an instruction to be skipped based on a condition, without using an explicit branch.",
            "options": [
                "Nullification",
                "Pipelining",
                "Prefetching",
                "Speculation"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique in some ISAs that allows an instruction to be skipped based on a condition, without using an explicit branch.",
                "Expected Label": "A",
                "Predicted Label": "A) Nullification \n\n**Explanation:**\n\nNullification is a technique where an instruction is fetched and decoded but its execution is conditionally cancelled or \"nullified\" before it can modify any architectural state. This allows for conditional execution without the overhead of a branch instruction.\n\nLet's look at why the other options are incorrect:\n\n* **B) Pipelining:** Pipelining is a technique for improving instruction throughput by overlapping the execution of multiple instructions. It doesn't inherently provide conditional skipping.\n* **C) Prefetching:** Prefetching brings instructions into the cache before they are needed, aiming to reduce memory access latency. It doesn't handle conditional skipping.\n* **D) Speculation:** Speculation executes instructions based on a predicted outcome, even before the outcome is confirmed. While it can improve performance, it doesn't directly provide the mechanism for skipping instructions based on a condition. \n"
            }
        ]
    },
    {
        "doc_id": 1399,
        "doc": {
            "question_id": 123451598,
            "question": "____ instructions allow arbitrary bit fields to be selected from or inserted into registers.",
            "options": [
                "Branch vectored",
                "Extract and deposit",
                "Load and clear",
                "Debug"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ instructions allow arbitrary bit fields to be selected from or inserted into registers.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1400,
        "doc": {
            "question_id": 123451604,
            "question": "The ____ architecture was inspired by the Berkeley RISC project.",
            "options": [
                "MIPS",
                "ARM",
                "SPARC",
                "PowerPC"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ architecture was inspired by the Berkeley RISC project.",
                "Expected Label": "B",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1401,
        "doc": {
            "question_id": 123451605,
            "question": "The ____ instruction allows conditional execution of an instruction based on a test condition without requiring a branch.",
            "options": [
                "Branch prediction",
                "Prefetch",
                "Conditional move",
                "Annulling delayed branch"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ instruction allows conditional execution of an instruction based on a test condition without requiring a branch.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1402,
        "doc": {
            "question_id": 123451607,
            "question": "In the 80x86 architecture, ____ addressing mode uses a register to hold the effective address of the operand.",
            "options": [
                "Absolute",
                "Register indirect",
                "Based indexed",
                "Scaled indexed"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the 80x86 architecture, ____ addressing mode uses a register to hold the effective address of the operand.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1403,
        "doc": {
            "question_id": 123451611,
            "question": "In the 80x86 floating-point architecture, ____ is used to indicate the top of stack.",
            "options": [
                "TOP",
                "ST",
                "SP",
                "FP"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In the 80x86 floating-point architecture, ____ is used to indicate the top of stack.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1404,
        "doc": {
            "question_id": 123451613,
            "question": "In x86 architecture, the result of a floating-point comparison must be transferred to the ____ before it can be tested using branch instructions.",
            "options": [
                "FPU",
                "ALU",
                "cache",
                "integer CPU"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In x86 architecture, the result of a floating-point comparison must be transferred to the ____ before it can be tested using branch instructions.",
                "Expected Label": "D",
                "Predicted Label": "D. integer CPU \n\n**Explanation:**\n\nWhile floating-point comparisons are performed by the FPU (Floating Point Unit), the comparison results (like greater than, less than, equal) need to be transferred to the integer CPU's flags register.  Branch instructions in x86 rely on the status flags in the integer CPU to determine whether to take a branch or not. \n"
            }
        ]
    },
    {
        "doc_id": 1405,
        "doc": {
            "question_id": 123451614,
            "question": "____ is the maximum possible instruction length for an 80386 instruction.",
            "options": [
                "17 bytes",
                "32 bytes",
                "8 bytes",
                "64 bytes"
            ],
            "answer": "A",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the maximum possible instruction length for an 80386 instruction.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1406,
        "doc": {
            "question_id": 123451620,
            "question": "The ____ instruction set architecture was designed to be close to programming languages in order to simplify compilers.",
            "options": [
                "RISC",
                "x86",
                "VAX",
                "ARM"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ instruction set architecture was designed to be close to programming languages in order to simplify compilers.",
                "Expected Label": "C",
                "Predicted Label": "C) VAX \n\n**Explanation:**\n\n* **VAX (Virtual Address eXtension)** was known for its complex instruction set (CISC) designed to closely resemble high-level programming languages. This approach aimed to simplify compiler design.\n\nLet's look at why the other options are incorrect:\n\n* **RISC (Reduced Instruction Set Computing)** focuses on simplifying instructions for faster execution, often at the expense of compiler complexity.\n* **x86** is also a CISC architecture, but it evolved for backward compatibility rather than focusing on compiler simplification.\n* **ARM (Advanced RISC Machine)** is a RISC architecture, prioritizing efficient instruction execution. \n"
            }
        ]
    },
    {
        "doc_id": 1407,
        "doc": {
            "question_id": 123451621,
            "question": "____ is an addressing mode where the address specified points to the location of the actual operand address, requiring an additional memory access.",
            "options": [
                "Deferred addressing",
                "Immediate addressing",
                "Direct addressing",
                "Relative addressing"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is an addressing mode where the address specified points to the location of the actual operand address, requiring an additional memory access.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1408,
        "doc": {
            "question_id": 123451625,
            "question": "____ are set as a side effect of an operation and indicate whether the result is positive, negative, or zero or if an overflow occurred.",
            "options": [
                "Condition codes",
                "Status flags",
                "Result registers",
                "Branch predictors"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are set as a side effect of an operation and indicate whether the result is positive, negative, or zero or if an overflow occurred.",
                "Expected Label": "A",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1409,
        "doc": {
            "question_id": 123451631,
            "question": "In RISC architectures like MIPS, ____ is typically used to allocate space for local variables and save registers.",
            "options": [
                "The heap",
                "The stack",
                "Global memory",
                "Cache memory"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In RISC architectures like MIPS, ____ is typically used to allocate space for local variables and save registers.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **The stack** is the primary mechanism for managing function calls and local variables in RISC architectures. It operates in a Last-In, First-Out (LIFO) manner, making it efficient for pushing and popping register values during function calls, as well as allocating and deallocating memory for local variables as functions are called and return.\n\nLet's look at why the other options are incorrect:\n\n* **The heap** is used for dynamic memory allocation, where the program requests memory as needed during runtime. \n* **Global memory** stores variables accessible from anywhere in the program, not specifically for local variables.\n* **Cache memory** is a high-speed buffer that stores frequently accessed data for faster retrieval, not for direct variable storage. \n"
            }
        ]
    },
    {
        "doc_id": 1410,
        "doc": {
            "question_id": 123451632,
            "question": "In computer architecture, ____ typically refers to the ability to perform complex operations and use varied addressing modes within a single instruction.",
            "options": [
                "Pipelining",
                "Branch prediction",
                "Superscalar execution",
                "Instruction set complexity"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ typically refers to the ability to perform complex operations and use varied addressing modes within a single instruction.",
                "Expected Label": "D",
                "Predicted Label": "The answer is **D) Instruction set complexity**. Here's why:\n\n* **Instruction set complexity** directly relates to the power and flexibility of individual instructions. A complex instruction set allows for operations like memory access with complex addressing modes or even string manipulations within a single instruction. \n\nLet's look at why the other options are not the best fit:\n\n* **A) Pipelining:**  Improves performance by overlapping the execution of multiple instructions. It doesn't directly define the complexity of a single instruction.\n* **B) Branch prediction:** A performance enhancement technique that guesses the outcome of conditional branches. Not related to instruction complexity.\n* **C) Superscalar execution:** Allows multiple instructions to be executed simultaneously. While it benefits from complex instructions, it's not a defining characteristic of instruction complexity itself. \n"
            }
        ]
    },
    {
        "doc_id": 1411,
        "doc": {
            "question_id": 123515501,
            "question": "A ____ represents one of 16 possibilities.",
            "options": [
                "byte",
                "nibble",
                "word",
                "bit"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ represents one of 16 possibilities.",
                "Expected Label": "B",
                "Predicted Label": "B \n\nHere's why:\n\n* **Nibble:** A nibble is a group of 4 bits. Since each bit can be a 0 or 1, 4 bits give you 2 x 2 x 2 x 2 = 16 possible combinations. \n\nLet me know if you'd like more explanations on the other terms! \n"
            }
        ]
    },
    {
        "doc_id": 1412,
        "doc": {
            "question_id": 123515503,
            "question": "In sign/magnitude representation of binary numbers, the ____ is used to indicate whether the number is positive or negative.",
            "options": [
                "least significant bit",
                "second most significant bit",
                "middle bit",
                "most significant bit"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In sign/magnitude representation of binary numbers, the ____ is used to indicate whether the number is positive or negative.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1413,
        "doc": {
            "question_id": 123515504,
            "question": "In two's complement representation, ____ is performed by taking the two's complement of the second number, then adding.",
            "options": [
                "Multiplication",
                "Subtraction",
                "Division",
                "Exponentiation"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In two's complement representation, ____ is performed by taking the two's complement of the second number, then adding.",
                "Expected Label": "B",
                "Predicted Label": "Answer: B \n\n**Explanation:**\n\nTwo's complement simplifies subtraction by turning it into an addition operation.  Here's how it works:\n\n1. **Take the Two's Complement:** Invert all the bits of the number you want to subtract (the subtrahend) and add 1.\n2. **Add:** Add the result from step 1 to the number you want to subtract from (the minuend).\n3. **Discard Overflow:** If there's an overflow bit (carry out from the most significant bit), discard it. \n\nThis method eliminates the need for separate circuitry for subtraction in the CPU. \n"
            }
        ]
    },
    {
        "doc_id": 1414,
        "doc": {
            "question_id": 123515510,
            "question": "The ____ is the voltage range where a digital input is neither clearly high nor low.",
            "options": [
                "Unity gain point",
                "Noise margin",
                "Transfer characteristic",
                "Forbidden zone"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is the voltage range where a digital input is neither clearly high nor low.",
                "Expected Label": "D",
                "Predicted Label": "D) Forbidden zone \n"
            }
        ]
    },
    {
        "doc_id": 1415,
        "doc": {
            "question_id": 123515512,
            "question": "The junction between p-type and n-type silicon is called a ____.",
            "options": [
                "semiconductor",
                "diode",
                "transistor",
                "capacitor"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The junction between p-type and n-type silicon is called a ____.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1416,
        "doc": {
            "question_id": 123515513,
            "question": "In a MOSFET, the ____ layer separates the gate from the substrate.",
            "options": [
                "metal",
                "polycrystalline silicon",
                "silicon dioxide",
                "silicon nitride"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a MOSFET, the ____ layer separates the gate from the substrate.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe **gate oxide** is a critical component in a MOSFET transistor. It acts as an insulator, preventing current flow between the gate and the substrate.  **Silicon dioxide (SiO2)** is the material most commonly used for this insulating layer due to its excellent electrical properties and compatibility with silicon processing techniques. \n"
            }
        ]
    },
    {
        "doc_id": 1417,
        "doc": {
            "question_id": 123515518,
            "question": "In CMOS circuits, ____ transistors are slower than nMOS transistors.",
            "options": [
                "Junction",
                "Bipolar",
                "JFET",
                "pMOS"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In CMOS circuits, ____ transistors are slower than nMOS transistors.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1418,
        "doc": {
            "question_id": 123515520,
            "question": "In CMOS transistors, ____ turn ON when the gate is 1.",
            "options": [
                "pMOS transistors",
                "nMOS transistors",
                "Both pMOS and nMOS transistors",
                "Neither pMOS nor nMOS transistors"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In CMOS transistors, ____ turn ON when the gate is 1.",
                "Expected Label": "B",
                "Predicted Label": "Answer: B \n"
            }
        ]
    },
    {
        "doc_id": 1419,
        "doc": {
            "question_id": 123515522,
            "question": "The ____ number system, developed about 4000 years ago, uses base 60 for representing numbers.",
            "options": [
                "Decimal",
                "Binary",
                "Hexadecimal",
                "Sexagesimal"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ number system, developed about 4000 years ago, uses base 60 for representing numbers.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1420,
        "doc": {
            "question_id": 123515530,
            "question": "A ____ circuit has memory and its outputs depend on both current and previous values of the inputs.",
            "options": [
                "combinational",
                "logic",
                "sequential",
                "functional"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ circuit has memory and its outputs depend on both current and previous values of the inputs.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1421,
        "doc": {
            "question_id": 123515538,
            "question": "In Boolean algebra, the ____ method involves checking all possible combinations of variables to prove a theorem.",
            "options": [
                "Partial induction",
                "Deductive reasoning",
                "Perfect induction",
                "Syllogistic logic"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Boolean algebra, the ____ method involves checking all possible combinations of variables to prove a theorem.",
                "Expected Label": "C",
                "Predicted Label": "C. Perfect Induction \n\n**Explanation:**\n\nPerfect induction, also known as proof by exhaustion, tests every single possible combination of input values in a Boolean expression to verify the truth of a theorem. This method is exhaustive but can become impractical for expressions with many variables. \n"
            }
        ]
    },
    {
        "doc_id": 1422,
        "doc": {
            "question_id": 123515539,
            "question": "In Boolean algebra, the ____ theorem allows us to duplicate terms as many times as we want in an equation.",
            "options": [
                "Distributive",
                "Associative",
                "Commutative",
                "Idempotency"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In Boolean algebra, the ____ theorem allows us to duplicate terms as many times as we want in an equation.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1423,
        "doc": {
            "question_id": 123515540,
            "question": "____ is a diagram showing the elements and wires that connect them together in a digital circuit.",
            "options": [
                "A schematic",
                "A Karnaugh map",
                "A Boolean equation",
                "An implicant"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a diagram showing the elements and wires that connect them together in a digital circuit.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1424,
        "doc": {
            "question_id": 123515543,
            "question": "____ is a technique especially helpful in analyzing and designing multilevel circuits.",
            "options": [
                "Bubble pushing",
                "Logic minimization",
                "Boolean algebra",
                "Karnaugh mapping"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique especially helpful in analyzing and designing multilevel circuits.",
                "Expected Label": "A",
                "Predicted Label": "D \n\n**Explanation:**\n\nWhile all the options relate to digital logic, Karnaugh mapping (K-map) is specifically designed for simplifying Boolean expressions, which is essential in analyzing and designing multilevel circuits. \n\nHere's why the other options aren't the best fit:\n\n* **Bubble pushing:** A technique used to manipulate logic gates, helpful in understanding and simplifying circuits but not the primary method for multilevel design.\n* **Logic minimization:** A broad term encompassing various techniques, including K-maps. K-map is a specific and widely used method for this purpose.\n* **Boolean algebra:** The foundation of digital logic, used for representing and manipulating logic expressions. While essential, it's not the most efficient method for multilevel circuit analysis and design compared to K-maps. \n"
            }
        ]
    },
    {
        "doc_id": 1425,
        "doc": {
            "question_id": 123515547,
            "question": "In computer architecture, ____ is a graphical method for simplifying Boolean equations that works well for problems with up to four variables.",
            "options": [
                "Boolean algebra",
                "Karnaugh maps",
                "Truth tables",
                "Logic gates"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ is a graphical method for simplifying Boolean equations that works well for problems with up to four variables.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1426,
        "doc": {
            "question_id": 123515548,
            "question": "In logic minimization, two terms containing an implicant P and the true and complementary forms of some variable A can be combined to eliminate ____.",
            "options": [
                "P",
                "The implicant",
                "A",
                "The complementary form"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In logic minimization, two terms containing an implicant P and the true and complementary forms of some variable A can be combined to eliminate ____.",
                "Expected Label": "C",
                "Predicted Label": "C) A \n\n**Explanation:**\n\nIn logic minimization techniques like Karnaugh maps, combining terms with a variable and its complement allows you to *eliminate that variable*. This is based on the Boolean algebra principle of:\n\nP + (A * B) + (A' * B) = P + B \n\nThe variable A, present in both true and complemented forms, is removed in the simplified expression. \n"
            }
        ]
    },
    {
        "doc_id": 1427,
        "doc": {
            "question_id": 123515549,
            "question": "In K-map minimization, each circle represents a ____ implicant.",
            "options": [
                "minimal",
                "maximal",
                "complementary",
                "prime"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In K-map minimization, each circle represents a ____ implicant.",
                "Expected Label": "D",
                "Predicted Label": "D) prime \n\n**Explanation:**\n\nK-maps are used to visually identify **prime implicants**, which are product terms (or sum terms for POS forms) that are essential and cannot be further simplified. They are represented by circles encompassing the largest possible group of adjacent '1's in the K-map. \n"
            }
        ]
    },
    {
        "doc_id": 1428,
        "doc": {
            "question_id": 123515551,
            "question": "In a Karnaugh map, circles used to identify prime implicants can ____ the edges of the map.",
            "options": [
                "never cross",
                "wrap around",
                "only touch",
                "always bisect"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a Karnaugh map, circles used to identify prime implicants can ____ the edges of the map.",
                "Expected Label": "B",
                "Predicted Label": "B) wrap around \n"
            }
        ]
    },
    {
        "doc_id": 1429,
        "doc": {
            "question_id": 123515554,
            "question": "____ is a method used for logic simplification in combinational logic design.",
            "options": [
                "Boolean algebra",
                "Differential calculus",
                "Linear programming",
                "Quantum computing"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a method used for logic simplification in combinational logic design.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1430,
        "doc": {
            "question_id": 123515556,
            "question": "An N:1 multiplexer requires ____ select lines.",
            "options": [
                "N",
                "N/2",
                "log2N",
                "2^N"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "An N:1 multiplexer requires ____ select lines.",
                "Expected Label": "C",
                "Predicted Label": "C) log2N \n\n**Explanation:**\n\nA multiplexer (MUX) selects one of several input lines and forwards the selected input to a single output line. The number of select lines determines how many inputs the MUX can choose from. Since each select line can have two states (0 or 1), you need log\u2082N select lines to represent N different input options. \n"
            }
        ]
    },
    {
        "doc_id": 1431,
        "doc": {
            "question_id": 123515557,
            "question": "An ____ can be reprogrammed to perform different logic functions by changing its data inputs.",
            "options": [
                "AND gate",
                "OR gate",
                "XOR gate",
                "multiplexer"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "An ____ can be reprogrammed to perform different logic functions by changing its data inputs.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1432,
        "doc": {
            "question_id": 123515558,
            "question": "A ____ is a combinational logic element that selects one of several input signals and forwards it to a single output.",
            "options": [
                "multiplexer",
                "decoder",
                "flip-flop",
                "adder"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ is a combinational logic element that selects one of several input signals and forwards it to a single output.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1433,
        "doc": {
            "question_id": 123515560,
            "question": "The ____ is the point at which a signal is half-way between its LOW and HIGH values during a transition.",
            "options": [
                "contamination point",
                "propagation point",
                "50% point",
                "delay point"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is the point at which a signal is half-way between its LOW and HIGH values during a transition.",
                "Expected Label": "C",
                "Predicted Label": "**C) 50% point** \n\nThe 50% point is the standard reference point used to measure signal rise and fall times in digital circuits. \n"
            }
        ]
    },
    {
        "doc_id": 1434,
        "doc": {
            "question_id": 123515563,
            "question": "A circuit is considered ____ when the critical path is from the control signals to the output.",
            "options": [
                "data critical",
                "control critical",
                "time critical",
                "path critical"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A circuit is considered ____ when the critical path is from the control signals to the output.",
                "Expected Label": "B",
                "Predicted Label": "The answer is **B) control critical**. Here's why:\n\n* **Control critical** circuits are defined by the control signals dictating the overall speed. The critical path (the longest delay path determining clock speed) originates from the control signals and impacts when the output becomes valid.\n\nLet's look at why the other options aren't the best fit:\n\n* **Data critical:**  While data flow is important, this term isn't specifically used to describe the scenario where control signals dominate the critical path.\n* **Time critical:** This is a more general term and doesn't specifically relate to the control signals being the limiting factor in circuit speed.\n* **Path critical:**  While technically all circuits have a critical path, this term doesn't highlight the specific importance of control signals in this situation. \n"
            }
        ]
    },
    {
        "doc_id": 1435,
        "doc": {
            "question_id": 123515567,
            "question": "A ____ sets one of its outputs HIGH according to its input.",
            "options": [
                "multiplexer",
                "priority circuit",
                "decoder",
                "critical path"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ sets one of its outputs HIGH according to its input.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Decoders** take a binary input and activate a corresponding single output line. For example, a 2-bit decoder would have 4 output lines (00, 01, 10, 11), and only one output would be HIGH depending on the input. \n\nLet's look at why the other options are incorrect:\n\n* **Multiplexers (A)** select *one* input from multiple inputs and forward it to a single output.\n* **Priority circuits (B)** identify the highest priority input (among multiple inputs) and output that.\n* **Critical path (D)** refers to the longest path in a circuit that determines its maximum clock speed. \n"
            }
        ]
    },
    {
        "doc_id": 1436,
        "doc": {
            "question_id": 123515568,
            "question": "____ is a fundamental theorem in Boolean algebra that states the negation of a conjunction is the disjunction of the negations.",
            "options": [
                "De Morgan's Theorem",
                "The distributivity theorem",
                "The combining theorem",
                "The idempotency theorem"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a fundamental theorem in Boolean algebra that states the negation of a conjunction is the disjunction of the negations.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1437,
        "doc": {
            "question_id": 123515570,
            "question": "In Boolean logic, ____ entries in a truth table can be used to simplify logic equations.",
            "options": [
                "Redundant",
                "Conflicting",
                "Don't care",
                "Incomplete"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Boolean logic, ____ entries in a truth table can be used to simplify logic equations.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1438,
        "doc": {
            "question_id": 123515573,
            "question": "A ____ is used to create high-impedance outputs in digital circuits.",
            "options": [
                "multiplexer",
                "decoder",
                "tristate buffer",
                "flip-flop"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ is used to create high-impedance outputs in digital circuits.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1439,
        "doc": {
            "question_id": 123515581,
            "question": "A ____ node in a latch design is susceptible to noise and charge leakage over time.",
            "options": [
                "static",
                "buffered",
                "floating",
                "inverted"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ node in a latch design is susceptible to noise and charge leakage over time.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) floating**. Here's why:\n\n* **Floating nodes** are nodes in a circuit that are not actively driven to a specific logic level (0 or 1). This means their voltage can fluctuate due to external noise or internal charge leakage, making them unreliable for storing data.\n\nLet's look at the other options:\n\n* **A) Static nodes:** These nodes are actively held at a specific logic level, making them less susceptible to noise.\n* **B) Buffered nodes:** Buffers strengthen signals, making them more resistant to noise.\n* **D) Inverted nodes:**  Inversion just flips the logic level; it doesn't inherently make a node more or less susceptible to noise. \n\n**Therefore, a floating node is the most susceptible to noise and charge leakage in a latch design.** \n"
            }
        ]
    },
    {
        "doc_id": 1440,
        "doc": {
            "question_id": 123515583,
            "question": "____ circuits are sequential circuits that have no stable states.",
            "options": [
                "Astable",
                "Synchronous",
                "Combinational",
                "Latching"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ circuits are sequential circuits that have no stable states.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1441,
        "doc": {
            "question_id": 123515585,
            "question": "A ____ is the simplest synchronous sequential circuit.",
            "options": [
                "Multiplexer",
                "Adder",
                "Flip-flop",
                "Decoder"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ is the simplest synchronous sequential circuit.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Flip-flops** are the fundamental building blocks of synchronous sequential circuits. They are memory elements that hold and change their state based on clock signals, making them ideal for creating circuits where the output depends on both the current input and the previous state.\n\nLet's break down why the other options are incorrect:\n\n* **Multiplexers** are combinational circuits that select one of several input signals and forward it to a single output.\n* **Adders** are combinational circuits that perform addition on binary numbers.\n* **Decoders** are combinational circuits that convert coded inputs into decoded outputs, often used for selecting specific memory locations. \n \n**Therefore, the correct answer is (C).** \n"
            }
        ]
    },
    {
        "doc_id": 1442,
        "doc": {
            "question_id": 123515588,
            "question": "In a finite state machine, the ____ determines when state transitions occur.",
            "options": [
                "reset button",
                "clock",
                "sensor",
                "output"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a finite state machine, the ____ determines when state transitions occur.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1443,
        "doc": {
            "question_id": 123515593,
            "question": "In a one-____ encoding, K states are represented with K bits, exactly one of which is FALSE.",
            "options": [
                "hot",
                "warm",
                "cool",
                "cold"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a one-____ encoding, K states are represented with K bits, exactly one of which is FALSE.",
                "Expected Label": "D",
                "Predicted Label": "D) cold \n"
            }
        ]
    },
    {
        "doc_id": 1444,
        "doc": {
            "question_id": 123515594,
            "question": "____ encoding uses a single bit to represent each state in a finite state machine.",
            "options": [
                "One-hot",
                "Binary",
                "Gray code",
                "Decimal"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ encoding uses a single bit to represent each state in a finite state machine.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1445,
        "doc": {
            "question_id": 123515596,
            "question": "In a Moore state machine, the output is determined solely by the ____ state.",
            "options": [
                "previous",
                "next",
                "input",
                "current"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a Moore state machine, the output is determined solely by the ____ state.",
                "Expected Label": "D",
                "Predicted Label": "D) current \n"
            }
        ]
    },
    {
        "doc_id": 1446,
        "doc": {
            "question_id": 123515597,
            "question": "____ is a technique used to break down complex finite state machines into simpler interacting state machines.",
            "options": [
                "Factoring",
                "Pipelining",
                "Multiplexing",
                "Parallelization"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a technique used to break down complex finite state machines into simpler interacting state machines.",
                "Expected Label": "A",
                "Predicted Label": "A) Factoring \n\n**Factoring** is the process of decomposing a complex FSM into smaller, manageable FSMs that operate in parallel or hierarchically. \n\nLet's break down why the other options are incorrect:\n\n* **Pipelining:** Improves performance by overlapping execution of instructions, not breaking down state machines.\n* **Multiplexing:** Selecting one input from multiple inputs, irrelevant to FSM decomposition.\n* **Parallelization:**  Focuses on executing tasks concurrently, not simplifying FSM design. \n"
            }
        ]
    },
    {
        "doc_id": 1447,
        "doc": {
            "question_id": 123515600,
            "question": "The ____ states that the inputs of a synchronous sequential circuit must be stable during the setup and hold aperture time around the clock edge.",
            "options": [
                "static discipline",
                "dynamic discipline",
                "timing specification",
                "functional specification"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ states that the inputs of a synchronous sequential circuit must be stable during the setup and hold aperture time around the clock edge.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\nThe **dynamic discipline** governs the timing relationships between different signals in a synchronous system. It specifically addresses the requirement for stable inputs (data) within a window around the clock edge to ensure proper data capture by flip-flops.  \n\nLet's break down why the other options are incorrect:\n\n* **A) Static discipline:**  Focuses on the logical levels of signals (e.g., voltage thresholds for 0 and 1) rather than timing.\n* **C) Timing specification:** A broader term referring to all timing requirements of a circuit, including but not limited to setup and hold times.\n* **D) Functional specification:**  Describes *what* a circuit does, not *how* it achieves it in terms of timing. \n"
            }
        ]
    },
    {
        "doc_id": 1448,
        "doc": {
            "question_id": 123515601,
            "question": "The ____ is the time that must elapse after a clock edge before a flip-flop's input can change without affecting the captured value.",
            "options": [
                "setup time",
                "propagation delay",
                "clock-to-Q delay",
                "hold time"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is the time that must elapse after a clock edge before a flip-flop's input can change without affecting the captured value.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1449,
        "doc": {
            "question_id": 123515603,
            "question": "____ are elements used in some high-performance microprocessors that behave like flip-flops but have a short clock-to-Q delay and a long hold time.",
            "options": [
                "Latches",
                "Registers",
                "Pulsed latches",
                "Buffers"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ are elements used in some high-performance microprocessors that behave like flip-flops but have a short clock-to-Q delay and a long hold time.",
                "Expected Label": "C",
                "Predicted Label": "C) Pulsed latches \n"
            }
        ]
    },
    {
        "doc_id": 1450,
        "doc": {
            "question_id": 123515605,
            "question": "____ is considered when performing timing analysis to ensure a circuit will work under all circumstances.",
            "options": [
                "Worst-case scenario",
                "Best-case scenario",
                "Average-case scenario",
                "Typical-case scenario"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is considered when performing timing analysis to ensure a circuit will work under all circumstances.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1451,
        "doc": {
            "question_id": 123515606,
            "question": "____ is intentionally increased in some flip-flop designs to mitigate hold time failures in circuits with substantial clock skew.",
            "options": [
                "Setup time",
                "Clock-to-Q delay",
                "Propagation delay",
                "Clock period"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is intentionally increased in some flip-flop designs to mitigate hold time failures in circuits with substantial clock skew.",
                "Expected Label": "B",
                "Predicted Label": "B) Clock-to-Q delay \n"
            }
        ]
    },
    {
        "doc_id": 1452,
        "doc": {
            "question_id": 123515610,
            "question": "The ____ is the reciprocal of the probability that the system will fail in any given second.",
            "options": [
                "System failure rate",
                "Mean time between failures",
                "Resolution time",
                "Synchronizer clock period"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ is the reciprocal of the probability that the system will fail in any given second.",
                "Expected Label": "B",
                "Predicted Label": "B) Mean time between failures \n\n**Explanation:**\n\n* **Mean Time Between Failures (MTBF)** represents the average time a system runs smoothly before a failure occurs. A higher MTBF indicates better reliability. The reciprocal of MTBF would indeed give you the probability of failure in a given time unit. \n\nLet's look at why the other options are incorrect:\n\n* **System failure rate** directly measures how often a system fails and is the inverse of MTBF.\n* **Resolution time** focuses on how long it takes to fix a system after a failure.\n* **Synchronizer clock period** is relevant to digital circuit design and clock synchronization, not system reliability in a broader sense. \n"
            }
        ]
    },
    {
        "doc_id": 1453,
        "doc": {
            "question_id": 123515612,
            "question": "The time required for a bistable device to resolve from a near-metastable state is called the ____ time.",
            "options": [
                "settling",
                "stabilization",
                "convergence",
                "resolution"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The time required for a bistable device to resolve from a near-metastable state is called the ____ time.",
                "Expected Label": "D",
                "Predicted Label": "A) settling \n"
            }
        ]
    },
    {
        "doc_id": 1454,
        "doc": {
            "question_id": 123515613,
            "question": "____ is a measure of the number of tokens that can be produced per unit time in a system.",
            "options": [
                "Throughput",
                "Latency",
                "Resolution time",
                "Gain"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a measure of the number of tokens that can be produced per unit time in a system.",
                "Expected Label": "A",
                "Predicted Label": "A) Throughput \n\n**Explanation:**\n\n* **Throughput** directly refers to the amount of work completed in a given time frame.  In this case, the \"work\" is the production of tokens.\n\nLet's look at why the other options are incorrect:\n\n* **Latency:** Measures the delay between a request and its completion (how long it takes for one token to be produced).\n* **Resolution time:**  Typically refers to the time taken to resolve an issue or complete a task, not directly tied to a continuous production rate.\n* **Gain:**  Usually refers to an increase in signal strength or amplitude, not relevant in this context. \n"
            }
        ]
    },
    {
        "doc_id": 1455,
        "doc": {
            "question_id": 123515616,
            "question": "The primary limitation of applying parallelism techniques in computer architecture is ____.",
            "options": [
                "Power consumption",
                "Heat dissipation",
                "Clock speed",
                "Dependencies"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary limitation of applying parallelism techniques in computer architecture is ____.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nWhile power consumption, heat dissipation, and clock speed are all important factors in computer architecture, **dependencies** are the primary limitation for parallelism. Here's why:\n\n* **Dependencies:** Instructions or tasks often rely on the results of previous instructions.  Parallelism is limited by these dependencies, as you cannot execute instructions simultaneously if they rely on each other's output. \n* **Other options:**\n    * **Power consumption and Heat dissipation:**  These are significant concerns in parallel systems but are often manageable through design and cooling techniques. They don't represent the fundamental limitation that dependencies do.\n    * **Clock speed:** Clock speed limitations affect both parallel and non-parallel architectures. While increasing clock speed can improve performance, it doesn't address the core issue of dependencies limiting how much work can be done concurrently. \n"
            }
        ]
    },
    {
        "doc_id": 1456,
        "doc": {
            "question_id": 123515617,
            "question": "____ is a powerful technique for designing sequential circuits that involves identifying inputs, outputs, and states of the machine.",
            "options": [
                "Finite state machine",
                "Pipelining",
                "Register transfer",
                "Logic synthesis"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a powerful technique for designing sequential circuits that involves identifying inputs, outputs, and states of the machine.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) Finite state machine**. Here's why:\n\n* **Finite State Machine (FSM):**  At its core, an FSM is a mathematical model used to represent sequential logic. It defines a system with a finite number of states, transitions between those states based on inputs, and outputs associated with those states. This perfectly aligns with the question's description.\n\nLet's look at why the other options are not the best fit:\n\n* **Pipelining:** A technique to improve performance by overlapping the execution of multiple instructions.\n* **Register Transfer:** A level of abstraction used to describe the movement of data between registers and the operations performed on them.\n* **Logic Synthesis:** The process of converting a high-level description of a circuit into a gate-level implementation. \n\n**In summary:** While the other options relate to digital logic and computer architecture, the most direct and accurate answer to the question's description is a Finite State Machine. \n"
            }
        ]
    },
    {
        "doc_id": 1457,
        "doc": {
            "question_id": 123515623,
            "question": "In synchronous digital systems, the ____ time of a flip-flop determines how long input data must be stable before the clock edge.",
            "options": [
                "hold",
                "setup",
                "propagation",
                "contamination"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In synchronous digital systems, the ____ time of a flip-flop determines how long input data must be stable before the clock edge.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1458,
        "doc": {
            "question_id": 123515624,
            "question": "The primary purpose of ____ in digital design is to increase throughput by allowing multiple instructions to be processed simultaneously in different stages.",
            "options": [
                "Multiplexing",
                "Demultiplexing",
                "Parallelism",
                "Pipelining"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The primary purpose of ____ in digital design is to increase throughput by allowing multiple instructions to be processed simultaneously in different stages.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1459,
        "doc": {
            "question_id": 123515625,
            "question": "____ is typically considered more verbose and cumbersome compared to Verilog.",
            "options": [
                "SystemVerilog",
                "VHDL",
                "SystemC",
                "Chisel"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically considered more verbose and cumbersome compared to Verilog.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1460,
        "doc": {
            "question_id": 123515629,
            "question": "In computer architecture, ____ refers to the ordering of bits in a multi-byte data type.",
            "options": [
                "Byte ordering",
                "Endianness",
                "Bit significance",
                "Data alignment"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In computer architecture, ____ refers to the ordering of bits in a multi-byte data type.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1461,
        "doc": {
            "question_id": 123515633,
            "question": "In VHDL, a ____ signal assignment statement can be used as a shorthand when selecting from one of several possibilities in a multiplexer design.",
            "options": [
                "conditional",
                "selected",
                "nested",
                "parallel"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In VHDL, a ____ signal assignment statement can be used as a shorthand when selecting from one of several possibilities in a multiplexer design.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1462,
        "doc": {
            "question_id": 123515637,
            "question": "In Verilog, if a gate receives a ____ input, it may produce an x output when it can't determine the correct output value.",
            "options": [
                "valid",
                "binary",
                "floating",
                "defined"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In Verilog, if a gate receives a ____ input, it may produce an x output when it can't determine the correct output value.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) floating**. Here's why:\n\n* **Floating** inputs in Verilog represent an unknown or undefined state. This is different from a logic '0' or '1'.  When a gate receives a floating input, it can't definitively determine the correct output, leading to an 'x' (unknown) output.\n\nLet's look at the other options:\n\n* **A) valid:**  This is too general. A valid input could be '0', '1', or in some cases, even 'x'. \n* **B) binary:** All digital logic works with binary values ('0' or '1').  This option doesn't specifically address the situation of an unknown input.\n* **D) defined:** A defined input implies a known value ('0' or '1'), which wouldn't result in an 'x' output. \n\n**In summary:**  Floating inputs represent uncertainty, and that uncertainty propagates through logic gates, potentially leading to 'x' outputs. \n"
            }
        ]
    },
    {
        "doc_id": 1463,
        "doc": {
            "question_id": 123515640,
            "question": "In VHDL, the ____ function is used to convert STD_LOGIC_VECTOR to INTEGER for positive (unsigned) values.",
            "options": [
                "TO_INTEGER",
                "CONV_INTEGER",
                "STD_TO_INT",
                "VECTOR_TO_INT"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In VHDL, the ____ function is used to convert STD_LOGIC_VECTOR to INTEGER for positive (unsigned) values.",
                "Expected Label": "B",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1464,
        "doc": {
            "question_id": 123515641,
            "question": "____ is a special port type in VHDL that behaves as an output but may also be used within the module.",
            "options": [
                "inout",
                "signal",
                "buffer",
                "generic"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Interconnection Networks"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ is a special port type in VHDL that behaves as an output but may also be used within the module.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1465,
        "doc": {
            "question_id": 123515645,
            "question": "____ are used to describe combinational logic in hardware description languages.",
            "options": [
                "Always statements",
                "Process statements",
                "Continuous assignment statements",
                "Sensitivity lists"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "____ are used to describe combinational logic in hardware description languages.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1466,
        "doc": {
            "question_id": 123515649,
            "question": "In a synchronizer circuit, the ____ signal is typically used to copy data from one flip-flop to another.",
            "options": [
                "reset",
                "enable",
                "clock",
                "data"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a synchronizer circuit, the ____ signal is typically used to copy data from one flip-flop to another.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\nThe **clock** signal is the heart of a synchronizer circuit.  Here's why:\n\n* **Synchronization:** Synchronizers are designed to transfer data between different clock domains safely. The clock signal of the receiving domain is used to sample the data coming from the source domain, ensuring the data is captured correctly and avoids metastability issues.\n\nLet's look at why the other options are incorrect:\n\n* **Reset (A):** While reset initializes flip-flops, it's not used for the actual data transfer within a synchronizer.\n* **Enable (B):** An enable signal might control whether data *passes through* a flip-flop, but it doesn't directly transfer data between them.\n* **Data (D):** This is the information being transferred, not the signal that triggers the transfer. \n"
            }
        ]
    },
    {
        "doc_id": 1467,
        "doc": {
            "question_id": 123515650,
            "question": "In Verilog, variables that appear on the left hand side of an assignment within an always statement must be declared as ____.",
            "options": [
                "wire",
                "input",
                "output",
                "reg"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In Verilog, variables that appear on the left hand side of an assignment within an always statement must be declared as ____.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1468,
        "doc": {
            "question_id": 123515651,
            "question": "____ is typically used to model combinational logic in HDLs.",
            "options": [
                "Blocking assignments",
                "Nonblocking assignments",
                "Sequential assignments",
                "Parallel assignments"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to model combinational logic in HDLs.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1469,
        "doc": {
            "question_id": 123515652,
            "question": "In hardware description languages, ____ are commonly used to describe large blocks of combinational logic in a more abstract and less error-prone way compared to explicit gate-level descriptions.",
            "options": [
                "Flip-flops",
                "State machines",
                "Case statements",
                "Latches"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In hardware description languages, ____ are commonly used to describe large blocks of combinational logic in a more abstract and less error-prone way compared to explicit gate-level descriptions.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1470,
        "doc": {
            "question_id": 123515655,
            "question": "____ is used in Verilog to describe truth tables with don't cares.",
            "options": [
                "case",
                "casez",
                "if",
                "always"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is used in Verilog to describe truth tables with don't cares.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1471,
        "doc": {
            "question_id": 123515657,
            "question": "In VHDL, ____ are used to model simple combinational logic outside of process statements.",
            "options": [
                "Blocking assignments",
                "Nonblocking assignments",
                "Sequential statements",
                "Concurrent assignments"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In VHDL, ____ are used to model simple combinational logic outside of process statements.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1472,
        "doc": {
            "question_id": 123515658,
            "question": "____ assignments in hardware description languages use the most up-to-date values of variables when computing outputs.",
            "options": [
                "Blocking",
                "Nonblocking",
                "Concurrent",
                "Sequential"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ assignments in hardware description languages use the most up-to-date values of variables when computing outputs.",
                "Expected Label": "A",
                "Predicted Label": "**B) Nonblocking** \n\n**Explanation:**\n\n* **Nonblocking assignments** in HDLs (like Verilog and VHDL) are specifically designed to model the parallel nature of hardware. They use the principle of \"evaluate now, assign later.\" This means they grab the current values of variables, compute the output *without waiting*, and then schedule the assignment to happen later (typically at the end of a simulation time step). \n\nLet's look at why the other options aren't the best fit:\n\n* **Blocking assignments:** These happen sequentially. One assignment must fully complete before the next one starts. This doesn't reflect how simultaneous operations occur in hardware.\n* **Concurrent:**  This is a general term in HDLs describing how statements execute without a predefined order. While nonblocking assignments are concurrent, the term itself doesn't specifically address the use of up-to-date values.\n* **Sequential:** This implies a specific order of execution, which doesn't guarantee the use of the most recent values in a parallel hardware context. \n"
            }
        ]
    },
    {
        "doc_id": 1473,
        "doc": {
            "question_id": 123515663,
            "question": "____ is typically used to describe combinational logic in hardware description languages.",
            "options": [
                "Nonblocking assignments",
                "Blocking assignments",
                "Concurrent assignments",
                "Asynchronous assignments"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically used to describe combinational logic in hardware description languages.",
                "Expected Label": "B",
                "Predicted Label": "**B) Blocking assignments** \n\n**Explanation:**\n\n* **Blocking assignments** are executed sequentially, meaning the statement completes fully (including any updates to variables) before the next one begins. This sequential execution mirrors the way combinational logic works, where outputs are determined directly from the current input values.\n\nLet's look at why the other options are less suitable:\n\n* **Nonblocking assignments (A):** Used for sequential logic (like flip-flops), where assignments happen concurrently with other statements and updates occur at specific clock edges. \n* **Concurrent assignments (C):** This is a general term and doesn't specifically imply the sequential nature crucial for describing combinational logic. \n* **Asynchronous assignments (D):** This implies events happening independent of a clock signal, which might be true for some aspects of hardware but doesn't accurately describe the deterministic input-output relationship in combinational logic. \n"
            }
        ]
    },
    {
        "doc_id": 1474,
        "doc": {
            "question_id": 123515665,
            "question": "In VHDL, the ____ keyword is used to override default parameters when instantiating a module.",
            "options": [
                "generic",
                "parameter",
                "override",
                "generic map"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In VHDL, the ____ keyword is used to override default parameters when instantiating a module.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1475,
        "doc": {
            "question_id": 123515675,
            "question": "The ____ is a sequential logic element that can be implemented using an HDL module and has two inputs that determine its next state based on the current state.",
            "options": [
                "D latch",
                "T flip-flop",
                "RS latch",
                "JK flip-flop"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ is a sequential logic element that can be implemented using an HDL module and has two inputs that determine its next state based on the current state.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1476,
        "doc": {
            "question_id": 123515682,
            "question": "In a prefix adder, the main challenge is to rapidly compute all the block ____ signals.",
            "options": [
                "propagate",
                "sum",
                "carry",
                "generate"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a prefix adder, the main challenge is to rapidly compute all the block ____ signals.",
                "Expected Label": "D",
                "Predicted Label": "D) generate \n\n**Explanation:**\n\nIn prefix adders, the primary challenge is to efficiently compute the \"generate\" signals for all the blocks. These generate signals indicate whether a block will produce a carry based on its input bits alone.  Quickly calculating these signals is crucial for achieving parallelism in carry propagation and ultimately speeding up the addition process. \n"
            }
        ]
    },
    {
        "doc_id": 1477,
        "doc": {
            "question_id": 123515685,
            "question": "The ____ forms the heart of most computer systems.",
            "options": [
                "Cache",
                "Register File",
                "Memory Controller",
                "ALU"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ forms the heart of most computer systems.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nThe **ALU (Arithmetic Logic Unit)** is responsible for performing arithmetic and logical operations, which are fundamental to the functioning of a computer.  While other options play crucial roles:\n\n* **Cache** speeds up memory access.\n* **Register File** stores frequently used data.\n* **Memory Controller** manages data transfer to and from memory.\n\nNone of them are as central to the core processing as the ALU. \n"
            }
        ]
    },
    {
        "doc_id": 1478,
        "doc": {
            "question_id": 123515688,
            "question": "In a rotator operation, bits shifted off one end of the number are ____ to fill empty spots at the other end.",
            "options": [
                "discarded",
                "complemented",
                "wrapped around",
                "zero-extended"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a rotator operation, bits shifted off one end of the number are ____ to fill empty spots at the other end.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1479,
        "doc": {
            "question_id": 123515692,
            "question": "____ is the bias used in 32-bit floating-point representation according to the IEEE 754 standard.",
            "options": [
                "127",
                "128",
                "255",
                "256"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the bias used in 32-bit floating-point representation according to the IEEE 754 standard.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 1480,
        "doc": {
            "question_id": 123515694,
            "question": "The ____ is typically distinct from the central processing unit (CPU) and is used to perform floating-point arithmetic operations quickly in hardware.",
            "options": [
                "ALU",
                "Cache",
                "FPU",
                "MMU"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ is typically distinct from the central processing unit (CPU) and is used to perform floating-point arithmetic operations quickly in hardware.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **C) FPU**.\n\nHere's why:\n\n* **FPU (Floating Point Unit)** is specifically designed to handle complex floating-point calculations (numbers with decimals) much faster than a general-purpose CPU could achieve.\n\nLet's break down the other options:\n\n* **ALU (Arithmetic Logic Unit)**: This unit within the CPU handles basic arithmetic and logic operations on integers. \n* **Cache**:  A small, fast memory that stores frequently accessed data to speed up CPU operations. It doesn't perform calculations itself.\n* **MMU (Memory Management Unit)**:  Manages memory allocation, protection, and translation between logical and physical addresses. \n\n**Therefore, the FPU is the component dedicated to accelerating floating-point arithmetic.** \n"
            }
        ]
    },
    {
        "doc_id": 1481,
        "doc": {
            "question_id": 123515695,
            "question": "The output of an N-bit binary counter cycles through ____ possible values.",
            "options": [
                "N",
                "N^2",
                "2^N - 1",
                "2^N"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Modeling and Simulation"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The output of an N-bit binary counter cycles through ____ possible values.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1482,
        "doc": {
            "question_id": 123568796,
            "question": "____ can be used to implement combinational logic functions by storing pre-computed output values for all possible input combinations.",
            "options": [
                "Registers",
                "Lookup tables",
                "Flip-flops",
                "Multiplexers"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Parallel/Spatial or Emerging Architectures"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ can be used to implement combinational logic functions by storing pre-computed output values for all possible input combinations.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1483,
        "doc": {
            "question_id": 123568804,
            "question": "____ is a type of volatile memory that is faster but requires more transistors compared to DRAM.",
            "options": [
                "SRAM",
                "ROM",
                "FPGA",
                "PLA"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a type of volatile memory that is faster but requires more transistors compared to DRAM.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **SRAM (Static RAM)** is known for its speed and is often used in CPU caches due to its faster access times compared to DRAM. However, it requires more transistors per bit, making it more expensive and less dense. \n\nLet's look at why the other options are incorrect:\n\n* **ROM (Read-Only Memory)** is non-volatile, meaning it retains data even when powered off.\n* **FPGA (Field-Programmable Gate Array)** and **PLA (Programmable Logic Array)** are not memory types but rather configurable logic circuits. \n"
            }
        ]
    },
    {
        "doc_id": 1484,
        "doc": {
            "question_id": 123568806,
            "question": "When adding two floating-point numbers, the number with the ____ exponent is shifted.",
            "options": [
                "larger",
                "equal",
                "normalized",
                "smaller"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "When adding two floating-point numbers, the number with the ____ exponent is shifted.",
                "Expected Label": "D",
                "Predicted Label": "D) smaller \n\n**Explanation:**\n\nBefore adding floating-point numbers, their exponents must match. This is achieved by shifting the significand (mantissa) of the number with the smaller exponent to the right. This effectively aligns the decimal points of the two numbers, allowing for accurate addition of the significands. \n"
            }
        ]
    },
    {
        "doc_id": 1485,
        "doc": {
            "question_id": 123568807,
            "question": "____ is typically used to represent the exponent in IEEE 754 single-precision floating-point format.",
            "options": [
                "Biased notation",
                "Two's complement",
                "One's complement",
                "Sign-magnitude"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to represent the exponent in IEEE 754 single-precision floating-point format.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nIEEE 754 uses **biased notation** (specifically, an excess-127 bias for single-precision) to represent the exponent. \n\nHere's why the other options are incorrect:\n\n* **Two's complement** and **one's complement** are methods for representing signed integers.\n* **Sign-magnitude** uses a dedicated sign bit and represents the magnitude directly, which is less efficient for exponent representation in floating-point numbers. \n"
            }
        ]
    },
    {
        "doc_id": 1486,
        "doc": {
            "question_id": 123568815,
            "question": "In MIPS architecture, registers beginning with ____ are used for storing temporary variables.",
            "options": [
                "$s",
                "$v",
                "$a",
                "$t"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, registers beginning with ____ are used for storing temporary variables.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1487,
        "doc": {
            "question_id": 123568816,
            "question": "____ is used to store procedure return values in the MIPS architecture.",
            "options": [
                "$v0\u2013$v1",
                "$a0\u2013$a3",
                "$t0\u2013$t7",
                "$s0\u2013$s7"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is used to store procedure return values in the MIPS architecture.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1488,
        "doc": {
            "question_id": 123568826,
            "question": "In MIPS architecture, the ____ instruction is used for combining bits from two registers.",
            "options": [
                "AND",
                "XOR",
                "NOR",
                "OR"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, the ____ instruction is used for combining bits from two registers.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1489,
        "doc": {
            "question_id": 123568827,
            "question": "In MIPS architecture, the ____ instruction is used for assigning 16-bit constants.",
            "options": [
                "lui",
                "addi",
                "ori",
                "sllv"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, the ____ instruction is used for assigning 16-bit constants.",
                "Expected Label": "B",
                "Predicted Label": "**C) ori** \n\n**Explanation:**\n\n* **lui (load upper immediate):** Loads a 16-bit constant into the **upper** 16 bits of a register, setting the lower 16 bits to zero. \n* **addi (add immediate):** Adds a 16-bit signed constant to a register.\n* **ori (or immediate):** Performs a bitwise OR of a 16-bit constant with a register. This is useful for setting specific bits within a register, effectively allowing you to assign a 16-bit constant.\n* **sllv (shift left logical variable):**  Shifts the bits in a register to the left by a variable amount, not directly related to assigning constants.\n\n**Therefore, `ori` is the most suitable instruction for assigning 16-bit constants in MIPS.** \n"
            }
        ]
    },
    {
        "doc_id": 1490,
        "doc": {
            "question_id": 123568829,
            "question": "The ____ instruction in MIPS is used by procedures to save a return address when jumping to a new location.",
            "options": [
                "j",
                "bne",
                "jr",
                "jal"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ instruction in MIPS is used by procedures to save a return address when jumping to a new location.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1491,
        "doc": {
            "question_id": 123568832,
            "question": "In MIPS assembly, the ____ instruction is used to multiply a register value by 2.",
            "options": [
                "mul",
                "add",
                "sll",
                "addi"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In MIPS assembly, the ____ instruction is used to multiply a register value by 2.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **sll (Shift Left Logical)** effectively multiplies a binary number by 2 for each bit position shifted. \n\nLet's break down why the other options are incorrect:\n\n* **mul:** While `mul` does perform multiplication, it's designed for general-purpose multiplication, not specifically multiplying by 2.\n* **add:**  `add` is used for addition. You could use `add` to add a register to itself (effectively multiplying by 2), but `sll` is the more efficient and direct approach.\n* **addi:** `addi` adds an immediate value to a register. It's not suitable for multiplication. \n"
            }
        ]
    },
    {
        "doc_id": 1492,
        "doc": {
            "question_id": 123568835,
            "question": "In MIPS architecture, the ____ instruction is typically used in combination with ori to load a full 32-bit constant into a register.",
            "options": [
                "addi",
                "lui",
                "lw",
                "sw"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, the ____ instruction is typically used in combination with ori to load a full 32-bit constant into a register.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1493,
        "doc": {
            "question_id": 123568841,
            "question": "____ registers are the responsibility of the calling function to save before making a procedure call.",
            "options": [
                "Callee-save",
                "Caller-save",
                "Argument",
                "Return value"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ registers are the responsibility of the calling function to save before making a procedure call.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1494,
        "doc": {
            "question_id": 123568843,
            "question": "In a recursive function call, the ____ is typically saved on the stack to allow proper return from nested calls.",
            "options": [
                "Program counter",
                "Stack pointer",
                "Frame pointer",
                "Return address"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a recursive function call, the ____ is typically saved on the stack to allow proper return from nested calls.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nThe **return address** is crucial in recursive (and all) function calls. Here's why:\n\n* **How Function Calls Work:** When a function is called, the program needs to know where to return execution after the function completes.  \n* **Return Address on the Stack:** The return address is pushed onto the stack before the function call. When the function finishes, the return address is popped off the stack, and the program counter is set to that address, ensuring execution resumes at the correct point. \n* **Recursion and Nested Calls:** In recursive scenarios, each nested call pushes its own return address. This creates a chain on the stack. As each nested call completes, the corresponding return address is popped, allowing the program to unwind back through the levels of recursion.\n\n**Other options and why they are incorrect:**\n\n* **A) Program Counter:** The program counter tracks the currently executing instruction. While it's involved in function calls, it's not specifically saved on the stack for return purposes.\n* **B) Stack Pointer:** The stack pointer tracks the top of the stack. It's essential for stack management but doesn't directly store the return address.\n* **C) Frame Pointer:** The frame pointer (if used) points to the current stack frame, which contains local variables and other function-related data. While helpful for accessing data within a function, it doesn't directly hold the return address. \n"
            }
        ]
    },
    {
        "doc_id": 1495,
        "doc": {
            "question_id": 123568844,
            "question": "____ are typically stored in $s0\u2013$s7 registers in MIPS architecture.",
            "options": [
                "Local variables",
                "Global variables",
                "Function parameters",
                "Return values"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ are typically stored in $s0\u2013$s7 registers in MIPS architecture.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **C) Function parameters**. Here's why:\n\n* **MIPS register conventions** dictate that registers $s0-$s7 are used to hold the first few parameters passed into a function. \n\nLet's break down the other options:\n\n* **A) Local variables:**  Local variables are typically stored on the stack, not in saved registers. \n* **B) Global variables:** Global variables reside in memory and are accessed using their addresses.\n* **D) Return values:**  Return values are typically stored in registers $v0 and $v1. \n\n**Therefore, the most accurate answer is C.** \n"
            }
        ]
    },
    {
        "doc_id": 1496,
        "doc": {
            "question_id": 123568847,
            "question": "In the MIPS memory map, the ____ segment stores the machine language program.",
            "options": [
                "text",
                "global data",
                "dynamic data",
                "reserved"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In the MIPS memory map, the ____ segment stores the machine language program.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1497,
        "doc": {
            "question_id": 123568851,
            "question": "____ is responsible for relocating data and instructions in object files during the linking process.",
            "options": [
                "The linker",
                "The compiler",
                "The assembler",
                "The loader"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is responsible for relocating data and instructions in object files during the linking process.",
                "Expected Label": "A",
                "Predicted Label": "A) The linker \n\n**Explanation:**\n\nThe linker is the program specifically designed to combine object files, relocate addresses, and resolve external references. Here's why the other options are incorrect:\n\n* **Compiler:** Translates high-level code into assembly language.\n* **Assembler:** Translates assembly language into machine code (object files).\n* **Loader:** Loads executable files into memory for execution. \n"
            }
        ]
    },
    {
        "doc_id": 1498,
        "doc": {
            "question_id": 123568852,
            "question": "The ____ is typically set to point to the middle of the global data segment when a program is loaded.",
            "options": [
                "$pc",
                "$gp",
                "$sp",
                "$ra"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ is typically set to point to the middle of the global data segment when a program is loaded.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **$gp (Global Pointer):**  This register is specifically designed to point to the middle of the global data segment. This allows for efficient access to global variables using a smaller offset. \n\nLet's break down the other options:\n\n* **$pc (Program Counter):** Holds the address of the next instruction to be executed.\n* **$sp (Stack Pointer):**  Points to the top of the current stack frame.\n* **$ra (Return Address):** Stores the return address for function calls. \n"
            }
        ]
    },
    {
        "doc_id": 1499,
        "doc": {
            "question_id": 123568856,
            "question": "____ is the typical number of floating-point registers in the MIPS architecture.",
            "options": [
                "16",
                "32",
                "64",
                "128"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is the typical number of floating-point registers in the MIPS architecture.",
                "Expected Label": "B",
                "Predicted Label": "B) 32 \n"
            }
        ]
    },
    {
        "doc_id": 1500,
        "doc": {
            "question_id": 123568857,
            "question": "The _____ architecture has been the de facto PC standard for more than two decades.",
            "options": [
                "ARM",
                "MIPS",
                "IA-32",
                "RISC-V"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The _____ architecture has been the de facto PC standard for more than two decades.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1501,
        "doc": {
            "question_id": 123568858,
            "question": "In IA-32 architecture, the ____ can directly operate on data in memory.",
            "options": [
                "program counter",
                "stack pointer",
                "condition codes",
                "instructions"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In IA-32 architecture, the ____ can directly operate on data in memory.",
                "Expected Label": "D",
                "Predicted Label": "D) instructions \n\n**Explanation:**\n\nInstructions in IA-32 architecture can use various addressing modes, some of which allow them to directly access and manipulate data stored in memory. \n\nHere's why the other options are incorrect:\n\n* **A) Program counter:** The program counter holds the memory address of the *next* instruction to be executed. It doesn't directly operate on data.\n* **B) Stack pointer:**  The stack pointer holds the memory address of the top of the stack, which is used for function calls and local variables. It manages memory locations but doesn't directly operate on the data within.\n* **C) Condition codes:** Condition codes (flags) reflect the outcome of the previous instruction (e.g., zero result, overflow). They are used in decision-making but don't directly operate on data in memory. \n"
            }
        ]
    },
    {
        "doc_id": 1502,
        "doc": {
            "question_id": 123568860,
            "question": "In IA-32 architecture, the ____ register stores status flags used for making decisions about branches and tracking arithmetic operations.",
            "options": [
                "EIP",
                "EFLAGS",
                "EAX",
                "EDX"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In IA-32 architecture, the ____ register stores status flags used for making decisions about branches and tracking arithmetic operations.",
                "Expected Label": "B",
                "Predicted Label": "Answer: B \n\n**Explanation:**\n\n* **EFLAGS (Extended Flags Register):** This register holds various status flags that reflect the outcome of arithmetic and logical operations. These flags are crucial for conditional jumps (branches) and error checking.\n\nLet's look at the other options:\n\n* **EIP (Extended Instruction Pointer):**  This register holds the address of the next instruction to be executed. \n* **EAX (Extended Accumulator Register):** This is a general-purpose register often used for arithmetic operations and function return values.\n* **EDX (Extended Data Register):**  Another general-purpose register, often used alongside EAX for larger calculations and input/output operations. \n"
            }
        ]
    },
    {
        "doc_id": 1503,
        "doc": {
            "question_id": 123568861,
            "question": "In the IA-32 architecture, the ____ instruction is used to perform a bitwise AND operation and set flags based on the result without modifying the destination operand.",
            "options": [
                "AND",
                "CMP",
                "TEST",
                "BT"
            ],
            "answer": "C",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the IA-32 architecture, the ____ instruction is used to perform a bitwise AND operation and set flags based on the result without modifying the destination operand.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1504,
        "doc": {
            "question_id": 123568864,
            "question": "The ____ prefix is used to choose between 16- and 32-bit operand sizes in IA-32 architecture.",
            "options": [
                "0x64",
                "0x66",
                "0x68",
                "0x6A"
            ],
            "answer": "B",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ prefix is used to choose between 16- and 32-bit operand sizes in IA-32 architecture.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1505,
        "doc": {
            "question_id": 123568868,
            "question": "A ____ attack exploits vulnerabilities in string copying functions to potentially overwrite memory and execute malicious code.",
            "options": [
                "stack overflow",
                "heap corruption",
                "buffer overflow",
                "integer overflow"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ attack exploits vulnerabilities in string copying functions to potentially overwrite memory and execute malicious code.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1506,
        "doc": {
            "question_id": 123568872,
            "question": "In MIPS assembly, the ____ instruction is used to return from a subroutine.",
            "options": [
                "ret",
                "return",
                "jr",
                "bra"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In MIPS assembly, the ____ instruction is used to return from a subroutine.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **jr $ra** is the instruction used in MIPS assembly to return from a subroutine.  `$ra` (return address) is a special register that stores the address of the instruction after the one that called the subroutine. \n\nLet's break down why the other options are incorrect:\n\n* **ret** and **return** are commonly used in other assembly languages (like x86), but not in MIPS.\n* **bra** (branch) is used for unconditional jumps within a program, not specifically for returning from subroutines. \n"
            }
        ]
    },
    {
        "doc_id": 1507,
        "doc": {
            "question_id": 123568883,
            "question": "In a typical MIPS-like architecture, the ____ instruction uses the rt field to specify the destination register.",
            "options": [
                "R-type",
                "lw",
                "sw",
                "beq"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a typical MIPS-like architecture, the ____ instruction uses the rt field to specify the destination register.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **lw (load word)** instructions load data from memory into a register. The **rt** field specifies the destination register where the loaded data will be stored.\n\nLet's look at why the other options are incorrect:\n\n* **A) R-type:** R-type instructions typically use the **rd** field for the destination register.\n* **C) sw (store word):**  sw instructions store data from a register into memory. The **rt** field specifies the *source* register containing the data to be stored.\n* **D) beq (branch if equal):** beq is a branch instruction and doesn't directly use registers for data manipulation like load/store. \n\nLet me know if you'd like a more detailed explanation of any of these instruction types! \n"
            }
        ]
    },
    {
        "doc_id": 1508,
        "doc": {
            "question_id": 123568884,
            "question": "In a typical MIPS processor control unit design, the ____ decodes the opcode to generate most control signals and produces an ALUOp signal for further decoding.",
            "options": [
                "ALU",
                "Instruction Memory",
                "Main Decoder",
                "Register File"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a typical MIPS processor control unit design, the ____ decodes the opcode to generate most control signals and produces an ALUOp signal for further decoding.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Main Decoder:** The primary role of the main decoder within a MIPS control unit is to interpret the opcode portion of an instruction.  Based on the opcode, it generates a significant portion of the control signals that orchestrate the execution of the instruction. It also produces an ALUOp signal, which is further decoded to determine the specific ALU operation required.\n\nLet me know if you'd like more detail on any aspect of MIPS control unit design! \n"
            }
        ]
    },
    {
        "doc_id": 1509,
        "doc": {
            "question_id": 123568886,
            "question": "____ is the control signal that determines whether data should be written to the register file.",
            "options": [
                "RegWrite",
                "MemWrite",
                "ALUSrc",
                "Branch"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the control signal that determines whether data should be written to the register file.",
                "Expected Label": "A",
                "Predicted Label": "A) RegWrite \n"
            }
        ]
    },
    {
        "doc_id": 1510,
        "doc": {
            "question_id": 123568890,
            "question": "In a single-cycle processor, the ____ is always 1.",
            "options": [
                "CPI",
                "IPC",
                "Clock frequency",
                "Pipeline depth"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In a single-cycle processor, the ____ is always 1.",
                "Expected Label": "A",
                "Predicted Label": "A) CPI \n\n**Explanation:**\n\n* **CPI (Cycles Per Instruction):**  In a single-cycle processor, each instruction completes in a single clock cycle. Therefore, the CPI is always 1. \n\nLet's look at why the other options are incorrect:\n\n* **IPC (Instructions Per Cycle):** While technically 1 in a single-cycle processor, IPC is a more relevant metric for pipelined processors where multiple instructions are processed concurrently.\n* **Clock Frequency:** This refers to the speed of the clock signal and is independent of the number of cycles per instruction.\n* **Pipeline Depth:** Single-cycle processors don't have pipelines. Pipelines are used in more complex processors to improve performance. \n"
            }
        ]
    },
    {
        "doc_id": 1511,
        "doc": {
            "question_id": 123568897,
            "question": "____ is responsible for generating the appropriate sequence of control signals for each step of instruction execution in a multicycle MIPS processor.",
            "options": [
                "Finite State Machine",
                "Instruction Register",
                "Program Counter",
                "ALU Decoder"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is responsible for generating the appropriate sequence of control signals for each step of instruction execution in a multicycle MIPS processor.",
                "Expected Label": "A",
                "Predicted Label": "A"
            }
        ]
    },
    {
        "doc_id": 1512,
        "doc": {
            "question_id": 123568903,
            "question": "In a multicycle processor, the ____ signal is used to select between the incremented PC and other sources for the ALU input.",
            "options": [
                "ALUOp",
                "PCSrc",
                "ALUSrcA",
                "MemtoReg"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a multicycle processor, the ____ signal is used to select between the incremented PC and other sources for the ALU input.",
                "Expected Label": "C",
                "Predicted Label": "B) PCSrc \n\n**Explanation:**\n\nThe **PCSrc (Program Counter Source)** signal is specifically designed to determine whether the next instruction's address comes from:\n\n* **Incrementing the current PC:**  Used for the default sequential execution of instructions.\n* **A different source:**  Used for branches, jumps, or other instructions that alter the normal program flow. \n\nThe other options are not relevant to this specific function:\n\n* **ALUOp:**  Specifies the operation the ALU should perform.\n* **ALUSrcA:** Selects the source for one of the ALU inputs, often choosing between a register value or the immediate value from the instruction.\n* **MemtoReg:**  Determines whether the data written into the register file comes from the ALU result or from memory. \n"
            }
        ]
    },
    {
        "doc_id": 1513,
        "doc": {
            "question_id": 123568904,
            "question": "In a multicycle processor, the ____ stage is responsible for writing the result of an ALU operation back to a register.",
            "options": [
                "Fetch",
                "Decode",
                "Execute",
                "ALU Writeback"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In a multicycle processor, the ____ stage is responsible for writing the result of an ALU operation back to a register.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1514,
        "doc": {
            "question_id": 123568907,
            "question": "The main disadvantage of a multicycle processor compared to a single-cycle processor is the need for ____ .",
            "options": [
                "additional adders",
                "larger instruction memory",
                "slower clock speed",
                "additional multiplexers"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The main disadvantage of a multicycle processor compared to a single-cycle processor is the need for ____ .",
                "Expected Label": "D",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1515,
        "doc": {
            "question_id": 123568910,
            "question": "____ is the pipeline stage where the control unit examines the opcode and funct fields of the instruction to produce control signals.",
            "options": [
                "Decode",
                "Execute",
                "Memory",
                "Writeback"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the pipeline stage where the control unit examines the opcode and funct fields of the instruction to produce control signals.",
                "Expected Label": "A",
                "Predicted Label": "A) Decode \n"
            }
        ]
    },
    {
        "doc_id": 1516,
        "doc": {
            "question_id": 123568922,
            "question": "The ____ unit in a pipelined processor is responsible for detecting and resolving data hazards.",
            "options": [
                "ALU",
                "Hazard",
                "Control",
                "Register File"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "The ____ unit in a pipelined processor is responsible for detecting and resolving data hazards.",
                "Expected Label": "B",
                "Predicted Label": "C) Control \n\nHere's why:\n\n* **Control Unit:** The control unit is the \"brain\" of the processor. In a pipelined processor, it's responsible for:\n    * **Instruction Fetching and Decoding**\n    * **Hazard Detection:** Identifying potential data hazards caused by dependencies between instructions in the pipeline.\n    * **Hazard Resolution:** Implementing strategies to resolve hazards, such as stalling the pipeline or forwarding data. \n\nLet's look at why the other options are incorrect:\n\n* **ALU (Arithmetic Logic Unit):**  Performs arithmetic and logical operations on data. \n* **Hazard:**  A general term for a situation that can cause pipeline problems, not a specific unit.\n* **Register File:** Stores frequently used data values.\n\n**Therefore, the Control Unit is directly responsible for detecting and resolving data hazards in a pipelined processor.** \n"
            }
        ]
    },
    {
        "doc_id": 1517,
        "doc": {
            "question_id": 123568924,
            "question": "The ____ typically contains the main decoder and the ALU decoder in a MIPS processor design.",
            "options": [
                "Datapath",
                "Register file",
                "Instruction memory",
                "Controller"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ typically contains the main decoder and the ALU decoder in a MIPS processor design.",
                "Expected Label": "D",
                "Predicted Label": "D \n\n**Explanation:**\n\nThe **controller** is responsible for:\n\n* **Fetching instructions:**  It retrieves instructions from memory.\n* **Decoding instructions:** This is where the **main decoder** breaks down the instruction into its components (opcode, operands).\n* **Generating control signals:** Based on the decoded instruction, the controller generates signals that direct other components like the ALU, memory, and registers. \n* **Sequencing operations:** It ensures operations happen in the correct order.\n\nThe **ALU decoder**, specifically, is part of the control unit and  determines the ALU operation (add, subtract, etc.) based on the instruction.\n\n**Why other options are incorrect:**\n\n* **Datapath:** This encompasses the ALU, registers, and connections between them, handling the actual data manipulation.\n* **Register file:** Stores frequently used data values.\n* **Instruction memory:** Holds the program instructions. \n"
            }
        ]
    },
    {
        "doc_id": 1518,
        "doc": {
            "question_id": 123568925,
            "question": "____ is typically used to specify the operation to be performed by the ALU in a MIPS processor.",
            "options": [
                "alucontrol",
                "memtoreg",
                "regdst",
                "pcsrc"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to specify the operation to be performed by the ALU in a MIPS processor.",
                "Expected Label": "A",
                "Predicted Label": "A\n"
            }
        ]
    },
    {
        "doc_id": 1519,
        "doc": {
            "question_id": 123568927,
            "question": "In a typical MIPS-like architecture, the ____ instruction uses the ALU to perform subtraction.",
            "options": [
                "LW",
                "SW",
                "BEQ",
                "ADDI"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a typical MIPS-like architecture, the ____ instruction uses the ALU to perform subtraction.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **LW (Load Word)** and **SW (Store Word)** are data transfer instructions and primarily use the memory unit, not the ALU for subtraction. \n* **ADDI (Add Immediate)** uses the ALU for addition, not subtraction.\n* **BEQ (Branch if Equal)** compares two values.  To do this, it typically uses the ALU to subtract the values. If the result is zero (meaning they are equal), the branch is taken.\n\n**Therefore, the BEQ instruction utilizes the ALU for subtraction to determine if a branch should occur.** \n"
            }
        ]
    },
    {
        "doc_id": 1520,
        "doc": {
            "question_id": 123568928,
            "question": "In MIPS architecture, the ____ component is responsible for sign-extending 16-bit immediate values to 32 bits.",
            "options": [
                "alu",
                "regfile",
                "adder",
                "signext"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, the ____ component is responsible for sign-extending 16-bit immediate values to 32 bits.",
                "Expected Label": "D",
                "Predicted Label": "D\n"
            }
        ]
    },
    {
        "doc_id": 1521,
        "doc": {
            "question_id": 123568929,
            "question": "____ is commonly used to model combinational logic in hardware description languages.",
            "options": [
                "assign",
                "always",
                "process",
                "entity"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is commonly used to model combinational logic in hardware description languages.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **assign** statements are specifically used in HDLs (like Verilog and SystemVerilog) to describe combinational logic. They represent a continuous assignment of values based on the right-hand side expression. \n\nLet's look at why the other options are incorrect:\n\n* **always** blocks can be used for both combinational and sequential logic depending on their sensitivity list.\n* **process** is similar to 'always' and its usage depends on the sensitivity list within it.\n* **entity** defines the interface of a module (like defining the inputs and outputs) and doesn't describe the internal logic. \n"
            }
        ]
    },
    {
        "doc_id": 1522,
        "doc": {
            "question_id": 123568934,
            "question": "In MIPS architecture, the ____ register stores the code indicating the source of an exception.",
            "options": [
                "EPC",
                "PC",
                "Cause",
                "Status"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In MIPS architecture, the ____ register stores the code indicating the source of an exception.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1523,
        "doc": {
            "question_id": 123568936,
            "question": "____ is the primary factor that limits the maximum number of pipeline stages in a processor.",
            "options": [
                "Pipeline hazards",
                "Clock frequency",
                "Transistor size",
                "Cache size"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is the primary factor that limits the maximum number of pipeline stages in a processor.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\nPipeline hazards are the primary limiting factor for pipeline depth.  Here's why:\n\n* **Pipeline Hazards:** These occur when instructions in different pipeline stages depend on each other in a way that prevents the pipeline from flowing smoothly. This stalls the pipeline, negating the performance benefits of deeper pipelining.\n* **Clock Frequency:** While clock frequency affects overall performance, it's not the primary limiter on the number of pipeline stages. A well-designed pipeline can be clocked higher.\n* **Transistor Size:** Smaller transistors allow for more complex designs and potentially deeper pipelines, but don't directly limit the stage count.\n* **Cache Size:** Cache size primarily affects memory access times and has an indirect relationship to pipeline depth. \n\n**In summary:**  Deeper pipelines increase the risk and complexity of managing pipeline hazards, making them the primary bottleneck. \n"
            }
        ]
    },
    {
        "doc_id": 1524,
        "doc": {
            "question_id": 123568944,
            "question": "A ____ processor contains more than one copy of its architectural state, allowing multiple threads to be active simultaneously.",
            "options": [
                "superscalar",
                "out-of-order",
                "multithreaded",
                "pipelined"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A ____ processor contains more than one copy of its architectural state, allowing multiple threads to be active simultaneously.",
                "Expected Label": "C",
                "Predicted Label": "C \n\n**Explanation:**\n\n* **Multithreaded processors** are specifically designed to handle multiple threads concurrently by having multiple sets of registers and program counters (architectural state). \n\nLet's look at why the other options are incorrect:\n\n* **Superscalar processors:** Execute multiple instructions per clock cycle but typically operate on a single thread at a time.\n* **Out-of-order processors:**  Execute instructions in an order that may differ from the original program sequence to optimize performance, but they still focus on a single thread's context.\n* **Pipelined processors:** Divide instruction execution into stages to overlap execution, improving throughput, but don't inherently support multiple active threads simultaneously. \n"
            }
        ]
    },
    {
        "doc_id": 1525,
        "doc": {
            "question_id": 123568945,
            "question": "In ____, each processor has its own local memory system.",
            "options": [
                "symmetric multiprocessing",
                "asymmetric multiprocessing",
                "core-based multiprocessing",
                "clustered multiprocessing"
            ],
            "answer": "D",
            "difficulty": "HARD",
            "topic": "Processor Architecture"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "In ____, each processor has its own local memory system.",
                "Expected Label": "D",
                "Predicted Label": "D) clustered multiprocessing \n"
            }
        ]
    },
    {
        "doc_id": 1526,
        "doc": {
            "question_id": 123568946,
            "question": "____ is typically used to generate control signals in a multicycle processor.",
            "options": [
                "ALU",
                "Microcode PLA",
                "Cache",
                "Register file"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically used to generate control signals in a multicycle processor.",
                "Expected Label": "B",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Microcode PLA (Programmable Logic Array)** is commonly used in multicycle processors to decode instructions and generate the necessary control signals for each step of instruction execution.\n\nLet's break down why the other options are incorrect:\n\n* **ALU (Arithmetic Logic Unit)** performs arithmetic and logical operations but doesn't generate control signals.\n* **Cache** is a high-speed memory that improves performance but doesn't directly manage control signals.\n* **Register file** stores data and instructions but doesn't generate control signals. \n"
            }
        ]
    },
    {
        "doc_id": 1527,
        "doc": {
            "question_id": 123568947,
            "question": "______ is a technique used to increase the throughput of multiple threads in some processor designs.",
            "options": [
                "Multithreading",
                "Pipelining",
                "Out-of-order execution",
                "SIMD"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "______ is a technique used to increase the throughput of multiple threads in some processor designs.",
                "Expected Label": "A",
                "Predicted Label": "A) Multithreading \n\n**Explanation:**\n\n* **Multithreading** allows a single processor core to manage multiple threads of execution concurrently, switching between them rapidly to improve overall throughput. \n\nLet's look at why the other options are less suitable:\n\n* **Pipelining** improves the execution of a single instruction stream, not multiple threads.\n* **Out-of-order execution** optimizes instruction execution within a single thread.\n* **SIMD** (Single Instruction, Multiple Data) is used for parallel processing of data within a single instruction, not for managing multiple threads. \n \nLet me know if you'd like more details on any of these concepts! \n"
            }
        ]
    },
    {
        "doc_id": 1528,
        "doc": {
            "question_id": 123568950,
            "question": "____ is a common technique used to reduce the delay of arithmetic operations in processors.",
            "options": [
                "Prefix adder",
                "Suffix adder",
                "Postfix adder",
                "Infix adder"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Processor Architecture"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is a common technique used to reduce the delay of arithmetic operations in processors.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **Prefix adders** (also known as carry-lookahead adders) are specifically designed to accelerate addition by calculating carry bits in parallel. This significantly reduces the delay compared to ripple-carry adders, making them a fundamental optimization technique in modern processors. \n\nThe other options are not relevant to arithmetic operation optimization:\n\n* **Suffix adder, Postfix adder, Infix adder:** These terms are not commonly used in computer architecture and don't represent established adder types. \n"
            }
        ]
    },
    {
        "doc_id": 1529,
        "doc": {
            "question_id": 123568956,
            "question": "____ is typically used to build the cache memory in modern processors.",
            "options": [
                "SRAM",
                "DRAM",
                "Flash",
                "ROM"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to build the cache memory in modern processors.",
                "Expected Label": "A",
                "Predicted Label": "A \n\n**Explanation:**\n\n* **SRAM (Static RAM)** is fast and expensive, making it suitable for cache memory where speed is crucial. \n* **DRAM (Dynamic RAM)** is slower and cheaper, commonly used for main memory.\n* **Flash** is non-volatile memory used for storage, not suitable for cache.\n* **ROM (Read-Only Memory)** is also non-volatile and primarily used for storing firmware or boot instructions. \n"
            }
        ]
    },
    {
        "doc_id": 1530,
        "doc": {
            "question_id": 123568957,
            "question": "____ is typically the slowest component in the memory hierarchy of a computer system.",
            "options": [
                "Cache",
                "Hard disk",
                "Main memory",
                "CPU registers"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ is typically the slowest component in the memory hierarchy of a computer system.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1531,
        "doc": {
            "question_id": 123568958,
            "question": "The ____ of a cache refers to the number of data words it can hold.",
            "options": [
                "latency",
                "bandwidth",
                "miss rate",
                "capacity"
            ],
            "answer": "D",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "D",
        "data_review_info": [
            {
                "Question": "The ____ of a cache refers to the number of data words it can hold.",
                "Expected Label": "D",
                "Predicted Label": "D \n"
            }
        ]
    },
    {
        "doc_id": 1532,
        "doc": {
            "question_id": 123568961,
            "question": "In a cache memory system, the ____ bits of the address indicate which set holds the data.",
            "options": [
                "most significant",
                "tag",
                "least significant",
                "offset"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a cache memory system, the ____ bits of the address indicate which set holds the data.",
                "Expected Label": "C",
                "Predicted Label": "**C) least significant** \n\n**Explanation:**\n\nIn a set-associative or direct-mapped cache, the memory address is divided into three parts:\n\n* **Tag:**  Used to identify if the data from a particular memory location is currently stored in the cache line.\n* **Index (Set bits):**  Determines which *set* within the cache the data might reside in.\n* **Offset:**  Specifies the exact byte location *within* the cache line where the data is stored.\n\nThe **least significant bits** of the address are used for the offset, as they pinpoint the byte within a block. Moving upwards, the next set of bits form the **index**, indicating the set.  The remaining **most significant bits** are the tag. \n"
            }
        ]
    },
    {
        "doc_id": 1533,
        "doc": {
            "question_id": 123568962,
            "question": "In an N-way set associative cache, ____ is the number of blocks in each set where data mapping to that set might be found.",
            "options": [
                "C",
                "N",
                "S",
                "V"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In an N-way set associative cache, ____ is the number of blocks in each set where data mapping to that set might be found.",
                "Expected Label": "B",
                "Predicted Label": "Answer: **B) N** \n\n**Explanation:**\n\nIn a set-associative cache:\n\n* **N** refers to the **associativity**, which determines how many different cache blocks within a set a particular piece of data can be mapped to.  \n* **C** might represent the total cache capacity, but not the number of blocks per set.\n* **S** isn't a standard term used in cache organization.\n* **V** also isn't a standard term in this context. \n"
            }
        ]
    },
    {
        "doc_id": 1534,
        "doc": {
            "question_id": 123568963,
            "question": "A fully associative cache contains a single set with ____ ways, where B is the number of blocks.",
            "options": [
                "2",
                "4",
                "B",
                "log2B"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "A fully associative cache contains a single set with ____ ways, where B is the number of blocks.",
                "Expected Label": "C",
                "Predicted Label": "The answer is **(C) B**. \n\nHere's why:\n\n* **Fully associative caches** allow a block to be placed anywhere within the cache. There are no restrictions based on index bits.\n* **Sets and Ways:**  \n    * A \"set\" in cache organization refers to a group of blocks that a particular memory address can map to. \n    * \"Ways\" are the number of blocks within a set.\n\nSince a fully associative cache allows placement anywhere, it effectively has only one set. To accommodate any block, the number of \"ways\" (places within that single set) must equal the total number of blocks in the cache. \n"
            }
        ]
    },
    {
        "doc_id": 1535,
        "doc": {
            "question_id": 123568965,
            "question": "In cache design, ____ are used to select words within a block.",
            "options": [
                "Multiplexers",
                "Decoders",
                "Adders",
                "Shifters"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "In cache design, ____ are used to select words within a block.",
                "Expected Label": "A",
                "Predicted Label": "B \n\n**Explanation:**\n\n* **Decoders** are used to select a specific word within a cache block. The block offset bits in the memory address are used as input to the decoder, which then activates a single output line corresponding to the desired word within the block.\n\nLet's look at why the other options are incorrect:\n\n* **Multiplexers** are used to select one input from multiple inputs, not for selecting words within a block.\n* **Adders** perform arithmetic addition and are not involved in word selection within a cache block.\n* **Shifters** shift bit patterns and are not directly related to selecting words within a cache block. \n"
            }
        ]
    },
    {
        "doc_id": 1536,
        "doc": {
            "question_id": 123568969,
            "question": "In a ____ cache, dirty blocks are written back to main memory only when evicted.",
            "options": [
                "write-through",
                "write-back",
                "fully associative",
                "direct-mapped"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In a ____ cache, dirty blocks are written back to main memory only when evicted.",
                "Expected Label": "B",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1537,
        "doc": {
            "question_id": 123568973,
            "question": "A ____ caches the most commonly used page table entries to speed up address translation.",
            "options": [
                "page directory",
                "translation lookaside buffer",
                "shadow page table",
                "virtual address cache"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "A ____ caches the most commonly used page table entries to speed up address translation.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1538,
        "doc": {
            "question_id": 123568977,
            "question": "In virtual memory systems, ____ are used to determine which programs can write to shared physical pages.",
            "options": [
                "Page tables",
                "Virtual addresses",
                "Control bits",
                "Physical pages"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In virtual memory systems, ____ are used to determine which programs can write to shared physical pages.",
                "Expected Label": "C",
                "Predicted Label": "C\n"
            }
        ]
    },
    {
        "doc_id": 1539,
        "doc": {
            "question_id": 123568979,
            "question": "____ is typically the size of a page in virtual memory systems.",
            "options": [
                "4 KB",
                "8 KB",
                "16 KB",
                "32 KB"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically the size of a page in virtual memory systems.",
                "Expected Label": "A",
                "Predicted Label": "B\n"
            }
        ]
    },
    {
        "doc_id": 1540,
        "doc": {
            "question_id": 123568990,
            "question": "____ is typically used to store mappings between virtual page numbers and physical page numbers in a virtual memory system.",
            "options": [
                "Page table",
                "Cache",
                "Register file",
                "Instruction buffer"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Memory Systems"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "____ is typically used to store mappings between virtual page numbers and physical page numbers in a virtual memory system.",
                "Expected Label": "A",
                "Predicted Label": "A \n"
            }
        ]
    },
    {
        "doc_id": 1541,
        "doc": {
            "question_id": 123568992,
            "question": "In a ____ address space, the maximum amount of virtual memory each program can use is 4 GB.",
            "options": [
                "16-bit",
                "24-bit",
                "32-bit",
                "64-bit"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Architectural Support"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In a ____ address space, the maximum amount of virtual memory each program can use is 4 GB.",
                "Expected Label": "C",
                "Predicted Label": "Answer: C \n\n**Explanation:**\n\nA 32-bit address space provides 2^32 possible addresses.  Since each address typically points to a byte, this equates to 4,294,967,296 bytes, or 4GB. \n"
            }
        ]
    },
    {
        "doc_id": 1542,
        "doc": {
            "question_id": 123568996,
            "question": "A ____ is typically used to decode binary inputs and drive a seven-segment display.",
            "options": [
                "7-segment decoder",
                "ALU",
                "comparator",
                "multiplexer"
            ],
            "answer": "A",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "A",
        "data_review_info": [
            {
                "Question": "A ____ is typically used to decode binary inputs and drive a seven-segment display.",
                "Expected Label": "A",
                "Predicted Label": "The answer is **A) 7-segment decoder**. Here's why:\n\n* **7-segment decoder:** These circuits are specifically designed to take a binary input (representing a number) and activate the correct segments on a seven-segment display to visually represent that number.\n\nLet's look at why the other options are incorrect:\n\n* **B) ALU (Arithmetic Logic Unit):** ALUs perform arithmetic and logical operations within a CPU, not display decoding.\n* **C) Comparator:** Comparators compare two input values and output a signal indicating equality, greater than, or less than relationships.\n* **D) Multiplexer:** Multiplexers select one of several input signals and route it to a single output. \n\nLet me know if you'd like more details on any of these components! \n"
            }
        ]
    },
    {
        "doc_id": 1543,
        "doc": {
            "question_id": 123568998,
            "question": "In the FPGA design flow, ____ converts the HDL into Boolean functions.",
            "options": [
                "Design Entry",
                "Logic Verification",
                "Logic Synthesis",
                "Mapping"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "Electronic Design Automation (EDA)"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "In the FPGA design flow, ____ converts the HDL into Boolean functions.",
                "Expected Label": "C",
                "Predicted Label": "C \n"
            }
        ]
    },
    {
        "doc_id": 1544,
        "doc": {
            "question_id": 123569004,
            "question": "____ packages have pins that can be inserted through holes in a printed circuit board or into a socket.",
            "options": [
                "Surface mount",
                "Through-hole",
                "Thin small outline",
                "Plastic leaded chip carrier"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "____ packages have pins that can be inserted through holes in a printed circuit board or into a socket.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1545,
        "doc": {
            "question_id": 123569007,
            "question": "In transmission line modeling, electromagnetic waves propagate at the speed of ____ in a given medium.",
            "options": [
                "sound",
                "light",
                "electricity",
                "heat"
            ],
            "answer": "B",
            "difficulty": "EASY",
            "topic": "Circuits"
        },
        "target": "B",
        "data_review_info": [
            {
                "Question": "In transmission line modeling, electromagnetic waves propagate at the speed of ____ in a given medium.",
                "Expected Label": "B",
                "Predicted Label": "B \n"
            }
        ]
    },
    {
        "doc_id": 1546,
        "doc": {
            "question_id": 123569019,
            "question": "The ____ cost of an ASIC implementation is typically higher than that of an FPGA implementation.",
            "options": [
                "Recurring",
                "Variable",
                "Non-recurring engineering (NRE)",
                "Operating"
            ],
            "answer": "C",
            "difficulty": "EASY",
            "topic": "IP Design and Manufacturing"
        },
        "target": "C",
        "data_review_info": [
            {
                "Question": "The ____ cost of an ASIC implementation is typically higher than that of an FPGA implementation.",
                "Expected Label": "C",
                "Predicted Label": "C) Non-recurring engineering (NRE) \n"
            }
        ]
    }
]